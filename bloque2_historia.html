
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>21. Revisión histórica &#8212; Minería de Textos</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/estilos.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="22. Representaciones de palabras y oraciones" href="bloque2_embeddings.html" />
    <link rel="prev" title="20. Técnicas para la minería de textos" href="bloque2.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo-master-ca.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Minería de Textos</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Materiales de Minería de Textos
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1.html">
   1. Introducción a la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_1Introduccion.html">
   2. Minería de textos y procesamiento del lenguaje natural.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_2CategorialSintactico.html">
   3. Análisis categorial y sintáctico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_3AnalisisSemantico.html">
   4. Análisis semántico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica1.html">
   5. Práctica 1.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_4AnalisisSemanticoVectorial.html">
   6. Análisis semántico vectorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica2.html">
   7. Práctica 1b :
   <em>
    Topic modeling
   </em>
   .
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3.html">
   8. Aplicaciones de la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t1_aplicaciones.html">
   9. T1. Aplicaciones generales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2_subaplicaciones-benchmarks.html">
   10. T2. Aplicaciones específicas y Benchmacks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2.1_analisis_sentimientos.html">
   11. T2.1. Aplicaciones específicas. Análisis de Sentimientos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t3.1_metricas.html">
   12. T3. Métricas de Evaluación
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t4_huggingface.html">
   13. T4. Centralización de datasets y modelos: Huggingface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5_automl.html">
   14. T5. Auto Machine Learning(AutoML)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5.1_autogoal.html">
   15. T5.1. AutoGOAL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p1_SA-Pipeline-Reviews.html">
   16. P1.1. Pipeline simple
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p2_SA-Transformers-Basic.html">
   17. P1.2. APIs Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p3_SA-Transformers-Training-FineTuning.html">
   18. P2. Reajustar modelos Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p4_SA-Transformers-Training-Custom.html">
   19. P3. Composición de vectores de características
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 3
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2.html">
   20. Técnicas para la minería de textos
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   21. Revisión histórica
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2_embeddings.html">
   22. Representaciones de palabras y oraciones
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2_practica.html">
   23. Práctica. Lectura y documentación del código de un extractor de entidades
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Extras
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="content.html">
   24. Content in Jupyter Book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markdown.html">
   25. Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   26. Content with notebooks
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/bloque2_historia.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algo-de-historia">
   21.1. Algo de historia
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-inicios-y-la-traduccion-automatica">
     21.1.1. Los inicios y la traducción automática
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-primeros-sistemas-conversacionales">
     21.1.2. Los primeros sistemas conversacionales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tecnicas-simbolicas-y-estadisticas">
     21.1.3. Técnicas simbólicas y estadísticas
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-primeros-modelos-neuronales">
     21.1.4. Los primeros modelos neuronales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-entrenamiento-no-supervisado">
     21.1.5. El entrenamiento no supervisado
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aprendizaje-multitarea">
     21.1.6. Aprendizaje multitarea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embeddings">
     21.1.7. Word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-inicio-del-auge-de-las-redes-neuronales">
     21.1.8. El inicio del auge de las redes neuronales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-recurrentes-sequence-to-sequence">
     21.1.9. Modelos recurrentes sequence-to-sequence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-mecanismo-de-atencion">
     21.1.10. El mecanismo de atención
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#redes-neuronales-con-memoria-explicita">
     21.1.11. Redes neuronales con memoria explícita
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-arquitectura-transformer">
     21.1.12. La arquitectura transformer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-preentrenados">
     21.1.13. Modelos preentrenados
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-modelo-gpt-3">
     21.1.14. El modelo GPT-3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#corpus-disponibles">
     21.1.15. Corpus disponibles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-de-subpalabras">
     21.1.16. Modelos de subpalabras
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aplicaciones">
     21.1.17. Aplicaciones
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Revisión histórica</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algo-de-historia">
   21.1. Algo de historia
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-inicios-y-la-traduccion-automatica">
     21.1.1. Los inicios y la traducción automática
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-primeros-sistemas-conversacionales">
     21.1.2. Los primeros sistemas conversacionales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tecnicas-simbolicas-y-estadisticas">
     21.1.3. Técnicas simbólicas y estadísticas
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#los-primeros-modelos-neuronales">
     21.1.4. Los primeros modelos neuronales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-entrenamiento-no-supervisado">
     21.1.5. El entrenamiento no supervisado
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aprendizaje-multitarea">
     21.1.6. Aprendizaje multitarea
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#word-embeddings">
     21.1.7. Word embeddings
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-inicio-del-auge-de-las-redes-neuronales">
     21.1.8. El inicio del auge de las redes neuronales
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-recurrentes-sequence-to-sequence">
     21.1.9. Modelos recurrentes sequence-to-sequence
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-mecanismo-de-atencion">
     21.1.10. El mecanismo de atención
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#redes-neuronales-con-memoria-explicita">
     21.1.11. Redes neuronales con memoria explícita
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#la-arquitectura-transformer">
     21.1.12. La arquitectura transformer
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-preentrenados">
     21.1.13. Modelos preentrenados
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#el-modelo-gpt-3">
     21.1.14. El modelo GPT-3
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#corpus-disponibles">
     21.1.15. Corpus disponibles
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-de-subpalabras">
     21.1.16. Modelos de subpalabras
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#aplicaciones">
     21.1.17. Aplicaciones
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="revision-historica">
<span id="label-historia"></span><h1><span class="section-number">21. </span>Revisión histórica<a class="headerlink" href="#revision-historica" title="Permalink to this headline">#</a></h1>
<p><em>«If I have seen further it is by standing on the shoulders of Giants.», Isaac Newton, 1675.</em></p>
<p>A continuación se presenta una revisión histórica de la evolución del procesamiento del lenguaje natural desde sus inicios hasta nuestro días que sirve para hilvanar los contenidos de la asignatura. En los siguientes apartados de este bloque se entrará en detalles técnicos sobre el funcionamiento de los modelos neuronales, que son los que actualmente proporcionan los mejores resultados en las principales aplicaciones.</p>
<div class="note admonition">
<p class="admonition-title">Nota</p>
<p>Hacia final de curso deberías ser capaz de leer esta revisión histórica entendiendo cada aspecto explicado.</p>
</div>
<div class="tip admonition">
<p class="admonition-title">Nota</p>
<p>El análisis “<a class="reference external" href="https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape">10 things</a> you need to know about BERT and the transformer architecture that are reshaping the AI landscape”, más centrado en el panorama reciente del procesamiento del lenguaje natural, también te puede servir para repasar todo este bloque a final de curso y asegurarte de que has entendido todos sus conceptos.</p>
</div>
<section id="algo-de-historia">
<h2><span class="section-number">21.1. </span>Algo de historia<a class="headerlink" href="#algo-de-historia" title="Permalink to this headline">#</a></h2>
<section id="los-inicios-y-la-traduccion-automatica">
<h3><span class="section-number">21.1.1. </span>Los inicios y la traducción automática<a class="headerlink" href="#los-inicios-y-la-traduccion-automatica" title="Permalink to this headline">#</a></h3>
<p>El procesamiento del lenguaje natural ha tenido en la tarea de la traducción automática uno de sus principales catalizadores, por lo que la evolución de esta tarea concreta es un buen reflejo de la evolución de la disciplina entera. En el año 1947, el matemático e informático Warren Weaver planteaba en una carta a un colega la posibilidad de usar los computadores que comenzaban a aparecer en la época para traducir la documentación de la Unesco: «Recognizing fully, even though necessarily vaguely, the semantic difficulties because of multiple meanings, etc., I have wondered if it were unthinkable to design a computer which would translate.» Cinco años más tarde se celebraba una reunión de expertos en traducción automática en el MIT de los EUA, y en 1954 se presentó un sistema desarrollado conjuntamente por la Universidad de Georgetown e IBM que traducía unas 60 frases del ruso al inglés utilizando un pequeño diccionario y seis reglas de reordenamiento y selección léxica. Pese a la euforia inicial, la mejora en la calidad de estos sistemas fue tan lenta que en 1966 un comité de expertos recomendó al gobierno de los EUA recortar el presupuesto de investigación invertido en ellos.</p>
</section>
<section id="los-primeros-sistemas-conversacionales">
<h3><span class="section-number">21.1.2. </span>Los primeros sistemas conversacionales<a class="headerlink" href="#los-primeros-sistemas-conversacionales" title="Permalink to this headline">#</a></h3>
<p>Una de las grandes pretensiones de la humanidad ha sido desde hace siglos la de disponer de ingenios parlantes. Ramón Llull en el siglo XII concibió un artilugio mecánico con el que pretendía que se pudiera probar o refutar cualquier proposición. En el capítulo 63 del segundo libro del Quijote, el Caballero de la Triste Figura queda sorprendido al entablar diálogo con una cabeza parlante, de la que nunca supo que se trataba de un engaño. Pero realmente los primeros sistemas conversacionales (<em>bots</em>) no aparecen hasta mediados de los 60, siendo ELIZA, que incluso podía adoptar diferentes personalidades o roles, o SHRDLU, que permitía interactuar con un mundo de bloques en lenguaje natural, los ejemplos más conocidos. Para construir un sistema de este tipo es necesario desarrollar técnicas de procesamiento del lenguaje natural y estas técnicas han tenido siempre un lugar privilegiado dentro del campo de la inteligencia artificial, hasta el punto de que cuando Alan Turing planteó en los años 40 su famosa prueba (<em>Turing’s test</em>) consideró la capacidad de diálogo como la demostración última de que una máquina pudiera pensar.</p>
</section>
<section id="tecnicas-simbolicas-y-estadisticas">
<h3><span class="section-number">21.1.3. </span>Técnicas simbólicas y estadísticas<a class="headerlink" href="#tecnicas-simbolicas-y-estadisticas" title="Permalink to this headline">#</a></h3>
<p>Los años comprendidos entre la década de los 50 y la década de los 80 vieron toda una serie de avances en el campo, la mayoría de ellos dentro del terreno simbólico con sistemas que integraban reglas escritas por lingüistas. Las teorías acerca del lenguaje humano introducidas por Chomsky en los años 60 tuvieron una gran influencia en el desarrollo de estos formalismos. Sin embargo, hacia finales de la década de los 80 comienzan a aparecer sistemas competitivos basados en estadística y aprendizaje automático. En 1988 el investigador Frederik Jelinek enuncia su famosa frase, luego <a class="reference external" href="http://www.lrec-conf.org/lrec2004/doc/jelinek.pdf">matizada</a>, en relación a su sistema de reconocimiento de voz: «Every time I fire a linguist, my performance goes up». Además de contextualizar su afirmación, el mismo Jelinek reconoció posteriormente que las técnicas estadísticas pueden aplicarse en contextos híbridos en los que también tenga cabida el conocimiento lingüístico.</p>
<p>Las técnicas estadísticas (árboles de decisión, modelos ocultos de Markov, etc.) basadas en la explotación de grandes corpus de texto se ven propiciadas por la llegada de la web, que incrementa notablemente la cantidad de datos disponibles, y por el incremento en la capacidad de cómputo de los ordenadores. Pese a la predominancia de enfoques neuronales, las técnicas basadas en reglas o las estadísticas aún se usan hoy día en numerosos escenarios, como, por ejemplo, con lenguas con muy pocos recursos o para el pre o postprocesamiento de los textos usados en sistemas neuronales.</p>
</section>
<section id="los-primeros-modelos-neuronales">
<h3><span class="section-number">21.1.4. </span>Los primeros modelos neuronales<a class="headerlink" href="#los-primeros-modelos-neuronales" title="Permalink to this headline">#</a></h3>
<p>Los primeros modelos neuronales que procesaban el lenguaje humano en la década de los 90 se centraron en tareas relativamente sencillas como predecir el siguiente elemento de una secuencia (por ejemplo, el siguiente carácter dado el prefijo de una frase). Aunque en la década de los 90 ya aparecieron trabajos que usaban redes neuronales con lenguaje natural, no es hasta unos años después cuando empiezan a obtenerse resultados significativos. Un artículo de Bengio presentó un modelo neuronal que aprende a la vez representaciones distribuidas de las palabras de la entrada (lo que luego se conocería como <em>word embedding</em>) y un modelo probabilístico de la siguiente palabra a la salida. Este <a class="reference external" href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">trabajo</a> de 2003 ya tiene tamaños de vocabulario aceptables (probablemente inviables unos años antes) de en torno a las 16000 palabras. El entrenamiento de este modelo con millones de parámetros llevó varias semanas en un sistema con 40 CPUs y la perplejidad total del modelo resultante demostró ser más baja que la de modelos estadísticos consolidados tal como los basados en trigramas suavizados.</p>
<p>Ya en aquel trabajo se plantea que “polysemous words are probably not well served by the model presented here, which assigns to each word a single point in a continuous semantic space” y se anticipa la necesidad de que una palabra pueda tener más de una representación en base a su contexto. También se menciona la conveniencia de utilizar redes neuronales recurrentes en lugar de redes <em>feedforward</em>. Las arquitecturas recurrentes se habían usado intensamente en la década anterior para tareas sencillas. De hecho, las unidades LSTM, que sustituyen a las neuronas tradicionales en estos modelos para mitigar el problema del gradiente evanescente, se propusieron en el año 1997 por Hochreiter y Schmidhuber.</p>
</section>
<section id="el-entrenamiento-no-supervisado">
<h3><span class="section-number">21.1.5. </span>El entrenamiento no supervisado<a class="headerlink" href="#el-entrenamiento-no-supervisado" title="Permalink to this headline">#</a></h3>
<p>El tipo de entrenamiento del trabajo de Bengio se conoce como no supervisado, en el sentido de que no se necesitan etiquetar datos o asignar categorías durante el entrenamiento. Este tipo de aprendizaje fue defendido por Yann LeCun <a class="reference external" href="https://ruder.io/highlights-nips-2016/#generalartificialintelligence">en 2016</a> como una tarea muy importante en el camino hacia una inteligencia artificial de propósito general que pueda trabajar con el “sentido común”, que él define como la capacidad de predecir el pasado, presente o futuro de cualquier información disponible.</p>
<p>El entrenamiento no supervisado es una piedra angular en desarrollos como los word embeddings, los modelos preetrenados o los modelos de traducción de secuencias (por ejemplo, los usados en sistemas de traducción automática que no requieren corpus paralelo para su entrenamiento al explotar el concepto de <em>back-translation</em>).</p>
</section>
<section id="aprendizaje-multitarea">
<h3><span class="section-number">21.1.6. </span>Aprendizaje multitarea<a class="headerlink" href="#aprendizaje-multitarea" title="Permalink to this headline">#</a></h3>
<p>Otro elemento fundamental en los modelos de procesamiento del lenguaje natural es el de aprendizaje multitarea (<em>multi-task learning</em>) que ya propuso Caruana en la década de los 90. En el contexto actual de las redes neuronales, esto significa que parte o la totalidad de los parámetros es compartida entre redes que resuelven problemas diferentes, pero relacionados: como caso límite, una red puede entrenarse para clasificar textos entre diferentes idiomas o para búsqueda de respuestas, resumen automático y traducción automática a la vez de manera que se aprovechen los efectos sinérgicos de combinbar todas las tareas.</p>
</section>
<section id="word-embeddings">
<h3><span class="section-number">21.1.7. </span>Word embeddings<a class="headerlink" href="#word-embeddings" title="Permalink to this headline">#</a></h3>
<p>Aunque las representaciones distribuidas de palabras ya existían desde varios años antes, en 2013 Mikolov et al. muestran un método eficiente para su cálculo que dispara su uso. Los embeddings pueden calcularse mediante modelos complejos con grandes corpus de texto y luego emplearse en otras tareas. Existen colecciones como <a class="reference external" href="https://fasttext.cc/">fastText</a>, que incluyen embeddings para dos millones de palabras en inglés (obtenidos procesando un corpus de 16.000 millones de palabras) o embeddings multilingües para más de cien idiomas. También cabe aprender los embeddings para una tarea concreta, lo que consiste en aprender los valores de la tabla de <em>embeddings</em> como cualquier otro parámetro de la red (usualmente mediante descenso por gradiente estocástico). Los embeddings suelen tener propiedades geométricas interesantes (<em>Londres es a Reino Unido como París es a Francia</em>), pero también reflejan los sesgos de los textos en los que son entrenados (el embedding de <em>nurse</em> suele estar cerca del de <em>woman</em> y el <em>doctor</em> cerca de <em>man</em>).</p>
</section>
<section id="el-inicio-del-auge-de-las-redes-neuronales">
<h3><span class="section-number">21.1.8. </span>El inicio del auge de las redes neuronales<a class="headerlink" href="#el-inicio-del-auge-de-las-redes-neuronales" title="Permalink to this headline">#</a></h3>
<p>A partir de 2013 aproximadamente los modelos neuronales empiezan a ser considerados como una opción seria para ciertas tareas de procesamiento del lenguaje natural, pero las técnicas tradicionales todavía tienen mayor peso. En esos momentos, en el terreno del procesamiento de imagen, los modelos neuronales ya superan competición tras competición a los más tradicionales. Las redes neuronales convolucionales DanNet en 2011 y AlexNet en 2013, por poner dos ejemplos, superaron en varios puntos a modelos hasta entonces consolidados y obtuvieron resultados <em>superhumanos</em>  en múltiples tareas de procesamiento de imágenes. Un equivalente en el área del procesamiento del lenguaje natural no llegaría hasta aproximadamente 2018 con la aparición de ELMo y BERT.</p>
<p>Una de las principales ventajas de los modelos neuronales respecto a los estadísticos es que se elimina en la mayor parte de los casos la necesidad de explicitar, diseñar y extraer las características relevantes de los textos en base al problema a resolver. La extracción de características (<em>feature engineering</em>) implica el conocimiento de expertos. Las redes neuronales profundas, sin embargo, son capaces de trabajar con los textos en bruto sin apenas ningún preprocesamiento previo.</p>
</section>
<section id="modelos-recurrentes-sequence-to-sequence">
<h3><span class="section-number">21.1.9. </span>Modelos recurrentes sequence-to-sequence<a class="headerlink" href="#modelos-recurrentes-sequence-to-sequence" title="Permalink to this headline">#</a></h3>
<p>En 2014, Sutskever et al. presentan el modelo neuronal <em>sequence-to-sequence</em> que incluye dos redes neuronales (de una o más capas cada una; a más capas, más profunda se considera la representación aprendida) en un modelo conocido como codificador-descodificador (<em>encoder-decoder</em>), que es capaz de obtener una representación intermedia de la secuencia completa de entrada y “desenrollarla” a continuación en una nueva secuencia  de longitud no necesariamente igual a la de la de entrada. Aunque hay muchas tareas que se pueden especificar de esta forma (por ejemplo, el resumen de textos o la descripción textual automática de imágenes), el sistema es aplicado inicialmente a la traducción automática. En ciertos casos, el modelo supera la calidad de la traducción de los sistemas estadísticos (basados en encontrar las traducciones más probables para segmentos de varias palabras y combinarlas y reordenarlas teniendo en cuenta un modelo de lengua), que eran los dominantes hasta ese momento.</p>
</section>
<section id="el-mecanismo-de-atencion">
<h3><span class="section-number">21.1.10. </span>El mecanismo de atención<a class="headerlink" href="#el-mecanismo-de-atencion" title="Permalink to this headline">#</a></h3>
<p>Si bien el modelo <em>sequence-to-sequence</em> supuso un gran avance, su principal limitación es que comprime toda la secuencia de entrada en un único vector a partir del cuál se va generando toda la secuencia de salida. En el año siguiente, Bahdanau et al. perfeccionan el modelo introduciendo el mecanismo de <em>atención</em>: en lugar de obtener un vector único para la frase de entrada combinando los vectores de cada palabra, el descodificador es capaz de utilizar las representaciones individuales de todas las palabras de la secuencia de entrada; cuando el descodificador va a generar la predicción de la siguiente palabra de la secuencia de salida, decide el grado o porcentaje de influencia de cada representación de la entrada y de las representaciones de la salida generadas hasta ese momento. La atención supuso otro salto cuantitativo en el rendimiento de los modelos que lleva a los grandes proveedores de sistemas de traducción automática a migrar rápidamente en los meses siguientes a tecnologías neuronales.</p>
</section>
<section id="redes-neuronales-con-memoria-explicita">
<h3><span class="section-number">21.1.11. </span>Redes neuronales con memoria explícita<a class="headerlink" href="#redes-neuronales-con-memoria-explicita" title="Permalink to this headline">#</a></h3>
<p>A partir de 2014 comienzan a aparecer modelos neuronales avanzados (aunque los modelos simples datan de unas décadas antes) que integran de forma explícita una memoria de lectura/escritura en la que la red puede decidir almacenar ciertos vectores para su uso posterior. Los <em>computadores diferenciables neuronales</em> son ejemplo de este tipo de sistemas y son capaces de aprender algoritmos sencillos (ordenación o enrutamiento, por ejemplo) o trabajar en tareas en las que cierta información debe almacenarse durante largos periodos de tiempo.</p>
</section>
<section id="la-arquitectura-transformer">
<h3><span class="section-number">21.1.12. </span>La arquitectura transformer<a class="headerlink" href="#la-arquitectura-transformer" title="Permalink to this headline">#</a></h3>
<p>El mecanismo de atención no solo se puede aplicar a la situación de un modelo recurrente en el que el descodificador tiene que integrar con diferentes grados de atención la información aportada por el codificador. En 2017, de nuevo con un enfoque inicial en la traducción automática, aparece la arquitectura conocida como <em>transformer</em>, que elimina la necesidad de utilizar redes recurrentes (que arrastran el problema del gradiente evanescente que dificulta la detección de dependencias a largo plazo, como cuando un adjetivo al final de una frase larga concuerda con un sustantivo que aparece al principio de esta, y que, además, son difíciles de paralelizar ya que cada paso depende del anterior) al aplicar el concepto de <em>autoatención</em>: la representación de cada palabra en el codificador se obtiene integrando mediante mecanismos de atención las representaciones en la capa anterior de todas las palabras de la frase de entrada; del mismo modo, el descodificador usa la atención para determinar la influencia de las representaciones de la entrada y del prefijo de la salida generado hasta el momento en la representación de la palabra a generar y, por ende, en las probabilidades emitidas para la siguiente palabra de la salida.</p>
</section>
<section id="modelos-preentrenados">
<h3><span class="section-number">21.1.13. </span>Modelos preentrenados<a class="headerlink" href="#modelos-preentrenados" title="Permalink to this headline">#</a></h3>
<p>Aunque ya habían sido propuestos con anterioridad, en el año 2018 comienza la <em>revolución</em> de los llamados <em>modelos preentrenados</em>. Las representaciones vectoriales obtenidas por un codificador en sus diferentes capas pueden considerarse como <em>embeddings</em> contextuales de cada palabra. Si estos embeddings son representativos (es decir, sin son entrenados con suficientes cantidades de texto) pueden usarse para codificar la frase de entrada en redes neuronales que resuelvan tareas como detección de sentimiento, búsqueda de respuestas o inferencia en lenguaje natural. Pero los embeddings aprendidos por el codificador de un sistema de traducción automática tienen un par de inconvenientes:</p>
<ol class="simple">
<li><p>están condicionados por la lengua meta, ya que se entrenan para generar buenas traducciones por lo que no son monolingües; por ejemplo, el codificador de español aprendido para un traductor español-inglés puede representar con embeddings similares la palabra <em>canal</em> de una oración que habla sobre televisión y la palabra <em>canal</em> en una oración que habla sobre cauces de agua, simplemente porque en ambos casos la traducción es <em>channel</em>;</p></li>
<li><p>para obtener estos embeddings es necesario entrenar un sistema de traducción automática con grandes cantidades de corpus bilingües y hay muchas lenguas para las que no existe este tipo de información supervisada en cantidad suficiente.</p></li>
</ol>
<p>Por ello, los sistemas preentrenados se obtienen mediante tareas no supervisadas. Los primeros de estos modelos como ELMo entrenaban el sistema para que predijera la siguiente palabra, pero los sistemas posteriores como BERT plantean una tarea no supervisada más difícil que permite obtener representaciones más complejas y elaboradas; esta tarea, conocida como <em>mask filling</em> consiste en sustituir aleatoriamente algunas palabras de la entrada por una marca especial (<em>mask</em>) y enseñar a la red a maximizar la probabilidad de las palabras originales a la salida. Existen múltiples variaciones de esta estrategia no supervisada de entrenamiento, pero su planteamiento básico es similar a este.</p>
<p>El uso de un modelo preentrenado en una tarea concreta de procesamiento del lenguaje natural pasa por añadir una o más capas a la salida del modelo preentrenado y entrenar la salida del modelo ampliado para la tarea en cuestión, lo que se denomina <em>ajuste fino</em> (<em>fine-tuning</em>). Los pesos de la parte correspondiente al modelo preetrenado suelen dejarse congelados, pero también podrían ajustarse o incorporar entre las capas los llamados <em>adaptadores</em>. La arquitectura de un modelo preentrenado (por ejemplo, BERT) es como la del codificador de un transformer cuando el modelo resultante se va a usar para tareas de clasificación de secuencias; la arquitectura es como la de un transformer completo (por ejemplo, BART o T5, o sus versiones multilingües mBART o mT5) cuando el modelo resultante se va a usar para tareas que generan secuencias a partir de secuencias. Hay también modelos preentrenados para tareas específicas: así, para la continuación de secuencias la arquitectura de GPT-3 usa el descodificador de un transformer; y M2M es un transformer entrenado sobre decenas de corpus bilingües de múltiples pares de idiomas.</p>
</section>
<section id="el-modelo-gpt-3">
<h3><span class="section-number">21.1.14. </span>El modelo GPT-3<a class="headerlink" href="#el-modelo-gpt-3" title="Permalink to this headline">#</a></h3>
<p>El modelo comercial <a class="reference external" href="https://arxiv.org/abs/2005.14165">GPT-3</a> aparecido a mediados de 2020 es la evolución de GPT-2 publicado apenas un año antes y que ya mostraba <a class="reference external" href="https://openai.com/blog/better-language-models/#samples">resultados</a> sorprendentes para ese momento. GPT-3 fue entrenado con 400.000 millones de <em>tokens</em>; para un cálculo aproximado podemos asumir que un <em>token</em> equivale a una palabra y tener en cuenta que los tres volúmenes de «El Señor de los Anillos» tienen en torno a medio millón de palabras y que toda la serie de Harry Potter tiene aproximadamente un millón de palabras. Se trata de un modelo autoregresivo similar al descodificador de un transformer, entrenado para predecir el siguiente token de la secuencia y generar representaciones profundas en los cerca de 175.000 millones de parámetros del modelo más grande (existen versiones del sistema de tamaños inferiores; en cualquier caso, en pocos meses aparecieron otros modelos con un número de parámetros un orden de magnitud superior). El consumo necesario para entrenar el modelo grande se estima en unos 350 años de una GPU V100, lo que, al precio actual en plataformas en la nube supondría unos 5 millones de dólares. El sistema GPT-3 obtiene resultados interesantes en tareas para las que no ha sido explícitamente entrenado (el trabajo original no presentaba resultados tras hacer <em>fine-tuning</em>) como traducir textos, jugar al ajedrez, responder a preguntas de sentido común o escribir código, simplemente dándole algo de contexto previo (los primeros movimientos de una partida o algunos ejemplos de traducciones, por ejemplo) y obteniendo la continuación dada por el modelo. Esta situación en la que un sistema se somete a tareas para las que no ha sido entrenado se conoce como evaluación <em>zero-shot</em>. Los autores de GPT-3, en cualquier caso, reservan <em>zero-shot</em> para el caso en el que al sistema no se le da ningún contexto previo, <em>one-shot</em> cuando este incluye un único elemento (un movimiento de ajedrez o un ejemplo de traducción) y <em>few-shot</em> cuando el contexto previo es mayor. Una variación del modelo GPT-3 bautizada como <a class="reference external" href="https://openai.com/blog/dall-e/">Dall·e</a> es capaz de generar imágenes a partir de descripciones textuales.</p>
</section>
<section id="corpus-disponibles">
<h3><span class="section-number">21.1.15. </span>Corpus disponibles<a class="headerlink" href="#corpus-disponibles" title="Permalink to this headline">#</a></h3>
<p>En los últimos años han ido apareciendo corpus monolingües o multilingües de tamaño cada vez mayor. Una tarea importante es la de filtrar su contenido para eliminar textos con ruido o en idiomas no deseados. Las lenguas con pocos recursos, no obstante, siguen padeciendo una falta importante de recursos lingüísticos y textuales, aunque diferentes iniciativas intentan paliar este problema. Los modelos multilingües (como mBART o mT5) son entrenados con textos de decenas de idiomas usando vocabularios compartidos y submuestreando los datos de los idiomas con más recursos. Aunque para el caso de los idiomas con más recursos el sistema multilingüe resultante suele presentar un rendimiento inferior al de sistemas monolingües específicos, las lenguas con pocos recursos se benefician de cierta <em>universalidad</em> en los patrones lingüísticos. En este sentido son también destacables proyectos como <em>universal dependencies</em> que pretende definir etiquetarios universales y elaborar colecciones de textos anotados para todas las lenguas de la Tierra.</p>
<p>Los modelos multilingües de traducción automática son incluso capaces de conseguir cierto grado de comportamiento <em>zero-shot</em> al permitir la traducción básica entre idiomas con los que no han sido entrenados si el corpus de entrenamiento contiene textos de lenguas similares.</p>
</section>
<section id="modelos-de-subpalabras">
<h3><span class="section-number">21.1.16. </span>Modelos de subpalabras<a class="headerlink" href="#modelos-de-subpalabras" title="Permalink to this headline">#</a></h3>
<p>Un cuello de botella de los modelos neuronales es el elevado tamaño de la matriz de embeddings o el cálculo de la función softmax en la capa de salida. Ambos están en función del tamaño del vocabulario. Para aligerarlo, se propuso el uso de unidades más pequeñas que la palabra, denominadas habitualmente <em>tokens</em>. Por ejemplo, si en lugar de tener un embedding para cada forma del presente de indicativo de los <em>cantar</em>, <em>bailar</em>, <em>danzar</em> y <em>pintar</em>, usamos representaciones para <em>cant</em>, <em>bail</em>, <em>danz</em> <em>pint</em>, <em>o</em>, <em>as</em>, <em>a</em>, <em>amos</em> <em>áis</em> y <em>an</em> habremos reducido el tamaño del vocabulario de 24 a 10. Estos tokens que representan unidades conocidas como subpalabras se obtienen mediante sencillas técnicas estadísticas de conteo como BPE o SentencePiece.</p>
</section>
<section id="aplicaciones">
<h3><span class="section-number">21.1.17. </span>Aplicaciones<a class="headerlink" href="#aplicaciones" title="Permalink to this headline">#</a></h3>
<p>Algunas tareas de bajo nivel típicas del procesamiento del lenguaje natural son la lematización, la segmentación morfológica, el etiquetado de partes de la oración, la inducción de gramáticas, el análisis sintáctico, la separación de un texto en oraciones, el reconocimiento de entidades nombradas, la extracción terminológica, la desambiguación del sentido de las palabras, la extracción de relaciones, el análisis semántico o la resolución de coreferencias. Aplicaciones de más alto nivel incluyen la obtención automática de resúmenes, el análisis de sentimiento u opinión, la generación de textos, los agentes conversacionales, los correctores ortográficos o de estilo, la traducción automática, la búsqueda de respuestas o la modelización del sentido común, entre otros.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="bloque2.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">20. </span>Técnicas para la minería de textos</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="bloque2_embeddings.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">22. </span>Representaciones de palabras y oraciones</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Universitat d'Alacant<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>