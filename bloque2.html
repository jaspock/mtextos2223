
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>22. Técnicas para la minería de textos &#8212; Minería de Textos</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/estilos.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="23. Content in Jupyter Book" href="content.html" />
    <link rel="prev" title="21. Ev. Evaluación de prácticas del Bloque 2" href="bloque3_ev.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo-master-ca.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Minería de Textos</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Materiales de Minería de Textos
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1.html">
   1. Introducción a la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_1Introduccion.html">
   2. Minería de textos y procesamiento del lenguaje natural.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_2CategorialSintactico.html">
   3. Análisis categorial y sintáctico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica1.html">
   4. Práctica 1a: PLN con SpaCy.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_3AnalisisSemantico.html">
   5. Análisis semántico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_4AnalisisSemanticoVectorial.html">
   6. Análisis semántico vectorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica2.html">
   7. Práctica 1b :
   <em>
    Topic modeling
   </em>
   .
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3.html">
   8. Aplicaciones de la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t1_aplicaciones.html">
   9. T1. Aplicaciones generales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2_subaplicaciones-benchmarks.html">
   10. T2. Aplicaciones específicas y Benchmacks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2.1_analisis_sentimientos.html">
   11. T2.1. Aplicaciones específicas. Análisis de Sentimientos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t3.1_metricas.html">
   12. T3. Métricas de Evaluación
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t4_centralizacion.html">
   13. T4. Centralización de datasets y modelos: Huggingface, OpenAI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5_automl.html">
   14. T5. Auto Machine Learning(AutoML)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5.1_autogoal.html">
   15. T5.1. AutoGOAL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p1_SA-Pipeline-Reviews.html">
   16. P1.1. Pipeline simple
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p2_SA-Transformers-Basic.html">
   17. P1.2. APIs Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p3_SA-Transformers-Training-FineTuning.html">
   18. P2. Reajustar modelos Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p4_SA-Transformers-Training-Custom.html">
   19. P3. Composición de vectores de características
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p5-SA-Ensemble.html">
   20. P4. Ensemble de pipelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_ev.html">
   21. Ev. Evaluación de prácticas del Bloque 2
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 3
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   22. Técnicas para la minería de textos
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Extras
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="content.html">
   23. Content in Jupyter Book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markdown.html">
   24. Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   25. Content with notebooks
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/bloque2.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practicas-a-entregar-para-este-bloque">
   22.1. Prácticas a entregar para este bloque
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#primera-sesion-29-de-marzo-de-2023">
   22.2. Primera sesión (29 de marzo de 2023)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#segunda-sesion-26-de-abril-de-2023">
   22.3. Segunda sesión (26 de abril de 2023)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tercera-sesion-10-de-mayo-de-2023">
   22.4. Tercera sesión (10 de mayo de 2023)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practica-sobre-interpretabilidad-mecanicista-de-transformers">
   22.5. Práctica sobre interpretabilidad mecanicista de transformers
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Técnicas para la minería de textos</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practicas-a-entregar-para-este-bloque">
   22.1. Prácticas a entregar para este bloque
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#primera-sesion-29-de-marzo-de-2023">
   22.2. Primera sesión (29 de marzo de 2023)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#segunda-sesion-26-de-abril-de-2023">
   22.3. Segunda sesión (26 de abril de 2023)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#tercera-sesion-10-de-mayo-de-2023">
   22.4. Tercera sesión (10 de mayo de 2023)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#practica-sobre-interpretabilidad-mecanicista-de-transformers">
   22.5. Práctica sobre interpretabilidad mecanicista de transformers
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="tecnicas-para-la-mineria-de-textos">
<span id="label-tecnicas"></span><h1><span class="section-number">22. </span>Técnicas para la minería de textos<a class="headerlink" href="#tecnicas-para-la-mineria-de-textos" title="Permalink to this headline">#</a></h1>
<p>En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio Pérez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor logístico, que nos servirá para asentar los conocimientos necesarios para entender posteriores modelos. A continuación se estudia con cierto nivel de detalle <em>skip-grams</em>, uno de los algoritmos para la obtención de <em>embeddings</em> incontextuales de palabras. Después se repasa el funcionamiento de las arquitecturas neuronales <em>feedforward</em> y se estudia su aplicación a modelos de lengua. El objetivo último es abordar el estudio de la arquitectura más importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un análisis del funcionamiento de los modelos preentrenados (modelos fundacionales), en general, y de los modelos de lengua, en particular.</p>
<p>Los materiales de clase complementan la lectura de algunos capítulos de un libro de texto (“Speech and Language Processing” de Dan Jurafsky y James H. Martin, borrador de la tercera edición, disponible online) con anotaciones realizadas por el profesor.</p>
<section id="practicas-a-entregar-para-este-bloque">
<h2><span class="section-number">22.1. </span>Prácticas a entregar para este bloque<a class="headerlink" href="#practicas-a-entregar-para-este-bloque" title="Permalink to this headline">#</a></h2>
<p>Durante las sesiones de este bloque, estudiaremos diferentes implementaciones en PyTorch de modelos neuronales para procesar textos. Para cada ejemplo de código, excepto el último, has de entregar un notebook con el código original y pequeños bloques de texto con tus comentarios explicando el código en base a lo que has aprendido sobre el tema y sobre PyTorch. Entrega todos los notebooks en forma de enlaces de Google Colab a través de una tutoría de UACloud. Crea los cuadernos de Google Colab con tu cuenta de <code class="docutils literal notranslate"><span class="pre">gcloud.ua.es</span></code> y compártelos con la cuenta del profesor que te indicará en clase. Para el último bloque de código (implementación del transformer), tendrás que complementar el cuaderno con código propio para realizar una tarea adicional y ajuntar un informe detallado dentro del cuaderno. El <strong>plazo de entrega acaba el 31 de mayo de 2023</strong> a las 23.59 horas. Las prácticas se pueden hacer en parejas. Recuerda que hay un examen final de la asignatura, por lo que es muy recomendable que ambos miembros del equipo se impliquen de igual manera.</p>
</section>
<section id="primera-sesion-29-de-marzo-de-2023">
<h2><span class="section-number">22.2. </span>Primera sesión (29 de marzo de 2023)<a class="headerlink" href="#primera-sesion-29-de-marzo-de-2023" title="Permalink to this headline">#</a></h2>
<p><strong><span style="font-size: 1.15em">Contenidos a preparar antes de la sesión del 29/03/2023</span></strong></p>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul class="simple">
<li><p>Lectura y estudio de los contenidos de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/regresor">esta página</a> sobre regresión logística. Puedes saltar por ahora el apartado de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch">implementación en PyTorch</a>, ya que será el eje central de la clase presencial. Como verás, la página te indica qué contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo propósito es ayudarte a entender los conceptos clave del capítulo. Después, realiza una segunda lectura del capítulo del libro. En total, esta parte debería llevarte unas 3 horas 🕒️ de trabajo.</p></li>
<li><p>Visionado y estudio de los tutoriales en vídeo de esta <a class="reference external" href="https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN">playlist oficial de PyTorch</a>.  Estudia al menos los 4 primeros vídeos (“Introduction to PyTorch”, “Introduction to PyTorch Tensors”, “The Fundamentals of Autograd” y “Building Models with PyTorch”). En total, esta parte debería llevarte unas 2 horas 🕒️ de trabajo.</p></li>
<li><p>Tras acabar con las dos partes anteriores, realiza este <a class="reference external" href="https://forms.gle/E1xzZHw6hzMWJaNr7">test de evaluación</a> de estos contenidos. Son pocas preguntas y te llevará unos minutos.</p></li>
</ul>
<p><strong><span style="font-size: 1.15em">Contenidos para la sesión presencial del 29/03/2023</span></strong></p>
<p>En la clase presencial (3 horas 🕒️ de duración), repasaremos los contenidos de la semana anterior y veremos cómo se implementa un regresor logístico en PyTorch siguiendo la implementación de un regresor logístico binario y de uno multinomial que se comentan en <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch">este apartado</a>.</p>
<p>La idea es que vayas creando una serie de notebooks en Google Colab en los que incluyas y comentes cada uno de los programas que vamos a ir viendo. En la última clase se presentará una práctica más avanzada que implicará modificar el código del transformer.</p>
<p><em>Nota:</em> por si te es de utilidad, tienes una copia del código que veremos en este bloque en <a class="reference external" href="https://drive.google.com/drive/folders/1W47uSa0ddxalj9OWQIKk1mRYJ7xq6ftv?usp=sharing">esta carpeta de Google Drive</a> (accede con tu cuenta de <code class="docutils literal notranslate"><span class="pre">gcloud.ua.es</span></code>).</p>
</section>
<section id="segunda-sesion-26-de-abril-de-2023">
<h2><span class="section-number">22.3. </span>Segunda sesión (26 de abril de 2023)<a class="headerlink" href="#segunda-sesion-26-de-abril-de-2023" title="Permalink to this headline">#</a></h2>
<p>Entre la sesión anterior y la del 26 de abril transcurren varias semanas, por lo que la carga de trabajo es mayor que en la sesión anterior.</p>
<p><strong><span style="font-size: 1.15em">Contenidos a preparar antes de la sesión del 26/04/2023</span></strong></p>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul class="simple">
<li><p>Lectura y estudio de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/embeddings">esta página</a> sobre la obtención de embeddings incontextuales. Puedes saltar de nuevo el apartado de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/embeddings#implementaci%C3%B3n-en-pytorch">implementación en PyTorch</a>, ya que se estudiará en la próxima clase presencial. Como verás, la página te indica qué contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo objetivo es ayudarte a entender los conceptos clave del capítulo. Después, realiza una segunda lectura del capítulo. En total, esta parte debería llevarte unas 4 horas 🕒️ de trabajo.</p></li>
<li><p>Lectura y estudio de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/ffw">esta página</a> sobre las redes neuronales hacia delante. Puedes saltar también aquí el apartado de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/ffw#implementaci%C3%B3n-en-pytorch">implementación en PyTorch</a>, ya que se estudiará también en la próxima clase presencial. En total, esta parte debería llevarte unas 3 horas 🕒️ de trabajo.</p></li>
<li><p>Primeros pasos en el estudio del modelo transformer. Volveremos a dedicar más horas a esta arquitectura para la próxima sesión de forma que la abordaremos en dos fases. Por ahora, lee con detenimiento la introducción a mecanismos de atención de <a class="reference external" href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">“Visualizing A Neural Machine Translation Model”</a>, así como la introducción visual a los transformers de <a class="reference external" href="http://jalammar.github.io/illustrated-transformer/">“The Illustrated Transformer”</a> y la más elaborada de <a class="reference external" href="https://jalammar.github.io/illustrated-gpt2/">“The Illustrated GPT-2”</a>. A continuación, lee el apartado 9.7 (solo este apartado) del capítulo <a class="reference external" href="https://web.archive.org/web/20221216193204/https://web.stanford.edu/~jurafsky/slp3/9.pdf">“Deep learning architectures for sequence processing”</a>; el objetivo es que entiendas conceptualmente el mecanismo de atención de los transformers, pero no es necesario que en este momento comprendas todos los detalles técnicos (especialmente las ecuaciones del modelo), ya que volverás a dedicarle tiempo a este capítulo más adelante. En total, esta parte debería llevarte ahora unas 4 horas 🕒️ de trabajo.</p></li>
<li><p>Realización del <a class="reference external" href="https://forms.gle/Eb3ZwwGxbQp88t4FA">test de evaluación</a> de estos contenidos. Son pocas preguntas y te llevará unos minutos.</p></li>
</ul>
<p><strong><span style="font-size: 1.15em">Contenidos para la sesión presencial del 26/04/2023</span></strong></p>
<p>En la clase presencial (3 horas 🕒️ de duración), repasaremos los contenidos de la semana anterior y veremos sendas implementaciones en PyTorch del algoritmo <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/embeddings#implementaci%C3%B3n-en-pytorch">skip-grams</a> y de un modelo de lengua basado en <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/ffw#implementaci%C3%B3n-en-pytorch">redes feedforward</a>.</p>
</section>
<section id="tercera-sesion-10-de-mayo-de-2023">
<h2><span class="section-number">22.4. </span>Tercera sesión (10 de mayo de 2023)<a class="headerlink" href="#tercera-sesion-10-de-mayo-de-2023" title="Permalink to this headline">#</a></h2>
<p><strong><span style="font-size: 1.15em">Contenidos a preparar antes de la sesión del 10/05/2023</span></strong></p>
<p>Las actividades a realizar antes de esta clase son:</p>
<ul class="simple">
<li><p>Afianzar el estudio de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/attention">esta página</a> sobre el modelo transformer y el capítulo correspondiente del libro. En realidad, ya estudiaste para la sesión anterior todos estos conceptos, pero se te pidió que no te detuvieras en exceso en los detalles técnicos del libro. Ahora, es el momento de que vuelvas a leerlo con más calma y consultes también las anotaciones del profesor que hay en la página web. Puedes saltar de nuevo el apartado de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/attention#implementaci%C3%B3n-en-pytorch">implementación en PyTorch</a>, ya que se estudiará en la próxima clase presencial. En total, esta parte debería llevarte unas 4 horas 🕒️ de trabajo.</p></li>
<li><p>Visualizar el <a class="reference external" href="https://youtu.be/kCc8FmEb1nY">vídeo</a> que introduce las ideas principales de la implementación del transformer que estudiaremos en la clase presencial. Pausa el vídeo y vuelve atrás cuando sea necesario para entender los conceptos clave. En total, esta parte debería llevarte unas 2 horas 🕒️ de trabajo.</p></li>
</ul>
<p><strong><span style="font-size: 1.15em">Contenidos para la sesión del 10/05/2023</span></strong></p>
<p>En la clase presencial (3 horas 🕒️ de duración), repasaremos los contenidos de la semana anterior y veremos cómo se <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/attention#implementaci%C3%B3n-en-pytorch">implementa el modelo transformer en PyTorch</a>. En esta clase, además, se presentará la parte final de la práctica a realizar en base al código del transformer.</p>
<p>Del código del transformer solo has de comentar en un cuaderno las clases <code class="docutils literal notranslate"><span class="pre">CausalSelfAttention</span></code> y <code class="docutils literal notranslate"><span class="pre">Block</span></code>, así como los métodos <code class="docutils literal notranslate"><span class="pre">forward</span></code>, <code class="docutils literal notranslate"><span class="pre">generate</span></code>, <code class="docutils literal notranslate"><span class="pre">__init__</span></code>, <code class="docutils literal notranslate"><span class="pre">_init_weights</span></code> y <code class="docutils literal notranslate"><span class="pre">get_default_config</span></code> de la clase <code class="docutils literal notranslate"><span class="pre">GPT</span></code>. Puedes añadir un pequeño código que use las clases del modelo. Haz la práctica final que se menciona a continuación en un cuaderno diferente.</p>
<p><strong><span style="font-size: 1.15em">Contenidos prácticos a trabajar tras la sesión</span></strong></p>
<p>Tras la sesión, ya puedes ponerte a trabajar en la práctica de desarrollo a realizar para este bloque y de la cual saldrá la mayor parte de la nota del bloque (un 90% aproximadamente). Se espera que dediques a ella unas 11 horas 🕒️ de trabajo. La práctica se basa en modificar ligeramente el código de <a class="reference external" href="https://jaspock.github.io/me/materials/transformers/attention#implementaci%C3%B3n-en-pytorch">minGPT</a> para poder realizar experimentos de interpretabilidad mecanicista. El enunciado completo está en el siguiente apartado.</p>
</section>
<section id="practica-sobre-interpretabilidad-mecanicista-de-transformers">
<h2><span class="section-number">22.5. </span>Práctica sobre interpretabilidad mecanicista de transformers<a class="headerlink" href="#practica-sobre-interpretabilidad-mecanicista-de-transformers" title="Permalink to this headline">#</a></h2>
<p>La <em>interpretabilidad mecanicista</em> en el contexto de la inteligencia artificial intenta dar una explicación motivada del funcionamiento de los modelos de aprendizaje automático. Es una propuesta muy importante de cara a generar confianza en los sistemas e inducir ciertos comportamientos en ellos. Dentro del campo de la interpretabilidad mecanicista existen un buen número de técnicas que se pueden aplicar a los transformers. Aquí nos centraremos en el <a class="reference external" href="https://dynalist.io/d/n2ZWtnoYHrU1s4vnFSAQ519J#z=qeWBvs-R-taFfcCq-S_hgMqx">parcheado de activaciones</a>.</p>
<p>El parcheado de activaciones <em>interviene</em> en una activación específica de un modelo mediante la sustitución de una activación <em>corrompida</em> con una activación <em>limpia</em>. Se mide entonces cómo afecta el cambio a la salida del modelo. Esto nos permite identificar qué activaciones son importantes para el resultado del modelo y localizar posibles causas de errores en la predicción.</p>
<p>En nuestro caso particular, vas a escribir código que ejecute la versión más pequeña de GPT2 (usa la cadena <code class="docutils literal notranslate"><span class="pre">gpt2</span></code> en el código) con dos entradas diferentes: dos textos que solo se diferencien en un único token. La idea es que al proporcionar al modelo la entrada corrompida, intervendremos en el embedding tras una cierta capa (uno solo cada vez) y lo parchearemos con el embedding correspondiente de la ejecución limpia. Luego mediremos cuánto cambia la predicción del siguiente token respecto a la ejecución limpia. Si el cambio es significativo, entonces podemos estar seguros de que la activación que hemos alterado es importante para la predicción. Este proceso de parcheado lo realizaremos para cada capa del modelo y para cada token de la entrada. Con toda esta información, obtendremos una gráfica y sacaremos conclusiones. Por motivos que entenderás en un momento, los dos textos han de tener el mismo número de tokens.</p>
<p><strong><span style="font-size: 1.15em">Ejemplo de análisis</span></strong></p>
<p>Daremos un ejemplo para que se entienda mejor. Considera el siguiente texto de entrada: “Michelle Jones was a top-notch student. Michelle”. Si se lo damos a GPT2 y estudiamos la probabilidad emitida por el modelo para el token que sigue a la segunda aparición de Michelle, obtendremos lo siguiente (solo se muestran los 20 tokens más probables):</p>
<table class="table">
<colgroup>
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Position</p></th>
<th class="head"><p>Token index</p></th>
<th class="head"><p>Token</p></th>
<th class="head"><p>Probability</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>1</p></td>
<td><p>373</p></td>
<td><p>was</p></td>
<td><p>0.1634</p></td>
</tr>
<tr class="row-odd"><td><p>2</p></td>
<td><p>5437</p></td>
<td><p>Jones</p></td>
<td><p>0.1396</p></td>
</tr>
<tr class="row-even"><td><p>3</p></td>
<td><p>338</p></td>
<td><p>‘s</p></td>
<td><p>0.0806</p></td>
</tr>
<tr class="row-odd"><td><p>4</p></td>
<td><p>550</p></td>
<td><p>had</p></td>
<td><p>0.0491</p></td>
</tr>
<tr class="row-even"><td><p>5</p></td>
<td><p>318</p></td>
<td><p>is</p></td>
<td><p>0.0229</p></td>
</tr>
<tr class="row-odd"><td><p>6</p></td>
<td><p>290</p></td>
<td><p>and</p></td>
<td><p>0.0227</p></td>
</tr>
<tr class="row-even"><td><p>7</p></td>
<td><p>11</p></td>
<td><p>,</p></td>
<td><p>0.0222</p></td>
</tr>
<tr class="row-odd"><td><p>8</p></td>
<td><p>531</p></td>
<td><p>said</p></td>
<td><p>0.0134</p></td>
</tr>
<tr class="row-even"><td><p>9</p></td>
<td><p>468</p></td>
<td><p>has</p></td>
<td><p>0.0120</p></td>
</tr>
<tr class="row-odd"><td><p>10</p></td>
<td><p>635</p></td>
<td><p>also</p></td>
<td><p>0.0117</p></td>
</tr>
<tr class="row-even"><td><p>11</p></td>
<td><p>1625</p></td>
<td><p>came</p></td>
<td><p>0.0091</p></td>
</tr>
<tr class="row-odd"><td><p>12</p></td>
<td><p>1297</p></td>
<td><p>told</p></td>
<td><p>0.0084</p></td>
</tr>
<tr class="row-even"><td><p>13</p></td>
<td><p>1422</p></td>
<td><p>didn</p></td>
<td><p>0.0070</p></td>
</tr>
<tr class="row-odd"><td><p>14</p></td>
<td><p>2993</p></td>
<td><p>knew</p></td>
<td><p>0.0067</p></td>
</tr>
<tr class="row-even"><td><p>15</p></td>
<td><p>1816</p></td>
<td><p>went</p></td>
<td><p>0.0061</p></td>
</tr>
<tr class="row-odd"><td><p>16</p></td>
<td><p>561</p></td>
<td><p>would</p></td>
<td><p>0.0061</p></td>
</tr>
<tr class="row-even"><td><p>17</p></td>
<td><p>3111</p></td>
<td><p>worked</p></td>
<td><p>0.0055</p></td>
</tr>
<tr class="row-odd"><td><p>18</p></td>
<td><p>750</p></td>
<td><p>did</p></td>
<td><p>0.0054</p></td>
</tr>
<tr class="row-even"><td><p>19</p></td>
<td><p>2486</p></td>
<td><p>Obama</p></td>
<td><p>0.0053</p></td>
</tr>
<tr class="row-odd"><td><p>20</p></td>
<td><p>2492</p></td>
<td><p>wasn</p></td>
<td><p>0.0050</p></td>
</tr>
</tbody>
</table>
<p>Como era de esperar, el token “Jones” tiene una probabilidad notablemente elevada. Ahora, considera la entrada corrompida “Michelle Smith was a top-notch student. Michelle”. Si le damos esta entrada a GPT2, esperamos que la probabilidad de “Jones” a como continuación del texto sea mucho menor que antes y que la de “Smith” sea mucho mayor, lo que (puedes comprobarlo) efectivamente ocurre. Pero queremos ir más allá y saber qué embeddings son los que más influyen en esta diferencia. Dado que ambas entradas tienen 11 tokens (más adelante explicaremos cómo averiguarlo) y que el transformer del modelo GPT2 pequeño tiene 12 capas, si nos centramos en los embeddings que se obtienen a la salida de cada capa, podemos parchear 11×12 = 132 embeddings diferentes. Calcularemos, por tanto, 132 veces la diferencia entre el logit de “Smith” y el logit de “Jones” en la salida del último token de la entrada (“Michelle”) en el modelo corrompido. Observa que también podríamos calcular las diferencias tras aplicar la función softmax, pero en este caso no lo haremos.</p>
<p>Una representación en forma de mapa de calor del resultado es la siguiente:</p>
<figure class="align-default" id="fig-mech">
<a class="reference internal image-reference" href="_images/mechanistic-michelle.png"><img alt="_images/mechanistic-michelle.png" src="_images/mechanistic-michelle.png" style="height: 540px;" /></a>
</figure>
<p>Recuerda que en un gráfico como el anterior, debido a la máscara de atención y a la disposición de las capas, la información fluye de izquierda a derecha y de arriba a abajo. Puedes ver cómo intervenir en la primera columna no tiene efectos en la predicción del siguiente token, lo que tiene todo el sentido, ya que los embeddings que se parchean tienen exactamente los mismos valores en el modelo limpio y en el corrompido, ya que el contexto anterior es el mismo. Tampoco parece haber cambios al parchear los embeddings de la tercera a la antepenúltima columna. Sin embargo, observa cómo al intervenir los embeddings de muchas capas del segundo token, la predicción se decanta hacia “Jones” (el color se hace oscuro cuando la diferencia entre el logit de “Smith” y el de “Jones” se va haciendo negativa porque “Jones” tiene un logit mayor). Modificar los embeddings de las últimas capas del segundo token tiene efectos mucho menores, ya que el embedding apenas puede influir en el futuro de la secuencia. En la última posición (“Michelle”) se observa que los embeddings de las capas finales van anticipándose al token que tienen que predecir.</p>
<p>Algunos textos corrompidos adicionales que puede ser interesante explorar son, por ejemplo, “Jessica Jones was a top-notch student. Michelle” o “Michelle Smith was a top-notch student. Jessica”.</p>
<p>En esta práctica se trata de que programes el código que te permite obtener gráficas y probabilidades como las anteriores, propongas tus propios textos limpios y corrompidos (intenta tirar de creatividad y no estudiar textos o fenómenos muy similares), realices un análisis parecido al anterior y escribas un informe dentro de un cuaderno de Python de unas 1500-2000 palabras en el que presentes y comentes el código que has implementado, además de presentar tu enfoque, los resultados y las conclusiones pertinentes. Serán bienvenidas las ideas originales y los experimentos adicionales que se te ocurran.</p>
<p><strong><span style="font-size: 1.15em">Tokenization</span></strong></p>
<p>El modelo GPT2 usa un tokenizador basado en BPE que trocea el texto de entrada en palabras o en unidades inferiores dependiendo de su frecuencia. El código de minGPT permite descargar dicho tokenizador y usarlo para segmentar los textos. El siguiente código muestra cómo tokenizar un texto para obtener sus índices y viceversa.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">mingpt.bpe</span> <span class="kn">import</span> <span class="n">BPETokenizer</span>

<span class="nb">input</span> <span class="o">=</span> <span class="s2">&quot;Michelle Jones was a top-notch student. Michelle&quot;</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input:&quot;</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">bpe</span> <span class="o">=</span> <span class="n">BPETokenizer</span><span class="p">()</span>
<span class="c1"># bpe() gets a string and returns a 2D batch tensor </span>
<span class="c1"># of indices with shape (1, input_length)</span>
<span class="n">tokens</span> <span class="o">=</span> <span class="n">bpe</span><span class="p">(</span><span class="nb">input</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Tokenized input:&quot;</span><span class="p">,</span> <span class="n">tokens</span><span class="p">)</span>
<span class="n">input_length</span> <span class="o">=</span> <span class="n">tokens</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Number of input tokens:&quot;</span><span class="p">,</span> <span class="n">input_length</span><span class="p">)</span>
<span class="c1"># bpe.decode gets a 1D tensor (list of indices) and returns a string</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Detokenized input from indices:&quot;</span><span class="p">,</span> <span class="n">bpe</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens</span><span class="p">))</span>  
<span class="n">tokens_str</span> <span class="o">=</span> <span class="p">[</span><span class="n">bpe</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">token</span><span class="p">]))</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Detokenized input as strings: &quot;</span> <span class="o">+</span> <span class="s1">&#39;/&#39;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">tokens_str</span><span class="p">))</span>
</pre></div>
</div>
<p><strong><span style="font-size: 1.15em">Detalles de implementación</span></strong></p>
<p>Lo siguiente son algunos detalles de implementación que te pueden ser útiles, pero que no es necesario que sigas.</p>
<p>Para conseguir un código que te permita realizar el parcheado de activaciones te tendrás que centrar en los ficheros <code class="docutils literal notranslate"><span class="pre">mingpt/model.py</span></code> y <code class="docutils literal notranslate"><span class="pre">generate.ipynb</span></code>. Si trabajas en local sin usar un <em>notebook</em> (recomendado) copia el código de <code class="docutils literal notranslate"><span class="pre">generate.ipynb</span></code> en un fichero <code class="docutils literal notranslate"><span class="pre">generate.py</span></code> que puedas ejecutar desde la línea de órdenes.</p>
<p>Puedes trabajar directamente en una sesión de Google Colab. Aquí tienes un <a class="reference external" href="https://colab.research.google.com/drive/1dq2EClvIbEtoEnHWoAXZQTArJDHivQly?usp=sharing">proyecto</a> (accede con tu cuenta de <code class="docutils literal notranslate"><span class="pre">gcloud.ua.es</span></code>) con instrucciones sobre cómo usarlo para desarrollar. Sin embargo, es mucho más cómodo desarrollar en local (entre otras cosas, puedes trabajar con un mejor editor de texto que el de Colab y también depurar). Incluso si no tienes una GPU, el código funciona sin problemas sobre CPU y solo tarda unos segundos más que sobre GPU al solo trabajar con un texto y con un modelo no excesivamente grande. Cuando tengas el código final, puedes subirlo a un notebook para su entrega.</p>
<p>Añade a la función <code class="docutils literal notranslate"><span class="pre">forward</span></code> del transformer, código que permita salvar (según el valor de cierto <em>flag</em> booleano recibido como parámetro) en una variable de instancia las activaciones de cada capa y cada posición. Recuerda hacer una copia profunda de los embeddings y no guardar únicamente una referencia que puede ser sobreescrita posteriormente; para ello, consulta la secuencia de llamadas <code class="docutils literal notranslate"><span class="pre">.detach().clone()</span></code> de PyTorch. Añade también código que permita (de nuevo en base a un parámetro booleano) parchear el embedding de una capa y posición concretas.</p>
<p>Añade también a la función <code class="docutils literal notranslate"><span class="pre">forward</span></code> código que guarde los logits del último token, que contienen la información que nos interesa sobre la predicción del siguiente token. Puedes guardar esta información en un atributo que luego puedes acceder desde el exterior de la clase. Observa que solo te interesa el vector correspondiente al último token.</p>
<p>Añade código al fichero <code class="docutils literal notranslate"><span class="pre">generate.py</span></code> que divida el texto limpio en tokens, lo pase por el modelo a través de la función <code class="docutils literal notranslate"><span class="pre">generate</span></code> (pidiéndole al modelo que guarde los embeddings intermedios) y muestre las continuaciones más probables a partir de los logits del último token. Ten en cuenta que si quieres saber la probabilidad de una continuación como el token “Jones”, por ejemplo, has de buscar el índice de dicho token en el vocabulario anteponiéndole un espacio en blanco (<code class="docutils literal notranslate"><span class="pre">index</span> <span class="pre">=</span> <span class="pre">bpe('</span> <span class="pre">Jones')</span></code>). Esto es así porque el segmentador de BPE trata de forma diferente los tokens que aparecen al principio de la secuencia y los que aparecen en medio. Una vez tengas el índice del token, puedes acceder a la posición correspondiente del vector de logits y obtener la probabilidad no normalizada de que sea la continuación.</p>
<p>Después, puedes trabajar con el texto corrupto. Incluye un doble bucle que itere sobre todas las capas y todas las posiciones y llame cada vez a <code class="docutils literal notranslate"><span class="pre">generate</span></code> pasándole la capa y la posición en la que realizar la intervención. En cada paso, evalúa la diferencia de logits oportuna y guárdala en una matriz de diferencias.</p>
<p>Usa finalmente la función <code class="docutils literal notranslate"><span class="pre">matshow</span></code> de <code class="docutils literal notranslate"><span class="pre">matplotlib</span></code> para visualizar la matriz de diferencias.</p>
<p><strong><span style="font-size: 1.15em">Una explicación más informal</span></strong></p>
<p>La siguiente explicación informal puede que te ayude a entender mejor el objetivo de la práctica.</p>
<p>Considera para simplificar la frase “a b c” y la versión corrompida “d e f”. En general, habrá muchos más tokens en común, pero así queda todo más claro en la siguiente discusión. Considera que el modelo neuronal basado en el transformer tiene 5 capas de atención. Considera que vamos a estudiar qué embeddings son importantes para la predicción de que tras estas frases vaya el token “X”.</p>
<p>Se trata primero de que permitas que en la función forward del transformer (clase <code class="docutils literal notranslate"><span class="pre">GPT</span></code>) se puedan guardar (por ejemplo en una lista de listas de tensores) los 3x5=15 embeddings que se generan a la salida de cada una de las capas cuando se procesa la frase “a b c”. En el enunciado se dan algunos detalles porque no puedes guardar simplemente la referencia a los tensores, ya que se modificarán la próxima vez que llames a forward, sino que has de clonar los tensores (aquello que en Programación 3 llamábamos “copia defensiva”). Con esto tendrás almacenados los 15 tensores (embeddings) de la frase limpia.</p>
<p>Guárdate también los logits tras la última capa. En particular, solo necesitarás los de la última posición (es decir, los logits correspondientes al token “c”), que te dan una medida de la probabilidad del siguiente token, es decir, del token que irá tras “c”. Recuerda que estos logits no son realmente probabilidades (son valores como -11.1, -0.5, 0.78, o 2.32323) porque no se les ha aplicado la función softmax, pero trabajar con ellos es más cómodo que trabajar con las probabilidades porque tenemos valores con un rango más amplio. No obstante, el estudio podría hacerse igualmente con probabilidades estrictas. En realidad, ni siquiera necesitas guardarte todos los logits, sino solo el escalar que corresponde al token “X” porque es lo único que usarás después</p>
<p>Ahora le das al modelo la versión corrompida “d e f”, indicándole que no sobreescriba la copia de los embeddings que obtuvimos con la frase limpia. La frase corrompida ha de tener el mismo número de tokens que la limpia para que la siguiente discusión tenga sentido. La idea es modificar uno solo de los 15 embeddings que se producen mientras se procesa la frase sucia. Si, por ejemplo, nos centramos en el embedding del primer token (“d”) tras la primera capa, se trataría de que el código de la función forward opere “casi” de la forma normal, pero cuando se obtenga la salida de la primera capa y antes de pasarla como entrada a la segunda capa, se ha de modificar el embedding correspondiente a la primera palabra (solo ese) y sustituirlo por el embedding correspondiente (de la misma capa y posición) que te guardaste para la frase limpia (es decir, en este caso, sería el embedding que te guardaste tras la primera capa para el token “a”). Con esto, la segunda capa recibirá como entrada el embedding que se generó para “a” en lugar del de “d”.</p>
<p>Tras intervenir en el embedding de la posición 1 tras la capa 1, el resto del modelo trabaja sin ningún “contratiempo”. De la misma manera que antes, ahora miramos los logits de la predicción del token que va tras el último token de la frase corrompida (es decir, “f”). Y nos centramos en el valor del logit de la predicción del token “X”. La diferencia entre este valor y el que nos guardamos para la frase limpia nos da una idea de cómo de relevante es el embedding de la capa 1 y posición 1 en la predicción del token “X”. En el enunciado se muestra cómo algunos embeddings son mucho más relevantes que otros. Y tú tienes que hacer un estudio similar con diferentes frases.</p>
<p>Si repites la operación anterior con los otros 14 embeddings (llamando 14 veces más a la función forward), terminarás teniendo 15 diferencias de logits (15 valores escalares) que puedes representar en un mapa de calor de 3x5 como se ve más arriba.</p>
<p><strong><span style="font-size: 1.15em">Ampliar conocimientos</span></strong></p>
<p>Lo anterior es solo uno de los múltiples análisis que se han propuesto dentro de la interpretabilidad mecanicista. Para esta práctica no se espera que vayas más allá de esto, pero si te interesa conocer un par de análisis más puedes consultar <a class="reference external" href="https://www.lesswrong.com/posts/hnzHrdqn3nrjveayv/how-to-transformer-mechanistic-interpretability-in-50-lines">este tutorial</a>. Observa que aunque el tutorial usa una librería para parchear las activaciones, en esta práctica no puedes usar ninguna librería para ello y lo has de hacer directamente sobre el código de minGPT. Una revisión mucho más detallada sobre la interpretabilidad mecanicista se puede encontrar en <a class="reference external" href="https://www.neelnanda.io/mechanistic-interpretability/glossary">este trabajo</a> de Neel Nanda.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="bloque3_ev.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">21. </span>Ev. Evaluación de prácticas del Bloque 2</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="content.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">23. </span>Content in Jupyter Book</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Universitat d'Alacant<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>