
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>6. Análisis semántico vectorial &#8212; Minería de Textos</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/estilos.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="7. Práctica 1b : Topic modeling." href="bloque1_Practica2.html" />
    <link rel="prev" title="5. Análisis semántico" href="bloque1_3AnalisisSemantico.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo-master-ca.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Minería de Textos</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Materiales de Minería de Textos
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1.html">
   1. Introducción a la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_1Introduccion.html">
   2. Minería de textos y procesamiento del lenguaje natural.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_2CategorialSintactico.html">
   3. Análisis categorial y sintáctico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica1.html">
   4. Práctica 1.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_3AnalisisSemantico.html">
   5. Análisis semántico
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   6. Análisis semántico vectorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica2.html">
   7. Práctica 1b :
   <em>
    Topic modeling
   </em>
   .
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Extras
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="content.html">
   8. Content in Jupyter Book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markdown.html">
   9. Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   10. Content with notebooks
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/bloque1_4AnalisisSemanticoVectorial.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#indice">
   6.1. Índice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lectura-obligatoria">
   6.2. Lectura obligatoria:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objetivos">
   6.3. Objetivos
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   6.4. Introducción
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#origen-computacional">
   6.5. Origen computacional
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fundamentos-linguisticos">
   6.6. Fundamentos lingüísticos
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representacion-vectorial-del-significado">
   6.7. Representación vectorial del significado
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representacion-del-contexto">
     6.7.1. Representación del contexto
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representacion-de-las-palabras">
     6.7.2. Representación de las palabras
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculo-de-los-valores-o-pesos">
     6.7.3. Cálculo de los valores o pesos
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tf-idf-term-frequency-inverse-document-frequency-sparck-jones-1972">
       6.7.3.1. TF/IDF:
       <em>
        term frequency / inverse document frequency
       </em>
       (Sparck Jones, 1972)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#matriz-dispersa-y-matriz-densa">
       6.7.3.2. Matriz dispersa y matriz densa
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretacion-semantica-distancia-y-similitud">
   6.8. Interpretación semántica: distancia y similitud.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusiones">
     6.8.1. Conclusiones
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#situacion-actual">
   6.9. Situación actual
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#herramientas-y-recursos">
   6.10. Herramientas y recursos
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apendice-estudio-de-caso">
   6.11. Apéndice. Estudio de caso.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia">
   6.12. Bibliografía
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Análisis semántico vectorial</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#indice">
   6.1. Índice
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#lectura-obligatoria">
   6.2. Lectura obligatoria:
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#objetivos">
   6.3. Objetivos
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduccion">
   6.4. Introducción
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#origen-computacional">
   6.5. Origen computacional
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#fundamentos-linguisticos">
   6.6. Fundamentos lingüísticos
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#representacion-vectorial-del-significado">
   6.7. Representación vectorial del significado
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representacion-del-contexto">
     6.7.1. Representación del contexto
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representacion-de-las-palabras">
     6.7.2. Representación de las palabras
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#calculo-de-los-valores-o-pesos">
     6.7.3. Cálculo de los valores o pesos
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#tf-idf-term-frequency-inverse-document-frequency-sparck-jones-1972">
       6.7.3.1. TF/IDF:
       <em>
        term frequency / inverse document frequency
       </em>
       (Sparck Jones, 1972)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#matriz-dispersa-y-matriz-densa">
       6.7.3.2. Matriz dispersa y matriz densa
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#interpretacion-semantica-distancia-y-similitud">
   6.8. Interpretación semántica: distancia y similitud.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conclusiones">
     6.8.1. Conclusiones
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#situacion-actual">
   6.9. Situación actual
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#herramientas-y-recursos">
   6.10. Herramientas y recursos
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#apendice-estudio-de-caso">
   6.11. Apéndice. Estudio de caso.
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia">
   6.12. Bibliografía
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="analisis-semantico-vectorial">
<span id="label-semantica-vectorial"></span><h1><span class="section-number">6. </span>Análisis semántico vectorial<a class="headerlink" href="#analisis-semantico-vectorial" title="Permalink to this headline">#</a></h1>
<p><font color="red">Esta sección es todavía un <strong>borrador</strong>. En unos días tendréis el texto definitivo.</color></p>
<section id="indice">
<h2><span class="section-number">6.1. </span>Índice<a class="headerlink" href="#indice" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Introducción a los modelos semánticos vectoriales.</p></li>
<li><p>Estudio de caso: <em>topic modeling</em>.</p></li>
</ul>
</section>
<section id="lectura-obligatoria">
<h2><span class="section-number">6.2. </span>Lectura obligatoria:<a class="headerlink" href="#lectura-obligatoria" title="Permalink to this headline">#</a></h2>
<p>Peter D. Turney y Patrick Pantel (2010) “From Frequency to Meaning: Vector Space Models of Semantics” en <em>Journal of Artificial Intelligence  Research</em>, 37, págs. 141-188.  DOI: <a class="reference external" href="https://doi.org/10.1613/jair.2934">https://doi.org/10.1613/jair.2934</a></p>
<p><a class="reference external" href="https://www.jair.org/index.php/jair/article/view/10640">https://www.jair.org/index.php/jair/article/view/10640</a></p>
<p><a class="reference external" href="https://www.jair.org/index.php/jair/article/view/10640/25440">https://www.jair.org/index.php/jair/article/view/10640/25440</a></p>
</section>
<section id="objetivos">
<h2><span class="section-number">6.3. </span>Objetivos<a class="headerlink" href="#objetivos" title="Permalink to this headline">#</a></h2>
<p>En este tema se expone la semántica distribucional, modelo semántico en el que se basan los actuales sistemas de <em>deep learning</em>. Tras definir la semántica distribucional, se mostrará cómo se puede representar el significado mediante vectores, los principales factores que determinan la representación vectorial y finalmente los conceptos de distancia y similitud textual.</p>
</section>
<section id="introduccion">
<h2><span class="section-number">6.4. </span>Introducción<a class="headerlink" href="#introduccion" title="Permalink to this headline">#</a></h2>
<p>La semántica vectorial es un aproximación <strong>formal</strong> a la <strong>semántica</strong> de las lenguas naturales. A diferencia de otros modelos computacionales, el formalismo está basado en espacios vectoriales y álgebra linea; y la interpretación de un texto se expresa en términos geométricos de distancia y similitud (Widdows 2004).</p>
<p>Desde un punto de vista lingüístico, el modelo semántico vectorial representa el significado distrubucional de las palabras. Como se comentará luego, el significado distrubucional es aquél que podemos derivar a partir del contexto en el que una palabra es utilizada. En este modelo, el signficado no es una unidad atómica como en lógica forma ni está definido en un diccionario, sino que es el propio uso de cada palabra en los diferentes contextos donde suele aparecer.</p>
</section>
<section id="origen-computacional">
<h2><span class="section-number">6.5. </span>Origen computacional<a class="headerlink" href="#origen-computacional" title="Permalink to this headline">#</a></h2>
<p>La aplicación de modelos vectoriales para procesar texto proviene del área llamada <strong>Recuperación de información</strong> (<em>Information Retrieval</em>). En esta área se desarrollan sistemas que, dada una consulta, recupera un conjunto de documentos ordenados de mayor a menor relevancia. El producto más conocido desarrollado en esta área son los buscadores de internet.</p>
<p>Para determinar la relación de la consulta (conjunto de palabras) con los documentos, éstos se representan mediante una matriz término-documento. En este matriz, cada palabra está representada por su relevancia en cada documento (por ejemplo, mediante su frecuencia). Así, dada una palabra (en la cosultad), se pude derivar en qué documentos esa palabra es más relevante. Estos modelos se llaman tambien ``modelos de bolsa de palabras’’ (<em>bag of words</em>) porque las palabras se tratan como un conjunto sin orden ni relación entre ellas. En su estado mas básico se ignora la información categorial, sintátictca, etc. del texto.</p>
<p>En esta matriz, por tanto, cada columna representa un documento y cada línea una palabra o término. El valor de cada celda es la relevancia del término o palabra en el documento. Aquí se pueden aplicar varios fórmulas para determinar esa relevancia, que se verán después. La más básica sería la frecuencia relativa del término en el documento. De esta manera, cada columna es un vector que representa un texto, y cada línea es un vector que representa los contextos de aparición de una palabra (cada documento es aquí un contexto de uso). La siguiente matriz representa las frecuencias absolutas de las palabras de dos documentos (doc1 y doc2), cada uno con tres palabras.</p>
<p>doc1 <span class="math notranslate nohighlight">\(= \{casa, madera, mesa\}\)</span></p>
<p>doc2 <span class="math notranslate nohighlight">\(= \{papel, rama, madera\}\)</span></p>
<hr class="docutils" />
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>        Doc 1   Doc 2 
casa      1       0   
madera    1       1   
mesa      1       0   
papel     0       1   
rama      0       1   
</pre></div>
</div>
<hr class="docutils" />
<p>De esta tabla se obtiene la siguiente matriz D:</p>
<div class="math notranslate nohighlight">
\[\begin{split}D = \begin{pmatrix}
  1 &amp; 0 \\
  1 &amp; 1 \\
  1 &amp; 0 \\
  0 &amp; 1 \\
  0 &amp; 1 
\end{pmatrix}\end{split}\]</div>
<p>De esta manera, los documentos ahora está representado como dos vectores:</p>
<p><span class="math notranslate nohighlight">\(\vec{doc1} = \{1 1 1 0 0\}\)</span><br />
<span class="math notranslate nohighlight">\(\vec{doc2} = \{0 1 0 1 1\}\)</span></p>
<p>Y cada palabra o término está representado con su vector contextual:</p>
<p><span class="math notranslate nohighlight">\(\vec{casa} = \{1 0\}\)</span><br />
<span class="math notranslate nohighlight">\(\vec{madera} = \{1 1\}\)</span><br />
<span class="math notranslate nohighlight">\(\vec{mesa} = \{1 0\}\)</span><br />
<span class="math notranslate nohighlight">\(\vec{papel} = \{0 1\}\)</span>\<br />
<span class="math notranslate nohighlight">\(\vec{rama} = \{0 1\}\)</span></p>
</section>
<section id="fundamentos-linguisticos">
<h2><span class="section-number">6.6. </span>Fundamentos lingüísticos<a class="headerlink" href="#fundamentos-linguisticos" title="Permalink to this headline">#</a></h2>
<p>Los modelos semánticos vectoriales asumen básicamente tres propuestas teóricas (Clarke 2011):</p>
<ol>
<li><p>La idea de Wittgenstein (1953) de que “meaning just is use” (Wittgenstein 1953);</p></li>
<li><p>El concepto de <em>collocation</em> de Firth (1957) y su idea de que</p>
<blockquote>
<div><p>“you shall know a word by the company it keeps”;</p>
</div></blockquote>
</li>
<li><p>La hipótesis distribucional de Harris (1968), según la cual:</p>
<blockquote>
<div><p>“words will occur in similar contexts if and only if they have similar meanings”.</p>
</div></blockquote>
</li>
</ol>
<p>Todo ello se engloba dentro del concepto de “significado distribucional”. Este, por tanto, es el significado que una palabra asume cuando se usa en un contexto concreto y queda determinado a partir de las palabras de ese contexto con las que aparece. Esta inferencia semántica (determinar el significado de una palabra a partir de las palabras del contexto) es algo que hacemos constantemente. Mira las siguientes oraciones, ¿qué significado tiene <em>XXX</em> en cada una?</p>
<blockquote>
<div><p>Mañana iré al <em>XXX</em> a firmar la hipoteca, y ya de paso sacaré dinero del cajero.</p>
</div></blockquote>
<blockquote>
<div><p>He intentado ponerme los <em>XXX</em> de mi hermano pero me vienen pequeños: mis pies son muy grandes y necesito una talla más.</p>
</div></blockquote>
</section>
<section id="representacion-vectorial-del-significado">
<h2><span class="section-number">6.7. </span>Representación vectorial del significado<a class="headerlink" href="#representacion-vectorial-del-significado" title="Permalink to this headline">#</a></h2>
<p>El vector de una palabra como se ha mostrado antes representa el significado distribucional de una palabra ya que captura la relevancia de esa palabra (o término o token o lema, según se quiera llamar) en cada uno de los contextos (en este caso documentos) que forman la colección. Así, en la siquiente matriz término - documento:</p>
<hr class="docutils" />
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>       doc1   doc2
car     7      6
taxi    5      6
train   6      1
</pre></div>
</div>
<hr class="docutils" />
<p>el significado de cada palabra sería el vector contextual:</p>
<p><span class="math notranslate nohighlight">\(car = (7,6)\)</span><br />
<span class="math notranslate nohighlight">\(taxi = (5,6)\)</span><br />
<span class="math notranslate nohighlight">\(train = (6,1)\)</span></p>
<p>Esto se puede representar en un espacio euclídeo (plano o lineal) mediante coordenadas cartesiana: los valores del vector se proyectan en los ejes de coordenadas, siendo la abscisa <span class="math notranslate nohighlight">\(x\)</span> el documento 1 la ordenada <span class="math notranslate nohighlight">\(y\)</span> el documento 2.</p>
<p><img alt="cartesanio1" src="_images/cartesiano_1.png" /></p>
<p><img alt="cartesanio2" src="_images/cartesiano_2.png" /></p>
<p>Esto en un plano cartesiano de dos dimensiones. Si la colección está formada por <span class="math notranslate nohighlight">\(n-\)</span> documentos, obtendríamos un espacio <span class="math notranslate nohighlight">\(n\)</span>-dimesionales o multidimensionales en el que cada dimensión es un posible contexto.</p>
<hr class="docutils" />
<p>Para que este modelo vectorial represente el significado real de las palabras, hay que modelar bien:</p>
<ul class="simple">
<li><p>la representación del contexto (columnas o dimensiones),</p></li>
<li><p>la representación de las palabras (filas de la matriz)</p></li>
<li><p>los valores o pesos de cada palabra en cada contexto.</p></li>
</ul>
<section id="representacion-del-contexto">
<h3><span class="section-number">6.7.1. </span>Representación del contexto<a class="headerlink" href="#representacion-del-contexto" title="Permalink to this headline">#</a></h3>
<p>Cada contexto de la palabra será una dimensión de la matriz. El problema es cómo deliminar este contexto: ¿cuántas palabras forman el contexto?, ¿dónde está el límite del contexto?</p>
<p>En el modelo de matriz término-documento que se utiliza en recuperación de información el contexto es todo el documento porque son documentos lo que quieren recuperar, pero se puede limiar a recuperación de pasajes, párrafos, etc. Otras opciones con motivación lingüística podrían ser:</p>
<ul class="simple">
<li><p>la oración,</p></li>
<li><p>una ventana deslizante (un cojunto de <span class="math notranslate nohighlight">\(n\)</span> palabras delante y detrás de la término),</p></li>
<li><p>el párrafo o cualquier otra unidad textual,</p></li>
<li><p>el capítulo,</p></li>
<li><p>etc.</p></li>
</ul>
<p>Por otro lado, además de la matriz término-documento que hemos visto (donde las columnas representan documentos y las filas palabras), se puede crear otro tipo de matriz: la llamad <strong>matriz de co-ocurrencias</strong> o <strong>matriz término-término</strong>. En estas matrices (normalmente cuadradas), tanto las columnas como las filas representan palabra, y los valores la relación entre esas dos palabras. Por ejemplo, en cuántos contextos aparecen esas dos papalabras, como en el siguiente caso:</p>
<hr class="docutils" />
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>       red   readable   blue
car     5       0        1
book    3       6        0
</pre></div>
</div>
<hr class="docutils" />
<p>Según esta matriz, la palabra “car” aparece el mismo contexto de la palabra “red” en cinco ocasiones, no coincide nunca con la palabra “readable” y solo en una ocación con la palabra “blue”. “book”, por su parte, aparece tres veces en el mismo contexto de “red”, sies en el mismo contexto de “readable” y ninguna con “blue”. En ocasiones estas matrices son cuadradas porque tienen los mismo términos en las filas y en las columnas.</p>
<p>Se pueden plantear otros tipos de matrices. Turne y Pantel (2010), por ejemplo, plantean una matriz <em>Pair-Pattern</em> donde las filas son parejas de palabras <span class="math notranslate nohighlight">\(X:Y\)</span> (“carpenter:wood”) y las columnas son relaciones entre palabras co-ocurrentes (“X cut Y”).</p>
<p>Sea como sea el tipo de matriz, es muy relevente dónde se sitúa el límite del contexto (el documento, el párrafo, la oración…)</p>
</section>
<section id="representacion-de-las-palabras">
<h3><span class="section-number">6.7.2. </span>Representación de las palabras<a class="headerlink" href="#representacion-de-las-palabras" title="Permalink to this headline">#</a></h3>
<p>Hasta ahora hemos estado</p>
<p>Según vimos en sesiones anteriores:</p>
<ul class="simple">
<li><p>Token</p></li>
<li><p>Raíz (stem)</p></li>
<li><p>Lemas</p></li>
<li><p>Lema + Categoría Gramatical</p></li>
<li><p>Filtro <em>stopwords</em></p></li>
<li><p>Sólo nombres (o sólo verbos, o sólo adjetivos, etc.)</p></li>
<li><p>Lema + Función sintáctica</p></li>
<li><p>etc.</p></li>
</ul>
<p>Requieren pre-proceso del corpus con técnicas de PLN vistas con anterioridad.</p>
</section>
<section id="calculo-de-los-valores-o-pesos">
<h3><span class="section-number">6.7.3. </span>Cálculo de los valores o pesos<a class="headerlink" href="#calculo-de-los-valores-o-pesos" title="Permalink to this headline">#</a></h3>
<p>Representación cuantitativa de la relevancia (peso) que tiene la palabra en cada contexto.</p>
<p>Frecuencias simples y relativas: número de veces que la palabra aparece en el contexto, normalizado por el tamaño del contexto.</p>
<p>Problemas:</p>
<ul class="simple">
<li><p>Depende del tamaño del contexto.</p></li>
<li><p>No discrimina la importancia real de cada palabra en el contexto.</p></li>
<li><p>El <em>[hapax legomenon]</em>(<a class="reference external" href="https://en.wikipedia.org/wiki/Hapax_legomenon">https://en.wikipedia.org/wiki/Hapax_legomenon</a>)</p></li>
</ul>
<section id="tf-idf-term-frequency-inverse-document-frequency-sparck-jones-1972">
<h4><span class="section-number">6.7.3.1. </span>TF/IDF: <em>term frequency / inverse document frequency</em> (Sparck Jones, 1972)<a class="headerlink" href="#tf-idf-term-frequency-inverse-document-frequency-sparck-jones-1972" title="Permalink to this headline">#</a></h4>
<p>Idea intuitiva: palabras de uso común que aparecen con alta frecuencia en muchos contextos no son discriminativas ni relevantes. TF/IDF intenta dar más peso a las palabras específicas de cada documento.</p>
<ul class="simple">
<li><p><em>Term frequency</em> (tf): frecuencia de una palabra en un documento dado.</p></li>
<li><p><em>Document frequency</em> (df): cantidad de documentos donde aparece una determinada palabra.</p></li>
<li><p><em>Inverse document frequency</em> (idf): <span class="math notranslate nohighlight">\(N/df\)</span> donde N = cantidad total de documentos.</p></li>
</ul>
<p>Así, el valor tf-idf (<span class="math notranslate nohighlight">\(w\)</span>) de una palabra <span class="math notranslate nohighlight">\(t\)</span> en un documento <span class="math notranslate nohighlight">\(d\)</span> es:</p>
<div class="math notranslate nohighlight">
\[w_t,_d = tf_t,_d · idf_t\]</div>
</section>
<section id="matriz-dispersa-y-matriz-densa">
<h4><span class="section-number">6.7.3.2. </span>Matriz dispersa y matriz densa<a class="headerlink" href="#matriz-dispersa-y-matriz-densa" title="Permalink to this headline">#</a></h4>
<p>Dadas las caracterísiticas de los idiomas, este tipo de matrices son siempre muy dispersas, en las que la mayoría de los valores con cero.</p>
<blockquote>
<div><p><em>Sparse matrix</em>: la mayoría de los valores son ceros.</p>
</div></blockquote>
<p>Para solucionar esto se reduce la dimensionalidad de la matriz, generando así  matrices densas (<em>dense matrix</em>) donde la mayoría de los valores no son ceros pero manteniendo las relaciones semánticas entre las palabras (los valores semánticos).</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Latent_semantic_analysis"><em>Latent semantic analysis</em></a>: descomposición de la matriz dispesar en valores singulares (<a class="reference external" href="https://en.wikipedia.org/wiki/Singular_value_decomposition"><em>singular value decomposition</em></a>), generando una matriz densa de 300 dimensiones. Se considera que mantiene relaciones semánticas “latentes”. Origen de los <em>word embeddings</em> que veréis en próximos temas.</p>
</section>
</section>
</section>
<section id="interpretacion-semantica-distancia-y-similitud">
<h2><span class="section-number">6.8. </span>Interpretación semántica: distancia y similitud.<a class="headerlink" href="#interpretacion-semantica-distancia-y-similitud" title="Permalink to this headline">#</a></h2>
<p>La interpretación en esta aproximación vectorial a la semántica distribucional se realiza por relaciones de similitud entre palabras o documentos. La similitud se calcular según la distancia entre los vectores en el espacio vectorial: a menor distancia entre vectores, mayor similitud semántica.</p>
<p>Así, desde un punto de vista lingüístico, dos vectores (de palabras) serán similares en la medida que tengan valores relevantes en los mismos contextos.</p>
<p>Cualquier aplicación de semántica vectorial debe pensarse en términos de
similitudes (entre palabras, grupos de palabras, textos, etc.).</p>
<p>Hay diferentes medidas. Las más utilizada es la similitud del coseno, que mide el ángulo entre dos vectores ambos con origen en 0,0.</p>
<div class="math notranslate nohighlight">
\[cos(a,b) = \frac{a · b}{||a|| ||b||}\]</div>
<p><img alt="cartesanio2" src="_images/cartesiano_2.png" />{height=”10cm”}</p>
<section id="conclusiones">
<h3><span class="section-number">6.8.1. </span>Conclusiones<a class="headerlink" href="#conclusiones" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Representación formal del significado distribucional.</p></li>
<li><p>El significado se represente mediante vectores dentro de un espacio semántica vectorial.</p></li>
<li><p>El vector está formado por el peso de la palabra en cada uno de los contextos (documentos, oraciones, etc.)</p></li>
<li><p>El proceso de interpretación se basa en la distancia entre vectores: similitud.</p></li>
</ul>
</section>
</section>
<section id="situacion-actual">
<h2><span class="section-number">6.9. </span>Situación actual<a class="headerlink" href="#situacion-actual" title="Permalink to this headline">#</a></h2>
<p>De aquí derivan los <em>word embeddings</em> que, junto con las redes neuronales, han revolucionado el campo del PLN. De todo esto os hablará el prof. Juan Antonio Pérez Ortiz en las próximas sesiones.</p>
</section>
<section id="herramientas-y-recursos">
<h2><span class="section-number">6.10. </span>Herramientas y recursos<a class="headerlink" href="#herramientas-y-recursos" title="Permalink to this headline">#</a></h2>
<p>Para crear espacios vectoriales y calcular similitudes:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://radimrehurek.com/gensim/">GENSIM</a></p></li>
<li><p><a class="reference external" href="http://www.nltk.org/">NLTK</a></p></li>
<li><p><a class="reference external" href="http://www.clips.ua.ac.be/pattern">Pattern</a></p></li>
<li><p><a class="reference external" href="https://spacy.io/">SpaCy</a></p></li>
</ul>
</section>
<section id="apendice-estudio-de-caso">
<h2><span class="section-number">6.11. </span>Apéndice. Estudio de caso.<a class="headerlink" href="#apendice-estudio-de-caso" title="Permalink to this headline">#</a></h2>
<p>Extracción de <em>topics</em> con <em>Topic Modeling</em>.</p>
<p><a class="reference external" href="https://docs.google.com/presentation/d/e/2PACX-1vRhjksmebwfZ8CfMNCqp7ucPr0i--fPNCa6dqb0NH3jiMOQV1lSvnlnF7qptbtqEsA5O4IzpcJa-F9r/pub?start=false&amp;loop=false&amp;delayms=60000">Acceso a la presentación</a></p>
</section>
<section id="bibliografia">
<h2><span class="section-number">6.12. </span>Bibliografía<a class="headerlink" href="#bibliografia" title="Permalink to this headline">#</a></h2>
<p>David M. Blei (2012) “Probabilistic topic models” en <em>Communications of the ACM</em> vol. 55 (4), April 2012. Doi:10.1145/2133806.2133826
<a class="reference external" href="https://dl.acm.org/doi/10.1145/2133806.2133826">https://dl.acm.org/doi/10.1145/2133806.2133826</a></p>
<p>Juravsky y Martin (2020) <em>Speech and Language Processing</em>. <a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a>
(Caps. 12-14)</p>
<p>Ferrone Lorenzo y Zanzotto Fabio Massimo (2020) “Symbolic, Distributed, and Distributional Representations for Natural Language Processing in the Era of Deep Learning: A Survey” en <em>Frontiers in Robotics and AI</em>, pags, 153. DOI 10.3389/frobt.2019.00153
<a class="reference external" href="https://www.frontiersin.org/article/10.3389/frobt.2019.00153">https://www.frontiersin.org/article/10.3389/frobt.2019.00153</a></p>
<p>Navarro Colorado, Borja (2021) “Sistemas de anotación semántica para corpus de español” en Giovanni Parodi, Pascual Cantos &amp; Lewis Howe (Editores) <em>The Routledge Handbook of Spanish Corpus Linguistics</em> Routledge (en prensa).</p>
<p>D. Widdows (2004) <em>Geometry and meaning</em>, CSLI.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="bloque1_3AnalisisSemantico.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">5. </span>Análisis semántico</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="bloque1_Practica2.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">7. </span>Práctica 1b : <em>Topic modeling</em>.</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Universitat d'Alacant<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>