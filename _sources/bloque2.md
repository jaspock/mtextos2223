
(label_tecnicas)=
T√©cnicas para la miner√≠a de textos
==================================

En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio P√©rez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor log√≠stico, que nos servir√° para asentar los conocimientos necesarios para entender posteriores modelos. A continuaci√≥n se estudia con cierto nivel de detalle *skip-grams*, uno de los algoritmos para la obtenci√≥n de *embeddings* incontextuales de palabras. Despu√©s se repasa el funcionamiento de las arquitecturas neuronales *feedforward* y se estudia su aplicaci√≥n a modelos de lengua. El objetivo √∫ltimo es abordar el estudio de la arquitectura m√°s importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un an√°lisis del funcionamiento de los modelos preentrenados.

Los materiales de clase complementan la lectura de algunos cap√≠tulos de un libro de texto ("Speech and Language Processing" de Dan Jurafsky y James H. Martin, borrador de la tercera edici√≥n, disponible online) con anotaciones realizadas por el profesor.

## Pr√°ctica a entregar para este bloque

Durante las sesiones de este bloque, estudiaremos diferentes implementaciones en PyTorch de modelos neuronales para procesar textos. Para cada ejemplo de c√≥digo, excepto el √∫ltimo, has de entregar un notebook con el c√≥digo original y bloques de texto con tus comentarios explicando el c√≥digo en base a lo que has aprendido sobre el tema y sobre PyTorch. Entrega todos los notebooks en forma de enlaces de Google Colab a trav√©s de una tutor√≠a de UACloud. Crea los cuadernos de Google Colab con tu cuenta de `gcloud.ua.es` y comp√°rtelos con la cuenta del profesor que te indicar√° en clase. Para el √∫ltimo bloque de c√≥digo (implementaci√≥n del transformer), tendr√°s que complementar el cuaderno con c√≥digo propio para realizar una tarea adicional que ser√° convenientemente anunciada. El plazo de entrega acaba el 26 de mayo de 2023 a las 23.59 horas.

## Primera sesi√≥n (29 de marzo de 2023)

**Contenidos a preparar antes de la sesi√≥n del 29/03/2023**

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de los contenidos de [esta p√°gina](https://jaspock.github.io/me/materials/transformers/regresor) sobre regresi√≥n log√≠stica. Puedes saltar por ahora el apartado de [implementaci√≥n en PyTorch](https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch), ya que ser√° el eje central de la clase presencial. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop√≥sito es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo del libro. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.
- Visionado y estudio de los tutoriales en v√≠deo de esta [playlist oficial de PyTorch](https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN).  Estudia al menos los 4 primeros v√≠deos (‚ÄúIntroduction to PyTorch‚Äù, ‚ÄúIntroduction to PyTorch Tensors‚Äù, ‚ÄúThe Fundamentals of Autograd‚Äù y ‚ÄúBuilding Models with PyTorch‚Äù). En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.
- Tras acabar con las dos partes anteriores, realiza este [test de evaluaci√≥n](https://forms.gle/E1xzZHw6hzMWJaNr7) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.

**Contenidos para la sesi√≥n presencial del 29/03/2023**

En la clase presencial, repasaremos los contenidos de la semana anterior y veremos c√≥mo se implementa el regresor log√≠stico en PyTorch siguiendo la implementaci√≥n de un regresor log√≠stico binario y de uno multinomial que se comentan en [este apartado](https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch).

La idea es que vayas creando una serie de notebooks en Google Colab en los que incluyas y comentes cada uno de los programas que vamos a ir viendo. En la √∫ltima clase se presentar√° una pr√°ctica m√°s avanzada que implicar√° modificar el c√≥digo del transformer.

## Segunda sesi√≥n (26 de abril de 2023)

Entre la sesi√≥n anterior y la del 26 de abril transcurren varias semanas, por lo que la carga de trabajo es mayor que en la sesi√≥n anterior.

**Contenidos a preparar antes de la sesi√≥n del 26/04/2023**

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de [esta p√°gina](https://jaspock.github.io/me/materials/transformers/embeddings) sobre la obtenci√≥n de embeddings incontextuales. Puedes saltar de nuevo el apartado de [implementaci√≥n en PyTorch](https://jaspock.github.io/me/materials/transformers/embeddings#implementaci√≥n-en-pytorch), ya que se estudiar√° en la pr√≥xima clase presencial. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo objetivo es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo. En total, esta parte deber√≠a llevarte unas 4 horas üïíÔ∏è de trabajo.
- Lectura y estudio de [esta p√°gina](https://jaspock.github.io/me/materials/transformers/ffw) sobre las redes neuronales hacia delante. Puedes saltar de nuevo el apartado de [implementaci√≥n en PyTorch](https://jaspock.github.io/me/materials/transformers/ffw#implementaci√≥n-en-pytorch), ya que se estudiar√° tambi√©n en la pr√≥xima clase presencial. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.
- Primeros pasos en el estudio del modelo transformer. Volveremos a dedicar m√°s horas a esta arquitectura para la pr√≥xima sesi√≥n de forma que la abordaremos en dos fases. Por ahora, lee con detenimiento la introducci√≥n a mecanismos de atenci√≥n de ["Visualizing A Neural Machine Translation Model"](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/), as√≠ como la introducci√≥n visual a los transformers de ["The Illustrated Transformer"](http://jalammar.github.io/illustrated-transformer/). A continuaci√≥n, lee el apartado 9.7 (solo este apartado) del cap√≠tulo ["Deep learning architectures for sequence processing"](https://web.archive.org/web/20221216193204/https://web.stanford.edu/~jurafsky/slp3/9.pdf); el objetivo es que entiendas conceptualmente el mecanismo de atenci√≥n de los transformers, pero no es necesario que en este momento comprendas todos los detalles t√©cnicos (especialmente las ecuaciones del modelo), ya que volver√°s a dedicarle tiempo a este cap√≠tulo m√°s adelante. En total, esta parte deber√≠a llevarte ahora unas 4 horas üïíÔ∏è de trabajo.
- Realizaci√≥n del [test de evaluaci√≥n](https://forms.gle/Eb3ZwwGxbQp88t4FA) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.

**Contenidos para la sesi√≥n presencial del 26/04/2023**

En la clase presencial, repasaremos los contenidos de la semana anterior y veremos sendas implementaciones en PyTorch del algoritmo [skip-grams](https://jaspock.github.io/me/materials/transformers/embeddings#implementaci√≥n-en-pytorch) y de un modelo de lengua basado en [redes feedforward](https://jaspock.github.io/me/materials/transformers/ffw#implementaci√≥n-en-pytorch).

## Tercera sesi√≥n (10 de mayo de 2023)

**Contenidos a preparar antes de la sesi√≥n del 10/05/2023**

**Contenidos para la sesi√≥n del 10/05/2023**
