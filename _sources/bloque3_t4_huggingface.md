
T4. Centralizaci√≥n de datasets y modelos: Huggingface
====================================

Contenidos:

- [Introducci√≥n](#introduccion)
- [Repositorio de Datasets](#repositorio-de-datasets)
- [Repositorio de Modelos pre-entrenados](#repositorio-de-modelos-pre-entrenados)
- [Tecnolog√≠as de Generci√≥n (Copilot, ChatGPT)](#)

## Introducci√≥n

Huggingface.co una compa√±√≠a centrada en el PLN la cual ha desarrollado las [**librer√≠as Transformers**](https://huggingface.co/transformers/), **centralizado datasets** y ha **creado modelos de aprendizaje pre-entrenados** disponibles a trav√©s de sus librer√≠as de programaci√≥n.
Las librer√≠as de Huggingface actualmente dan soporte a empresas muy importantes del mercado tecnol√≥gico. Ver <https://huggingface.co/>.

## Repositorio de Datasets

Proporciona conjuntos de datos para muchas tareas de PLN como clasificaci√≥n de texto, respuesta a preguntas, modelado de lenguaje, etc.  
Instalaci√≥n de librer√≠a de manipulaci√≥n de datasets
Para la **instalaci√≥n** de la librer√≠a de manipulaci√≥n de datasets se debe ejecutar la siguiente instrucci√≥n pip:

````
>>> pip install datasets
````

Para asegurarnos de que Transformers dataset se ha instalado correctamente es necesario ejecutar la siguiente instrucci√≥n:

````
>>> python -c "from datasets import load_dataset; print(load_dataset('squad', split='train')[0])"

````

Esta instrucci√≥n debe descargar la versi√≥n 1 del conjunto de datos de respuesta a preguntas de Stanford, cargar su divisi√≥n de entrenamiento e imprimir el primer ejemplo de entrenamiento de la siguiente manera:

````
{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes"...', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': array(['Saint Bernadette Soubirous'], dtype=object), 'answer_start': array([515], dtype=int32)}}
````

### Listar datasets disponibles en el repositorio

Para listar los conjuntos de datos disponibles es necesario ejecutar la siguiente funci√≥n ``datasets.list_datasets ()`` que pertenece a la clase ``datasets``.

````
>>> from datasets import list_datasets
>>> datasets_list = list_datasets()
>>> len(datasets_list)
656
>>> print(', '.join(dataset for dataset in datasets_list))
aeslc, ag_news, ai2_arc, allocine, anli, arcd, art, billsum, blended_skill_talk, blimp, blog_authorship_corpus, bookcorpus, boolq, break_data,
c4, cfq, civil_comments, cmrc2018, cnn_dailymail, coarse_discourse, com_qa, commonsense_qa, compguesswhat, coqa, cornell_movie_dialog, cos_e,
cosmos_qa, crime_and_punish, csv, definite_pronoun_resolution, discofuse, docred, drop, eli5, empathetic_dialogues, eraser_multi_rc, esnli,
event2Mind, fever, flores, fquad, gap, germeval_14, ghomasHudson/cqc, gigaword, glue, ‚Ä¶
````

Otra alternativa es:

1. Ir a la web <https://huggingface.co>
2. Seleccionar el men√∫ Datasets: <https://huggingface.co/datasets>
3. Filtrar por categor√≠a, idioma, tarea y/o licencia

### ¬øC√≥mo cargar datasets?

Haciendo uso de la funci√≥n ``load_dataset`` se nos permite recuperar cualquier dataset registrado en el repositorio. Por ejemplo, el dataset **MRPC** que ha sido proporcionado en el √≠ndice de referencia GLUE (<https://gluebenchmark.com/leaderboard>).

```
>>> from datasets import load_dataset
>>> dataset = load_dataset('glue', 'mrpc', split='train')
```

O podemos ver otro ejemplo como el de [**eHealth-KD**](https://knowledge-learning.github.io/ehealthkd-2020/)

````
>>> from datasets import load_dataset
>>> dataset = load_dataset("ehealth_kd")
````

No obstante, la librer√≠a ``datasets`` permite adem√°s **cargar conjuntos de datos propios** que no formen parte del repositorio. Por ejemplo:
````
>>> from datasets import load_dataset
>>> dataset = load_dataset('csv', data_files='my_file.csv')
````

Para m√°s detalles sobre las distintas funciones y par√°metros permitidos para manipular datasets ver la siguiente documentaci√≥n:
- <https://huggingface.co/docs/datasets/quicktour.html>

### Categor√≠as, tareas e idiomas de datasets

**Categor√≠as:**
En este repositorio podemos encontrar un amplio catalogo de categor√≠as por las cuales filtrar y y especificar el tipo de dateset que estamos buscando. Hemos de resaltar que estos datasets existen originalmente en diferentes formatos, nos obstante en una vez incluido en este repositorio, el formato es estandar. Por tal motivo, a trav√©s de las libr√≠as de manipulaci√≥n (las mencionadas enteriormente) que ofrece Huggingface, podemos acceder a ellos y gestionarlos.

```{image} /images/bloque3/t4/hf_dataset_categoria.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 1. Categor√≠as filtro de datasets

**M√°s de 134 tareas y m√°s de 194 idiomas:**

```{image} /images/bloque3/t4/hf_dataset_tareas_idiomas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 2. Tareas e idiomas filtro datasets

## Repositorio de Modelos pre-entrenados

La biblioteca de Transformers permite el **uso de modelos previamente entrenados** para tareas de Comprensi√≥n del lenguaje natural (NLU), i.e. como analizar el sentimiento de un texto, y Generaci√≥n del lenguaje natural (NLG), i.e. como completar un mensaje con texto nuevo o traducir a otro idioma.
A groso modo listamos los modelos que nos podemos encontrar
- **An√°lisis de sentimiento**: Conocer si un texto es positivo o negativo
- **Generaci√≥n de texto** (en ingl√©s): proporcionar un mensaje para el cual el modelo generar√° un texto.
- **Reconocimiento de entidades nombradas** (NER): Dado en una oraci√≥n de entrada se etiqueta cada palabra con la entidad que esta representa (persona, lugar, organizaci√≥n, etc.)
- **Respuesta a preguntas**: Teniendo en cuenta un modelo de un contexto determinado, dado un pregunta se obtiene una respuesta.
- **Relleno de texto con m√°scara**: Dado un texto con palabras enmascaradas (p. Ej., Reemplazado por [M√ÅSCARA]), completar los espacios en blanco.
- **Resumen**: Generaci√≥n de un resumen a partir de texto extenso.
- **Traducci√≥n**: Traducci√≥n de un texto a otro idioma.
- **Extracci√≥n de caracter√≠sticas**: Obtener una representaci√≥n tensorial del texto.
Tomado de https://huggingface.co/transformers/quicktour.html

**Listado de tareas tal y como las podemos encontrar en el repositorio:**
El listado de tareas, como categor√≠as, en las que podemos filtar los distintos modelos preentrenados que ofrece el repositorio Huggingface, es igual de amplio que el de los datasets. 


```{image} /images/bloque3/t4/hf_modelos_tareas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 3. Tareas filtro modelos
 
**Idiomas para los que se han entrenado los modelos:** 
El listado de idiomas,como categor√≠as, en las que podemos filtar los distintos modelos preentrenados que ofrece el repositorio Huggingface, es igual de amplio que el de los datasets.
 
```{image} /images/bloque3/t4/hf_modelos_idiomas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```
Figura 4. Idiomas filtro modelos

Una explicaci√≥n detallada sobre cada una de estas tareas y ejemplos de uso con Huggingface Transformer la podemos encontrar en el siguiente enlace: 
- <https://huggingface.co/transformers/task_summary.html>


**Huggingface a partir de 2022**
A mediados de 2022 esta plataforma federativa da un paso agigantado expandiendo datasets y modelos preentrenados de solo ofrecer recursos para la modalidad de Procesamiento del Lenguaje Natural, a ofrecer recursos Multimodales, Visi√≥n por Computadora, Procesamiento de Audio, Procesamiento de datos Tabulares y  para Aprendisaje por reforzamiento.

En la mayor√≠a de los casos se ofrece una ejemplo de uso y documentaci√≥n. Poner en marcha cualquiera de estas tareas, reajustando o no los modelos prentrenados que se ofrecen en esta plataforma, se encuentra bien documentado y ejemplificado en ella: Ver Categor√≠as <https://huggingface.co/tasks>


```{image} /images/bloque3/t4/doc-tareas-hf.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```
Figura 5. Categor√≠as de documentaciones agrupadas por tareas y modalidades 


!!!!!!!!!!!!!!!!!HASTA AQUI!!!!!!!!!!!!!!!!!

Ejemplo de An√°lisis de Sentimientos con Huggingface Transformer:

````
>>> from transformers import pipeline
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to show you the ü§ó Transformers library.')
[{'label': 'POSITIVE', 'score': 0.9997795224189758}]
````

Si os fij√°is hemos cargado un modelo pre-entrenado a trav√©s del pipeline  ``sentiment-analysis`` para utilizarlo como clasificador. Este **modelo** se puede **reentrenar** a escenarios espec√≠ficos si queremos realizando un ajuste sobre un nuevo corpus. Para m√°s detalles ver la clase pr√°ctica [``bloque3_p3_SA-Transformers-Training-FineTuning``](https://jaspock.github.io/mtextos/bloque3_p3_SA-Transformers-Training-FineTuning.html)


Si queremos que el pipeline sea multilingue, podemos indicar el modelo exacto que contemple un diccionario de este tipo y el pipeline lo ensamblar√° internamente. Mirad el siguiente ejemplo:

````
>>> from transformers import pipeline
>>> classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment' )
>>> classifier('Estoy muy triste')
[{'label': '1 star', 'score': 0.7241697907447815}]
````

Para otras tareas como Rellenado de M√°scaras podemos ver como podemos simplemente indicar el tipo de tarea para que el pipeline seleccione el tipo de configuraci√≥n m√°s adecuada a esta y el modelo que queremos aplicarle. Con solo cambiar el modelo base podemos hacer esta tarea unilingue a multilingue o cambiar de idioma. Ver el ejemplo a continuaci√≥n:

````
>>> from transformers import AutoModelWithLMHead, AutoTokenizer
>>> model = AutoModelWithLMHead.from_pretrained('mrm8488/RuPERTa-base')
>>> tokenizer = AutoTokenizer.from_pretrained("mrm8488/RuPERTa-base", do_lower_case=True)

>>> from transformers import pipeline

>>> pipeline_fill_mask = pipeline("fill-mask", model=model, tokenizer=tokenizer)

>>> pipeline_fill_mask("Espa√±a es un pa√≠s muy <mask> en la UE")

[{'score': 0.19951821863651276,
  'sequence': 'Espa√±a es un pa√≠s muy importante en la UE',
  'token': 1560,
  'token_str': ' importante'},
 {'score': 0.04137842729687691,
  'sequence': 'Espa√±a es un pa√≠s muy grande en la UE',
  'token': 2741,
  'token_str': ' grande'},
 {'score': 0.029216745868325233,
  'sequence': 'Espa√±a es un pa√≠s muy peque√±o en la UE',
  'token': 2948,
  'token_str': ' peque√±o'},
 {'score': 0.02563760057091713,
  'sequence': 'Espa√±a es un pa√≠s muy popular en la UE',
  'token': 5782,
  'token_str': ' popular'},
 {'score': 0.022264542058110237,
  'sequence': 'Espa√±a es un pa√≠s muy antiguo en la UE',
  'token': 5240,
  'token_str': ' antiguo'}]
````

### Listado de Pipelines 
En Huggingface podemos encontrar una serie de Pipelines ya preparados para enfrentar tareas concretas a los cuales les podemos suministrar distintos modelos y tokenizadores transformes. Ver ejemplos: <https://huggingface.co/transformers/main_classes/pipelines.html>

### ¬øC√≥mo buscar y reutilizar modelos pre-entrenados en la plataforma?

A continuaci√≥n, se listan los pasos a seguir:
1. Dirigirse al repositorio <https://huggingface.co/>
2. Seleccionar el men√∫ ``models`` que nos llevar√° a <https://huggingface.co/models>
3. Filtrar el listado de modelos seg√∫n la tarea, idioma, librer√≠a (Pytorch o TensorFlow), dataset sobre el que fue entrenado, o licencia. Por ejemplo:  tarea ``Text Classification``; idioma ``es``.
4. Elegir un modelo de la lista. Por ejemplo: ``bert-base-multilingual-uncased-sentiment``
5. Obtendremos la documentaci√≥n necesaria para utilizar el modelo.

**Conociendo el nombre del modelo** a utilizar entonces podemos **hacer uso** de este a trav√©s de la librer√≠a **Transformer**. 
En la propia documentaci√≥n se aporta el **c√≥digo de ejemplo** para hacer uso del modelo y en algunos casos una [**interfaz para probarlo**](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment): 

````
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")  # cargando el toquenizador basado en el modelo preentrenado
model = AutoModelForSequenceClassification.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment") # cargando del modelo preentrenado
````

### Configuraciones de modelos trasnformers
Los **modelos pre-entrenados** que se brindan en el repositorio se **basan** en alguna de las **arquitecturas Transformers** descrita en la documentaci√≥n del repositorio (<https://huggingface.co/docs>).
Si tomamos como referencia la arquitectura de modelo Transformer [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html#overview) podemos conocer c√≥mo **gestionar** los distintos **par√°metros**, [**configuraciones de red neuronal**](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertconfig), [**tokenizador**](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer) y **ejemplos documentados** para cada tipo de tarea, tal y como podemos encontrar en el siguiente enlace (<https://huggingface.co/course/chapter7/>).

````
>>> # !pip install transformers
>>> from transformers import DistilBertTokenizer, DistilBertModel
>>> import torch

>>> tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') # cargando de toquenizador basado en el modelo preentrenado
>>> model = DistilBertModel.from_pretrained('distilbert-base-uncased') # cargando el modelo preentrenado

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> print(last_hidden_states)

tensor([[[-1.8296e-01, -7.4054e-02,  5.0267e-02,  ..., -1.1261e-01,
           4.4493e-01,  4.0941e-01],
         [ 7.0631e-04,  1.4825e-01,  3.4328e-01,  ..., -8.6039e-02,
           6.9475e-01,  4.3353e-02],
         [-5.0721e-01,  5.3086e-01,  3.7163e-01,  ..., -5.6287e-01,
           1.3756e-01,  2.8475e-01],
         ...,
         [-4.2251e-01,  5.7314e-02,  2.4338e-01,  ..., -1.5223e-01,
           2.4462e-01,  6.4155e-01],
         [-4.9384e-01, -1.8895e-01,  1.2641e-01,  ...,  6.3241e-02,
           3.6913e-01, -5.8252e-02],
         [ 8.3269e-01,  2.4948e-01, -4.5440e-01,  ...,  1.1998e-01,
          -3.9257e-01, -2.7785e-01]]], grad_fn=<NativeLayerNormBackward>)

````

Es importante conocer que las **configuraciones** de modelos Transformer ya **cuentan** con **modelos base pre-entrenados**. En el caso de ``DistilBERT`` podemos encontrar ``distilbert-base-uncased``. 


## Tecnolog√≠as de generaci√≥n

### GPT
GPT significa "Generative Pretrained Transformer". Es un modelo de lenguaje que utiliza t√©cnicas de deep learning para generar texto de manera aut√≥noma. GPT ha sido entrenado en una amplia cantidad de contenido textual.


- GPT-1: Es la primera versi√≥n de GPT, con solo 117 millones de par√°metros. Aunque es significativamente m√°s limitada que las versiones posteriores, a√∫n es capaz de generar texto aceptable en muchos contextos.
- GPT-2: Es la segunda versi√≥n de GPT, con solo 1.5 mil millones de par√°metros. Es capaz de generar texto coherente y a menudo convincente.
- GPT-3: Es la tercera versi√≥n de GPT y es uno de los modelos de lenguaje m√°s grandes y avanzados jam√°s entrenados. Tiene m√°s de 175 mil millones de par√°metros, lo que le permite generar texto muy convincente en una amplia variedad de contextos.

Adem√°s de estas versiones, tambi√©n existen variantes m√°s peque√±as de GPT para diferentes usos, como GPT-3 Lite y GPT-2 Medium. Cada una de estas variantes tiene un tama√±o y capacidad diferente, lo que las hace m√°s adecuadas para diferentes aplicaciones y escenarios.

A continuaci√≥n se muestra un ejemplo de uso de GPT2:

````
import torch
from transformers import GPT2Tokenizer, GPT2LMHeadModel

# Inicializa el tokenizador
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# Inicializa el modelo
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Define el prompt
prompt = "Escribe un ensayo sobre la importancia de la tecnolog√≠a en la educaci√≥n"

# Tokeniza el prompt
input_ids = tokenizer.encode(prompt, return_tensors='pt')

# Marca el fin de la entrada
input_ids = input_ids[:, -1].unsqueeze(0)

# Activamos el modelo en modo evaluaci√≥n
model.eval()

# Generamos el texto
with torch.no_grad():
    outputs = model(input_ids, labels=input_ids)
    loss, logits = outputs[:2]
    logits = logits[0, -1, :]
    probs = torch.nn.functional.softmax(logits, dim=-1)
    next_token_id = torch.argmax(probs).unsqueeze(0)
    input_ids = torch.cat([input_ids, next_token_id], dim=-1)

# Convertimos los ids a texto
generated_text = tokenizer.decode(input_ids[0].tolist())

# Imprimimos el resultado
print(generated_text)

````

### Copilot
Es asistente de inteligencia artificial dise√±ado para ayudar enel completamiento de c√≥digo mediante el uso de la conversaci√≥n natural. Copilot utiliza modelos de lenguaje avanzados para comprender tus necesidades y brindarte la informaci√≥n y la ayuda que necesitas. Puedes interactuar con Copilot en una variedad de plataformas y dispositivos, incluyendo mensajer√≠a, aplicaciones de chat, aplicaciones de escritorio y m√°s.

Copilot est√° dise√±ado para ayudarte a realizar una amplia gama de tareas y responder preguntas de forma eficiente y precisa. Algunos ejemplos de las tareas que puedes realizar con Copilot incluyen:

- **Consultar informaci√≥n** sobre el clima, la hora actual y otras condiciones meteorol√≥gicas.
- Obtener informaci√≥n sobre **eventos actuales, noticias y tendencias**.
- Realizar **b√∫squedas en l√≠nea** y encontrar informaci√≥n sobre temas espec√≠ficos.
- **Programar recordatorios y citas**.
- **Obtener recomendaciones** de restaurantes, pel√≠culas y otras formas de entretenimiento.
- **Traducir** palabras y frases a otros idiomas.
- **Obtener informaci√≥n sobre la bolsa de valores**, la tasa de cambio y otras cotizaciones financieras.
- **Resolver problemas** **matem√°ticos** y **responder preguntas** sobre **conceptos cient√≠ficos** y **tecnol√≥gicos**.

Copilot est√° dise√±ado para ayudarte a realizar muchas tareas cotidianas y responder preguntas de una manera conveniente y r√°pida.

### ChatGPT

Es un modelo de lenguaje entrenado utilizando una gran cantidad de texto en internet. Se trata de una tecnolog√≠a de procesamiento del lenguaje natural que permite a los usuarios interactuar con el modelo mediante el uso de conversaciones naturales. 

Algunas de las funcionalidades m√°s destacadas incluyen:

- Responder preguntas: ChatGPT puede responder preguntas sobre una amplia gama de temas, incluyendo historia, geograf√≠a, ciencias, tecnolog√≠a y mucho m√°s.

- Completar oraciones o fragmentos de texto: ChatGPT puede utilizar el contexto y la informaci√≥n previa para completar oraciones o fragmentos de texto de manera eficiente.

- Generaci√≥n de texto: ChatGPT puede generar texto en una variedad de formatos, como descripciones de productos, rese√±as de pel√≠culas y mucho m√°s.

- Traducci√≥n de idiomas: ChatGPT puede traducir palabras y frases a otros idiomas, lo que lo hace ideal para aquellos que desean comunicarse en un idioma distinto al suyo.

- Resumen de texto: ChatGPT puede resumir grandes cantidades de texto en una forma concisa y f√°cil de entender.

- An√°lisis de sentimientos: ChatGPT puede analizar el contenido de un texto para determinar el sentimiento que se expresa en √©l, como por ejemplo si es positivo, negativo o neutral.

En la web oficial de OpenAI podemos ver un amplio listado de ejemplos de aplicaciones de esta tecnolog√≠a:

- Q&A
  - Answer questions based on existing knowledge.
Grammar correction
Corrects sentences into standard English.
Summarize for a 2nd grader
Translates difficult text into simpler concepts.
Natural language to OpenAI API
Create code to call to the OpenAI API using a natural language instruction.
Text to command
Translate text into programmatic commands.
English to other languages
Translates English text into French, Spanish and Japanese.
Natural language to Stripe API
Create code to call the Stripe API using natural language.
SQL translate
Translate natural language to SQL queries.
Parse unstructured data
Create tables from long form text
Classification
Classify items into categories via example.
Python to natural language
Explain a piece of Python code in human understandable language.
Movie to Emoji
Convert movie titles into emoji.
Calculate Time Complexity
Find the time complexity of a function.
Translate programming languages
Translate from one programming language to another
Advanced tweet classifier
Advanced sentiment detection for a piece of text.
Explain code
Explain a complicated piece of code.
Keywords
Extract keywords from a block of text.
Factual answering
Guide the model towards factual answering by showing it how to respond to questions that fall outside its knowledge base. Using a '?' to indicate a response to words and phrases that it doesn't know provides a natural response that seems to work better than more abstract replies.
Ad from product description
Turn a product description into ad copy.
Product name generator
Create product names from examples words. Influenced by a community prompt.
TL;DR summarization
Summarize text by adding a 'tl;dr:' to the end of a text passage. It shows that the API understands how to perform a number of tasks with no instructions.
Python bug fixer
Find and fix bugs in source code.
Spreadsheet creator
Create spreadsheets of various kinds of data. It's a long prompt but very versatile. Output can be copy+pasted into a text file and saved as a .csv with pipe separators.
JavaScript helper chatbot
Message-style bot that answers JavaScript questions
ML/AI language model tutor
Bot that answers questions about language models
Science fiction book list maker
Create a list of items for a given topic.
Tweet classifier
Basic sentiment detection for a piece of text.
Airport code extractor
Extract airport codes from text.
SQL request
Create simple SQL queries.
Extract contact information
Extract contact information from a block of text.
JavaScript to Python
Convert simple JavaScript expressions into Python.
Friend chat
Emulate a text message conversation.
Mood to color
Turn a text description into a color.
Write a Python docstring
An example of how to create a docstring for a given Python function. We specify the Python version, paste in the code, and then ask within a comment for a docstring, and give a characteristic beginning of a docstring (""").
Analogy maker
Create analogies. Modified from a community prompt to require fewer examples.
JavaScript one line function
Turn a JavaScript function into a one liner.
Micro horror story creator
Creates two to three sentence short horror stories from a topic input.
Third-person converter
Converts first-person POV to the third-person. This is modified from a community prompt to use fewer examples.
Notes to summary
Turn meeting notes into a summary.
VR fitness idea generator
Create ideas for fitness and virtual reality games.
Essay outline
Generate an outline for a research topic.
Recipe creator (eat at your own risk)
Create a recipe from a list of ingredients.
Chat
Open ended conversation with an AI assistant.
Marv the sarcastic chat bot
Marv is a factual chatbot that is also sarcastic.
Turn by turn directions
Convert natural language to turn-by-turn directions.
Restaurant review creator
Turn a few words into a restaurant review.
Create study notes
Provide a topic and get study notes.
Interview questions
Create interview questions.

## Bibliograf√≠a

[1] <https://huggingface.co/>

