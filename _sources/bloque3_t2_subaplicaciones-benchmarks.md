
T2. Aplicaciones espec√≠ficas y Benchmacks
====================================

```{admonition} Nota
:class: note
Lee con atenci√≥n el tema 2 del bloque 2. Realiza las lecturas propuestas y finalmente contesta el cuestionario que encontrar√°s en la secci√≥n de evaluaci√≥n relativo a este tema, el cual se encuentra en el √≠ndice del bloque 2.  En la clase presencial repasaremos los conceptos te√≥ricos principales correspondoentes a la sesi√≥n. **El plazo para realizar las lecturas y el cuestionario es: **plazo l√≠mite: 23:59 del 28/02/2023**  (el d√≠a anterior a la clase presencial).

Tiempo de dedicaci√≥n: 1 hora 20 minutos (as√≠ncrona) + 1 hora trabajo independiente
```

Contenidos:

- [Aplicaciones espec√≠ficas](#aplicaciones-especificas)
- [BenchMarks y competiciones cient√≠ficas](#benchmarks-y-competiciones-cientificas)


## Aplicaciones espec√≠ficas

Dentro de las aplicaciones generales del PLN existen una **gran diversidad de aplicaciones** espec√≠ficas y variantes. A continuaci√≥n, se mencionan **algunas de ellas**.

### An√°lisis de sentimientos

El **an√°lisis de sentimientos(SA)** (o miner√≠a de opiniones/emociones) se refiere al uso del procesamiento del lenguaje natural para **identificar, extraer, cuantificar y estudiar** sistem√°ticamente **estados afectivos e informaci√≥n subjetiva**. Se suele aplicar contenido proveniente de **redes sociales** y en l√≠nea, y **contenidos** de cualquier otra √≠ndole que ofrezcan un **criterio formulado**. Sus **aplicaciones de dominio** desde el **marketing** hasta el **servicio al cliente** y la **medicina cl√≠nica**.

Entre los mayores **promotores** del estudio del an√°lisis de sentimientos para idiomas iberoamericanos podemos encontrar el **TASS** (Taller de An√°lisis Sem√°ntico de la [SEPLN]).

[SEPLN]: http://tass.sepln.org/

#### Reputaci√≥n social

La reputaci√≥n de una entidad social (una **persona**, un **grupo social**, una **organizaci√≥n**, **marca** o un **lugar**) es una **opini√≥n sobre esa entidad**, t√≠picamente como resultado de una **evaluaci√≥n social** sobre un conjunto de **criterios**, como el comportamiento o el desempe√±o. Cuando a esto le a√±adimos el **componente social** digital nos trasladamos entonces a crear mecanismo que **cuantifiquen dicha opini√≥n**.
Existen m√∫ltiples **estrategias** para **cuantificar** la **opini√≥n social**, una de ellas es considerar elementos como,

**Subjetivos:**

- criterios positivos/negativos/neutrales
- intensidad de los criterios
- manifestaci√≥n de emociones
- otros

**Objetivos:**

- likes
- visualizaciones
- contenidos compartidos
- audiencia
- otros

Con la **combinaci√≥n** de estos **criterios** podemos formular una **valoraci√≥n de la reputaci√≥n** la cual nos permite establecer criterios de **comparaci√≥n** y **ordinalidad**.

A continuaci√≥n, se muestra un ejemplo tomado de `[1]` basa en la **herramienta** [**Social Analytics**](https://socialanalytics.gplsi.es/):

```{image} /images/bloque3/t2/gplsisa.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 700px
:align: center
```

Figura 1. Analitica de datos para la **monitorizaci√≥n de marcas**

```{image} /images/bloque3/t2/reputacion.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 400px
:align: center
```

Figura 2. Predicci√≥n de la **reputaci√≥n**

#### Identificaci√≥n de Haters o trols

El **contenido (lenguaje) de odio** en l√≠nea, o discurso de odio, se **caracteriza** por algunos **aspectos** clave (como la **viralidad** o el presunto **anonimato**) que lo **distinguen** de la comunicaci√≥n fuera de l√≠nea y lo hacen **potencialmente m√°s peligroso y da√±ino**. Por tanto, su **identificaci√≥n** se convierte en una **misi√≥n crucial** en muchos campos.

Mediante el uso de t√©cnicas de **SA** podemos **modelar lenguajes** con connotaci√≥n negativa (**da√±ina**) para su **seguimiento** en el tiempo.

```{image} /images/bloque3/t2/hate.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```

Figura 3. Trols y Haters. Tomado de <https://www.dw.com/en/german-anti-hate-speech-group-counters-facebook-trolls/a-38358671>

#### CiberBullying

Hay evidencias que apuntan a que el **Cyberbullyng** es un **fen√≥meno** que cada vez m√°s est√° **presente** entre los **actuales problemas** de nuestros **j√≥venes**. El bullyng **siempre ha existido**, pero con la aparici√≥n de las **redes sociales** y medios de **comunicaci√≥n instant√°neos** su **efecto** se ha hace **global** y con **mayor impacto**  ``[2]``. 

El cyberbullying **consiste** en **enviar mensajes instant√°neos** o en un chat para **herir a una persona**, **publicar fotos o videos vergonzosos** en las redes sociales y **crear rumores** en l√≠nea.

¬øComo el PLN puede intervenir en este problema? Pues creando **tecnolog√≠as de detecci√≥n precoz** de **mensajes de acoso, agresivos, vergonzosos**, dise√±ar algoritmos para **identificar** los distintos **roles** (acosado, acosador, etc.) y **derivar** dicha informaci√≥n a las **autoridades competentes**.

```{image} /images/bloque3/t2/bullyng.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 4. Cyberbullyng. Tomado de [3]

Estudio de **fen√≥menos sociol√≥gicos** y como el PLN puede ayudar: 

- Ver [**Proyecto Life**]

[Proyecto Life]: https://gplsi.dlsi.ua.es/gplsi13/en/node/245

#### NER (Name Entity Recognition) y sus aplicaciones

Named Entity Recognition, (**NER**) es la tarea o aplicaci√≥n de NLP que **ayuda** a **entender** el **qu√©**, **qui√©n** y **d√≥nde** de una serie de **documentos** [4].

Naci√≥ como sustituto de la extracci√≥n de informaci√≥n relevante de los textos, se pens√≥:

- ‚Äúsi al menos tengo qu√© se ha hecho, qui√©n lo ha hecho y d√≥nde, tendr√© informaci√≥n relevante de un documento‚Äù, y as√≠ es.

**Objetivo del NER**. Es **identificar** personas, organizaciones y localizaciones. **3 etiquetas b√°sicas**:

- **Personas** üßë‚Äçü§ù‚Äçüßë
- **Organizaciones** üèõÔ∏è
- **Localizaciones** üëá

Tambi√©n podemos encontrar NER de **7 etiquetas**:

- **Personas** üßë‚Äçü§ù‚Äçüßë
- **Organizaciones** üèõÔ∏è
- **Localizaciones** üëá
- **Tiempo** ‚è≤Ô∏è
- **Moneda** üí∂
- **Calles** üó∫Ô∏è
- **Colectivos** üë© üë®

Luego podemos tener NER para **dominios espec√≠ficos** como por ejemplo para **dominio cl√≠nico**:

- **F√°rmacos** üíä
- **Sustancias** ‚öóÔ∏è
- **Enfermedades** ü§ß
- **Diagn√≥sticos** ü©∫
- **Procedimientos** üè•
- etc.

Ejemplo: Tomado de la competici√≥n ‚ÄúCodiEsp: Clinical Case Coding in Spanish‚Äù [5] en la que se aplica NER para codificar terminolog√≠as en documentos cl√≠nicos.

```{image} /images/bloque3/t2/ner.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```

Figura 5. NER en documentos cl√≠nicos

#### Categorizaci√≥n de Noticias

El **contenido de prensa** suele utilizar diferentes **estilos de redacci√≥n y presentaci√≥n** del contenido para **captar una mayor atenci√≥n** de la audiencia. Entre los distintos **criterios** que pudi√©ramos necesitar caracterizar. 

Las noticias de prensa podemos **clasificarlas en**:

- **Propaganda**,
- **Informativa**,
- **De opini√≥n**,

**Por su tem√°tica**:

- **Deporte**,
- **Pol√≠tica**,
- **Sociedad**,
- **Econom√≠a**,
- **Educaci√≥n**,
- otros

**Por su √°mbito** en:

- **Local**,
- **Regional**
- **Nacional**
- **Internacional**


##### FakeNews/Bulos

Con respecto a **valorar la calidad o fiabilidad de las noticias** surge un nuevo, pero ya antiguo en su origen, concepto que **busca corroborar la veracidad** de los **contenidos informativos** o **de opini√≥n**, [Fake News](https://doi.org/10.1016/j.eswa.2019.112943). 

Dentro de las Fake News existen varios **problemas** que se pueden tratar con PLN:

- **S√°tira o parodia**
- **Falsa Conexi√≥n** (no relaci√≥n entre el titular y el contenido)
- **Contenido Enga√±oso**
- **Contenido Falso**
- **Contenido Impostor**
- **Contenido Manipulado**
- **Contenido Fabricado**

**Acciones** recomendadas para **identificar** noticias falsas:

```{image} /images/bloque3/t2/accionesFN.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 400px
:align: center
```
Figura 6. Acciones. Tomado de <https://www.ccbiblio.es/infografia-ifla-detectar-noticias-falsas/>

Estas **acciones** se pueden **automatizar** en funci√≥n de la **creaci√≥n** de **tecnolog√≠as de PLN** para el combate de las FN.

**T√©cnicas** para **identificar** noticias falsas:

- [Fact Checking](http://nlp.kiv.zcu.cz/projects/fact-checking;jsessionid=917106793AD9747F0496053855C69231) (**Verificaci√≥n de hechos**)
- [Stance detection](http://nlpprogress.com/english/stance_detection.html) (**Detecci√≥n o determinaci√≥n de posturas**)
- [Sentiment Polarity](https://ieeexplore.ieee.org/document/8844880) (**Analisis de Sentimientos**)
- [Deception detection](https://doi.org/10.1002/pra2.2015.145052010082) (**Detecci√≥n de enga√±os**)
- [Clickbait detection](https://www.aclweb.org/anthology/W17-4215.pdf) (**Atracci√≥n de la atenci√≥n y animar a dar click**)
- [Credibility](https://doi.org/10.1007/978-3-030-42699-6_9) (**Credibilidad**)
- [Writing style](https://home.ipipan.waw.pl/p.przybyla/bib/capturing.pdf) (**Estilo de escritura** (Sensacionalisamo))

**Eventos/Campa√±as** en los que se promueve el desarrollo de de tecnolog√≠as para enfrentar la problem√°tica de las **FakeNews**:

- Fever: <https://fever.ai/>
- FakeNews Detection Challenge: <https://www.kaggle.com/c/fakenewskdd2020>

**Benchmarks:**

- Fake News Detection: <https://paperswithcode.com/task/fake-news-detection/codeless>

Ejemplo:

- **Entrada:** Texto de la noticia
- **Salida:** ``"real"`` o ``"fake"``

Ver material de estudio recomendado sobre FakeNews ``[6]``:


### Semantic textual similarity (similitud textual sem√°ntica)

La similitud textual sem√°ntica trata de **determinar** qu√© tan **similares** son **dos piezas de texto**. Esto puede tomar la forma de **asignar una puntuaci√≥n** del 1 al 5. Las **tareas relacionadas** son la **identificaci√≥n parafraseada** o **duplicada**.
Por ejemplo, tomado de nlp-progress (<http://nlpprogress.com/english/semantic_textual_similarity.html>):

- **Premisa**: ``‚ÄúFuga de amon√≠aco l√≠quido mata a 15 personas en Shanghai‚Äù``
- **Hipotesis**: **‚ÄúFuga de amon√≠aco l√≠quido mata al menos a 15 personas en Shanghai‚Äù**
- **Salida**: 4.6

[1]: https://arxiv.org/pdf/1803.05449.pdf

### Paraphrase identification (identificaci√≥n de parafrases)

La detecci√≥n de **par√°frasis** es un **problema de clasificaci√≥n** de PLN donde dado un **par de oraciones**, el sistema determina la **similitud sem√°ntica** entre las dos oraciones. Si las dos oraciones **tienen el mismo significado**, entonces se etiqueta como **par√°frasis**; de lo **contrario**, se etiqueta como **no parafraseado**.

Esta t√©cnica permite se puede **aplicar**, por ejemplo, al problema de **detecci√≥n de plagio**, ya que no solo mide el ***match*** **l√©xico** entre ambos textos, sino que tambi√©n el ***match*** **sem√°ntico**.

Ejemplo tomado de [1]:

- **Premisa**: ``‚ÄúEl procedimiento generalmente se realiza en el segundo o tercer trimestre "``
- **Hipotesis**: ``‚ÄúLa t√©cnica se utiliza durante el segundo y, ocasionalmente, el tercer trimestre del embarazo.‚Äù``
- **Salida**:  ``parafrase``

#### Natural language inference (Inferencia del Language Natural)

La inferencia del lenguaje natural es la tarea de **determinar si** una **"hip√≥tesis" es verdadera** (implicaci√≥n), **falsa** (contradicci√≥n) o **indeterminada** (neutral) dada una **"premisa"**[NLP Prog TI].

##### Contradicci√≥n

- **Premisa**: ``‚ÄúUn hombre inspecciona todo el recinto antes de abrir las puertas.‚Äù``
- **Hip√≥tesis**: ``‚ÄúEl hombre duerme‚Äù``
- **Resultado**: contradicci√≥n

##### Neutral

- **Premisa**: ``"Un hombre mayor y uno m√°s joven sonr√≠en."``
- **Hip√≥tesis**: ``"Dos hombres sonr√≠en y se r√≠en de los gatos que juegan en el suelo."``
- **Resultado**: Neutral

##### Implicaci√≥n

- **Premisa**: ``‚ÄúUn partido de f√∫tbol con varios hombres jugando.‚Äù``
- **Hip√≥tesis**: ``‚ÄúAlgunos hombres practican alg√∫n deporte.‚Äù``
- **Resultado**: Implicaci√≥n

[NLP Prog TI]: http://nlpprogress.com/english/natural_language_inference.html

#### Textual entailment (implicaci√≥n textual)

Describe el problema en el que se requiere **reconocer** la **relaci√≥n direccional** **entre fragmentos de texto**. La relaci√≥n se mantiene siempre que la **verdad de un fragmento de texto** se **sigue** de **otro texto**.
Los **textos vinculantes** y **vinculados** se denominan **texto (``t``)** e **hip√≥tesis (``h``)**, respectivamente.

**"t implica h" (``t`` ‚áí ``h``)** si, t√≠picamente, una lectura humana ``t`` inferir√≠a que ``h`` es probablemente verdadera.

La relaci√≥n es direccional porque incluso si "``t`` implica ``h``" , la inversa "``h`` implica ``t``" es mucho menos segura. [TE]

- **Premisa (t)**: ``‚ÄúUna ni√±a peque√±a con una chaqueta rosa est√° montada en un carrusel‚Äù``
- **Hip√≥tesis(h)**: ``‚ÄúEl carrusel se mueve‚Äù``
- **Resultado**: implicaci√≥n

[TE]: http://clg.wlv.ac.uk/events/CALP07/papers/6.pdf

##### Topic Analysis (Modelado de temas)

El **an√°lisis de temas** (tambi√©n llamado detecci√≥n de temas, modelado de temas o extracci√≥n de temas) es una t√©cnica de aprendizaje autom√°tico que **organiza** y **comprende grandes colecciones de datos de texto**, mediante la **asignaci√≥n de etiquetas** o **categor√≠as** seg√∫n el tema o tema de cada texto individual.

**¬øQu√© es un tema en el modelado de temas?**
En PLN el modelo de tema es un **tipo de modelo estad√≠stico** para **descubrir** los **temas abstractos** que ocurren en una **colecci√≥n de documentos**. Los **temas** producidos por las t√©cnicas de modelado de temas **son grupos de palabras similares**.

Dado que un **documento trata** sobre un **tema** en particular, uno esperar√≠a que **aparecieran palabras espec√≠ficas** en el documento con m√°s o menos **frecuencia**.
Veamos un ejemplo de [TopicModeling1], [TopicModeling2]:

- ``"perro"`` y ``"hueso"`` **aparecer√°n** con m√°s **frecuencia en documentos sobre perros**,
- ``"gato"`` y ``"miau"`` aparecer√°n en documentos **sobre gatos**,
- y ``"el"`` y ``"es"`` aparecer√°n aproximadamente por **igual en ambos**.

A continuaci√≥n, dos **m√©todos** de modelado de temas:

- **An√°lisis sem√°ntico latente** (LSA). Ver m√°s en [LSA1] [LSA-LDA]
- y **Asignaci√≥n de Dirichlet latente** (LDA1). Ver m√°s en [sklearnLDA] [LSA-LDA]

[TopicModeling1]: https://monkeylearn.com/blog/introduction-to-topic-modeling/
[TopicModeling2]: https://www.kaggle.com/rcushen/topic-modelling-with-lsa-and-lda
[sklearnLDA]: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html
[LSA1]: https://www.analyticsvidhya.com/blog/2018/10/stepwise-guide-topic-modeling-latent-semantic-analysis/
[LSA-LDA]: https://www.kaggle.com/rcushen/topic-modelling-with-lsa-and-lda

#### Event extraction(Extracci√≥n de eventos)

Es el proceso de **recopilar conocimiento** sobre **incidentes peri√≥dicos** que se encuentran en los **textos**, **identificando** autom√°ticamente informaci√≥n sobre lo **qu√©** sucedi√≥ y **cu√°ndo** sucedi√≥.

Un elemento **vital** es **identificar** el **tema central** para conseguir su **seguimiento**. Tomado de [EventExtraction]

Por ejemplo:
```
1. 2018/10 ‚Äî ``El gobierno del presidente Donald Trump **prohibi√≥** a los pa√≠ses **importar petr√≥leo iran√≠** con exenciones a siete pa√≠ses.
2. 2019/04 ‚Äî El secretario de Estado de Estados Unidos, Mike Pompeo, **anunci√≥** que su pa√≠s **no abrir√≠a m√°s excepciones** despu√©s de la fecha l√≠mite.
3. 2019/05 ‚Äî Estados Unidos **termin√≥ con exenciones** que permit√≠an a los pa√≠ses **importar petr√≥leo de Ir√°n** sin sufrir las sanciones estadounidenses.
```
La **extracci√≥n de eventos** brinda la **capacidad de contextualizar** la informaci√≥n nos permite **conectar eventos** distribuidos **en el tiempo** y asimilar **sus efectos**, y c√≥mo se desarrolla un conjunto de episodios a lo largo del tiempo.

[EventExtraction]: https://towardsdatascience.com/natural-language-processing-event-extraction-f20d634661d3

##### Biomedical Event Extraction (Extracci√≥n de eventos en textos biom√©dicos)

Por otro lado, podemos ver **otro tipo de extracciones de eventos** para **dominios concretos** como por el ejemplo en **textos biom√©dicos**. Ver m√°s detalles en [KaggleEvent] y [eHealthKD2020].

```{image} /images/bloque3/t2/ehealth.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```
Figura 7. Extracci√≥n de entidades, eventos y relaciones

[KaggleEvent]: https://paperswithcode.com/dataset/genia
[eHealthKD2020]: https://knowledge-learning.github.io/ehealthkd-2020/

### Otras aplicaciones del PLN

Son **muchas las aplicaciones del PLN**, en los siguientes apartados se abordar√°n **copeticiones cient√≠ficas** y **marcos de referencia** en los cuales se aglutinan la mayor parte de estas aplicaciones.

```{image} /images/bloque3/t2/aplicacionesporareas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 800px
:align: center
```
Figura 8. T√©cnicas y aplicaciones por areas. Seg√∫n [Wikipedia](https://en.wikipedia.org/wiki/Natural_language_processing)


## Benchmarks y competiciones cient√≠ficas

### Definiciones sobre Benchmark

**Econom√≠a y finanzas:**
El **benchmark** es un **punto de referencia** utilizado para **medir el rendimiento de una inversi√≥n**. Se trata de un **indicador financiero** utilizado como herramienta de **comparaci√≥n** para **evaluar el rendimiento** de una inversi√≥n. [1](https://economipedia.com/definiciones/benchmark.html)

**Inform√°tica:**
Una **prueba de rendimiento** o **comparativa** (en ingl√©s benchmark) es una t√©cnica utilizada para **medir el rendimiento de un sistema** o uno de sus **componentes**‚Ä¶[3](https://es.wikipedia.org/wiki/Benchmark_(inform√°tica)

**Ventajas**:

- **Marco com√∫n de comparaci√≥n** bajo los **mismos criterios**, **recursos** y **est√°ndares**
- A menudo **disponibilidad de datasets** para el desarrollo de de problema a abordar
- Disponibilidad de **m√©tricas comunes** de evaluaci√≥n
- Compartici√≥n y a menudo **disponibilidad colectiva de conocimiento** sobre c√≥mo abordar el problema y ejemplos de iniciaci√≥n.
- Disponibilidad de **sistemas base** (baselines) como recurso b√°sico de comparaci√≥n

Desventajas:

- Obligatoriedad de **adecuar y adaptar nuestros sistemas** y tecnolog√≠as al **marco com√∫n**
- **Limitaciones** de nuestros **sistemas** al **objetivo del reto**
- Es preciso **medir nuestro sistema** con la **m√©tricas ya establecidas en el reto** lo cual puede no ser una forma fiel de medir su calidad


**Campa√±as de evaluaci√≥n m√°s populares** en Procesamiento del Lenguaje Natural:

- [Semeval](https://semeval.github.io/)
- [CLEF](http://www.clef-initiative.eu/)
- [TREC](https://trec.nist.gov/)
- [TAC](https://tac.nist.gov/)
- [IberLEF](https://sites.google.com/view/iberlef2021)
- otros.

**Marco gen√©rico de un concurso**

```{image} /images/bloque3/t2/challenge.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 800px
:align: center
```
Figura 9. Marco de trabajo de los concursos

**Infraestructura de concursos**

```{image} /images/bloque3/t2/challenge2.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 200px
:align: center
```
Figura 10. Infraestructura de concursos

**Repositorios**: [Github](http://github.com), [Gitlab](http://gitlab.com), Servidores online, etc.

**Infraestructura de Evaluaci√≥n**: [Colab](https://colab.research.google.com/), [Kaggle](https://www.kaggle.com/), etc.

### SenseEval/SemEval

URL: <https://semeval.github.io/>

**Lista de ediciones**:

- Senseval-1 (1998)
- Senseval-2 ( 2001)
- Senseval-3 (2004)
- SemEval-2007 (Senseval-4) (2007)
- SemEval-2010 (2010)
- SemEval-2012 (2012)
- SemEval-2013 (2013)
- ...
- Semeval-2020 (2020)
- Semeval-2021 (2021)
- Semeval-2022 (2022)
- Semeval-2023 (2023)

Por qu√© **primero** SensEval (Sens de sentido o significado y Eval de evaluaci√≥n) y luego SemEval (Sem de **sem√°ntica**). Pues porque Senseval-1 (1998) y Senseval-2 ( 2001) se centraron en **tareas de desambiguaci√≥n sem√°ntica** y ya **luego** fueron **incorpor√°ndose** otros tipos de **tareas de PLN** lo que di√≥ lugar a un cambio de nombre SemEval que pudiera ampliar el marco de representaci√≥n de dicho nombre.

```{image} /images/bloque3/t2/semeval.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```
Figura 11. Tareas Semeval

### CLEF

URL: <http://www.clef-initiative.eu/>

**CLEF**(inicios a√±o 2000) obtiene sus siglas en ingl√©s de **Conference and Labs of the Evaluation Forum**, y formalmente **conocido** como **Cross-Language Evaluation Forum**,  es un organismo auto-organizado cuya misi√≥n principal es **promover la investigaci√≥n**, la innovaci√≥n y el **desarrollo de sistemas de acceso a la informaci√≥n** con √©nfasis en la informaci√≥n **multiling√ºe** y **multimodal** con varios niveles de estructura.

<!---  
**CLEF promueve la investigaci√≥n** y el desarrollo proporcionando una infraestructura para:

- prueba, ajuste y evaluaci√≥n de sistemas multiling√ºes y multimodales;
- investigaci√≥n del uso de datos no estructurados, semiestructurados, altamente estructurados y sem√°nticamente enriquecidos en el acceso a la informaci√≥n;
- creaci√≥n de datasets reutilizables para evaluaci√≥n comparativa;
- exploraci√≥n de nuevas metodolog√≠as de evaluaci√≥n y formas innovadoras de utilizar datos experimentales;
- discusi√≥n de resultados, comparaci√≥n de enfoques, intercambio de ideas y transferencia de conocimientos

--->

```{image} /images/bloque3/t2/clef1.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```

Figura 12. Tareas CLEF

```{image} /images/bloque3/t2/clef2.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```

Figura 13. Durabilidad de tareas CLEF

**Tareas**:

1. IR: Ad-Hoc Track editions in CLEF. Multilingual Document Retrieval track in all the CLEF Editions.(<http://www.clef-initiative.eu/track/adhoc>)
2. IR: Domain-Specific Track editions in CLEF. Scientific Data Retrieval (Domain-Specific) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/domainspecific>
3. IR: iCLEF Track editions in CLEF. Interactive Cross-Language Retrieval (iCLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/iclef>
4. IR: CL-SR Track editions in CLEF. Cross-Language Speech Retrieval (CL-SR) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/clsr>
5. QA: QA@CLEF Track editions in CLEF. Multiple Language Question Answering (QA@CLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/qaclef>
6. IIR: ImageCLEF Track editions in CLEF. Cross-Language Retrieval in Image Collections (ImageCLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/imageclef>
7. IE,Sum, IF: WebCLEF Track editions in CLEF. Multilingual Web Track (WebCLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/webclef>
8. GIR: GeoCLEF Track editions in CLEF. Cross-Language Geographical Information Retrieval (GeoCLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/geoclef>
9. VIR: VideoCLEF Track editions in CLEF. Cross-LanguageVideo Retrieval (VideoCLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/videoclef>
10. IF: INFILE Track editions in CLEF. Multilingual Information Filtering (INFILE) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/infile>
11. GIR: LogCLEF Track editions in CLEF. Log File Analysis (LogCLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/logclef>
12. IR: CLEF-IP Track editions in CLEF. Intellectual Property (CLEF-IP) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/clefip>
13. IR: Grid@CLEF Track editions in CLEF. Grid Experiments (Grid@CLEF) track in all the CLEF Editions. <http://www.clef-initiative.eu/track/gridclef>
14. AP: Author Profiling 2017. <https://pan.webis.de/clef17/pan17-web/author-profiling.html>
15. WS ,IR: WePS 3: searching information about entities in the Web. <http://nlp.uned.es/weps/weps-3>
16. SS: Cross-lingual Expert Search - Bridging CLIR and Social Media (CriES).
17. IR: MusiClef: Multimodal Music Tagging Task. MusiClef  encourages multimodal  Music Information Retrieval approaches. <http://www.cp.jku.at/people/schedl/Research/Publications/pdf/musiclef_clef_2012.pdf>
18. TC: CLEF-INEX 2014 Tweet Contextualization Track. <https://hal-amu.archives-ouvertes.fr/hal-01479297>
19. IR: CHiC Cultural Heritage in CLEF - The CLEF Initiative. <http://www.promise-noe.eu/chic-2011/home>
20. AP, Reputation: RepLab Editions in CLEF - The CLEF Initiative. Reputation dimensions classification and author profiling. <http://www.clef-initiative.eu/track/replab>
21. IR, IE, IM: CLEF eHealth. * Information extraction (IE), Information management,  Information retrieval. <https://clefehealth.imag.fr/>
22. ER: CLEF-ER. Name Entity Recognition. <https://sites.google.com/site/mantraeu/clef-er>


**Layenda**:

- AP- Author Profiling (2)
- IR - Information Retrieval (10)
- GIR - Geographic IR (2)
- IIR - Image IR (1)
- VIR - Video IR (1)
- TC - Text comprehension (1)
- SS - Social Search (1)
- WS - Web Search (1)
- IF - Information Filtering (2)
- IE - Information Extraction (2)
- IM - Information Management (1)
- ER- Entity Recognition (1)
- Sum ‚Äì Summarisation (1)

#### Datasets and Corpora

Este marco de evaluaci√≥n como la mayor√≠a **proporciona** a los investigadores **datasets y corpus** de entrenamiento y evaluaci√≥n.

### TREC: Text REtrieval Conference

URL:  <https://trec.nist.gov/>

**Propone:**

- **fomentar** la investigaci√≥n en la **recuperaci√≥n de informaci√≥n** basada en **grandes colecciones** de pruebas;
- acelerar la **transferencia de tecnolog√≠a** de los laboratorios de investigaci√≥n a productos comerciales demostrando mejoras sustanciales en las metodolog√≠as de recuperaci√≥n de problemas del mundo real;
- ...

**Tareas y datos**:

- Versions of trec_eval
- Ad hoc Test Collections
- Web Test Collections
- Blog Track
- Chemical IR Track
- Clinical Decision Support Track
- Common Core Track
- Confusion Track
- Contextual Suggestion Track
- Conversational Assistance Track
- Crowdsourcing Track
- Dynamic Domain Track
- Decision Track
- Deep Learning Track
- Enterprise Track
- Entity Track
- Filtering Track
- Federated Web Search Track
- Genomics Track
- HARD Track
- Interactive Track
- Knowledge Base Acceleration Track
- Legal Track
- Medical Track
- Medical Misinformation Track
- Microblog Track
- Million Query Track
- News Track
- Novelty Track
- Query Track
- Question Answering Track
- Precision Medicine Track
- Real-time Summarization Track
- Relevance Feedback Track
- Robust Track
- Session Track
- SPAM Track
- Spoken Document Retrieval Track
- Tasks Track
- Temporal Summarization Track
- Terabyte Track
- Web Track

### TAC. Text Analysis Conference

URL: <https://tac.nist.gov/>

 La **Conferencia de An√°lisis de Texto** (TAC) se conforma por una serie de **talleres de evaluaci√≥n** organizados para fomentar la investigaci√≥n en el **procesamiento del lenguaje natural** y **aplicaciones relacionadas**. **Proporciona** una gran **colecci√≥n de pruebas, procedimientos de evaluaci√≥n comunes** y un foro para que las organizaciones compartan sus resultados. 
 
 TAC comprende conjuntos de **tareas conocidas como "pistas"**, cada una de las cuales se **centra en un subproblema particular** del PLN. Los tracks de TAC se centran en las **tareas del usuario final**, pero tambi√©n incluyen evaluaciones de componentes situadas dentro del contexto de estas.

 ```{image} /images/bloque3/t2/tablaTAC.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```

Figura 14. Tareas TAC

```{image} /images/bloque3/t2/graphTAC.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```

Figura 15. Tareas CLEF

### IberLEF: Iberian Languages Evaluation Forum

IberLEF es una **campa√±a de evaluaci√≥n** comparativa de sistemas de **Procesamiento de Lenguaje Natural** en **espa√±ol y otras lenguas ib√©ricas**.

Su objetivo es alentar a la comunidad investigadora a **organizar tareas competitivas** de **procesamiento**, **comprensi√≥n** y **generaci√≥n de textos** para definir nuevos desaf√≠os de investigaci√≥n y establecer nuevos resultados de vanguardia en esos idiomas.

#### Ediciones

- TASS 2012-2020: <http://tass.sepln.org/>

- IberEval 2017: <https://sepln2017.um.es/ibereval.html>
- IberEval 2028: <https://sites.google.com/view/ibereval-2018>

- Iberlef2019: <https://sites.google.com/view/iberlef-2019>
- Iberlef2020:  <https://sites.google.com/view/iberlef2020/home>
- Iberlef2021: <https://sites.google.com/view/iberlef2021>
- Iberlef2022: <https://sites.google.com/view/iberlef2022>
- Iberlef2023: <https://sites.google.com/view/iberlef-2023>


### Benchmarks e infraestructuras de evaluaci√≥n

#### [CodaLab]( https://codalab.org/) [cuadernos de trabajo, concursos]

CodaLab es una **plataforma de c√≥digo abierto** que proporciona un **ecosistema** para realizar **investigaci√≥n computacional** de una manera m√°s eficiente, **reproducible y colaborativa**.

Hay dos **aspectos de CodaLab**:

- **Hojas de trabajo**: permiten capturar **l√≠neas de investigaci√≥n** complejas de una manera **reproducible** y crear **"documentos ejecutables"**. Se puede utilizar **cualquier formato** de **datos** o **lenguaje** de programaci√≥n.
- **Concursos** (competiciones): Estos sirven para **reunir** a la **comunidad** cient√≠fica para **abordar los problemas** inform√°ticos y de datos m√°s desafiantes de la actualidad. Puedes **ganar premios** y tambi√©n crear tu propia competencia.

#### [Kaggle](https://www.kaggle.com/) [concursos, conjunto de datos, c√≥digos fuente]

Entidad **subsidiaria de Google LLC**, permite a los usuarios **encontrar y publicar conjuntos de datos, explorar y construir modelos** en un entorno de ciencia de datos basado en la web, trabajar **organizar concursos** para resolver desaf√≠os de ciencia de datos.

#### [GLUE](https://gluebenchmark.com) [tabla de rankings, conjunto de datos, c√≥digos fuente]

El **marco de referencia** de **evaluaci√≥n** de comprensi√≥n del lenguaje general (GLUE) es una **colecci√≥n de recursos** para **entrenar, evaluar y analizar sistemas de comprensi√≥n del lenguaje natural**. 

GLUE se compone de:

- Un **marco de referencia** (**Benckmark**) de **nueve tareas** de comprensi√≥n del lenguaje de oraciones o pares de oraciones basadas en conjuntos de datos existentes establecidos y seleccionados para cubrir una amplia gama de tama√±os de conjuntos de datos, g√©neros de texto y grados de dificultad.
- Un **conjunto de datos** (**datasets**) de diagn√≥stico dise√±ado para evaluar y analizar el rendimiento del modelo con respecto a una amplia gama de fen√≥menos ling√º√≠sticos que se encuentran en el lenguaje natural, y
- Una **tabla de ranking p√∫blica** (**leaderboard**) para **reflejar** el **desempe√±o** en el Benckmark y un **tablero para visualizar el desempe√±o** de los **modelos** en el conjunto de diagn√≥stico.

El **formato** Benckmark GLUE es **independiente del modelo**, por lo que **cualquier sistema capaz de procesar oraciones y pares de oraciones y producir las predicciones correspondientes es elegible para participar**. <!--Las tareas de referencia se seleccionan para favorecer los modelos que comparten informaci√≥n entre tareas utilizando el uso compartido de par√°metros u otras t√©cnicas de aprendizaje por transferencia.-->
Las tareas recogidas en el marco de GLUE actualmente ofrecen **rendimientos cercanos al nivel de humanos no expertos**, lo que sugiere un **margen limitado** para **futuras** investigaciones.

- Tareas: <https://gluebenchmark.com/tasks>
- Tablas de resultados: <https://gluebenchmark.com/leaderboard>

#### [SuperGLUE](https://super.gluebenchmark.com/) [tabla de rankings, conjunto de datos, c√≥digos fuente]

**SuperGLUE**, un **nuevo Benckmark** con el **estilo de GLUE** con un **nuevo conjunto de tareas** de comprensi√≥n del idioma **m√°s dif√≠ciles**, **recursos mejorados** y **una nueva tabla de clasificaci√≥n p√∫blica**.

#### [Huggingface ü§ó](https://huggingface.co/) [conjunto de datos, c√≥digo fuente, modelos]

Hugging Face es una **empresa emergente l√≠der en el PLN** con m√°s de mil empresas que utilizan sus bibliotecas de c√≥digo abierto (espec√≠ficamente: la **biblioteca** **Transformers**) en producci√≥n. La biblioteca Transformer basada en Python expone las API para usar r√°pidamente **arquitecturas NLP** como: **BERT** (Google, 2018)

La librer√≠a Transformers **proporciona**:

- miles de **modelos previamente entrenados** para realizar tareas en textos como clasificaci√≥n, extracci√≥n de informaci√≥n, respuesta a preguntas, resumen, traducci√≥n, generaci√≥n de texto, etc. en m√°s de 100 idiomas.
- **API para descargar y usar** r√°pidamente esos **modelos previamente entrenados**
- est√° **respaldado/integrado** por librer√≠as como [**PyTorch**](https://pytorch.org/) y [**TensorFlow**](https://www.tensorflow.org/)

#### [Extreme](https://sites.research.google/xtreme) [tabla de rankings, conjunto de datos, c√≥digos fuente. papers, modelos]

**TRansfer Evaluation of Multilingual Encoders** ([Extreme]) es un benckmark para la **evaluaci√≥n** de la capacidad de **generalizaci√≥n** entre **idiomas de modelos multiling√ºes** previamente entrenados. Cubre **40 idiomas** tipol√≥gicamente diversos (que abarcan 12 familias de idiomas) e incluye **nueve tareas** que colectivamente requieren razonamiento sobre diferentes niveles de sintaxis y sem√°ntica.
[Extreme](https://github.com/google-research/xtreme)


## Bibliograf√≠a

``[1]`` <https://www.acl-bg.org/proceedings/2017/RANLP%202017/pdf/RANLP032.pdf>

``[2]`` Van Hee, C., Jacobs, G., Emmery, C., Desmet, B., Lefever, E., Verhoeven, B., ... & Hoste, V. (2018). Automatic detection of cyberbullying in social media text. PloS one, 13(10), e0203794.

``[3]`` <https://dac.cs.vt.edu/research-project/semi-supervised-learning-cyberbullying-harassment-patterns-social-media/>

``[4]`` <https://www.iic.uam.es/inteligencia/la-importancia-tener-ner/>

``[5]`` <https://temu.bsc.es/codiesp/>

``[6]`` Estela Saquete, David Tom√°s, Paloma Moreda, Patricio Mart√≠nez-Barco, Manuel Palomar: Fighting post-truth using natural language processing: A review and open challenges. Expert Syst. Appl. 141 (2020)