
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>3. Análisis categorial y sintáctico &#8212; Minería de Textos</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/estilos.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="4. Análisis semántico" href="bloque1_3AnalisisSemantico.html" />
    <link rel="prev" title="2. Minería de textos y procesamiento del lenguaje natural." href="bloque1_1Introduccion.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo-master-ca.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Minería de Textos</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="intro.html">
                    Materiales de Minería de Textos
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 1
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1.html">
   1. Introducción a la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_1Introduccion.html">
   2. Minería de textos y procesamiento del lenguaje natural.
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   3. Análisis categorial y sintáctico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_3AnalisisSemantico.html">
   4. Análisis semántico
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica1.html">
   5. Práctica 1.
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_4AnalisisSemanticoVectorial.html">
   6. Análisis semántico vectorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque1_Practica2.html">
   7. Práctica 1b :
   <em>
    Topic modeling
   </em>
   .
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3.html">
   8. Aplicaciones de la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t1_aplicaciones.html">
   9. T1. Aplicaciones generales
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2_subaplicaciones-benchmarks.html">
   10. T2. Aplicaciones específicas y Benchmacks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t2.1_analisis_sentimientos.html">
   11. T2.1. Aplicaciones específicas. Análisis de Sentimientos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t3.1_metricas.html">
   12. T3. Métricas de Evaluación
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t4_huggingface.html">
   13. T4. Centralización de datasets y modelos: Huggingface
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5_automl.html">
   14. T5. Auto Machine Learning(AutoML)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_t5.1_autogoal.html">
   15. T5.1. AutoGOAL
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p1_SA-Pipeline-Reviews.html">
   16. P1.1. Pipeline simple
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p2_SA-Transformers-Basic.html">
   17. P1.2. APIs Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p3_SA-Transformers-Training-FineTuning.html">
   18. P2. Reajustar modelos Transformers
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_p4_SA-Transformers-Training-Custom.html">
   19. P3. Composición de vectores de características
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque3_ev.html">
   20. Ev. Evaluación del bloque 2
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Bloque 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2.html">
   21. Técnicas para la minería de textos
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2_historia.html">
   22. Revisión histórica
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2_embeddings.html">
   23. Representaciones de palabras y oraciones
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="bloque2_practica.html">
   24. Práctica. Lectura y documentación del código de un extractor de entidades
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Extras
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="content.html">
   25. Content in Jupyter Book
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="markdown.html">
   26. Markdown Files
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="notebooks.html">
   27. Content with notebooks
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/bloque1_2CategorialSintactico.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unidades-de-comunicacion-basica-la-palabra-type-token-y-lema">
   3.1. Unidades de comunicación básica. La palabra.
   <em>
    Type
   </em>
   ,
   <em>
    token
   </em>
   y lema.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenizacion">
     3.1.1. Tokenización
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lematizacion-y-stemming">
     3.1.2. Lematización y
     <em>
      stemming
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-morfologico-y-categorial">
   3.2. Análisis morfológico y categorial.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algunos-conceptos-linguisticos">
     3.2.1. Algunos conceptos lingüísticos.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relevancia-del-analisis-categorial-en-mineria-de-textos">
     3.2.2. Relevancia del análisis categorial en Minería de textos
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representacion-formal-de-la-informacion-morfologica-y-categorial">
     3.2.3. Representación formal de la información morfológica y categorial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#arquitectura-de-un-pos-tagger">
     3.2.4. Arquitectura de un
     <em>
      PoS_tagger
     </em>
     .
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algoritmos-de-desambiguacion-categorial">
     3.2.5. Algoritmos de desambiguación categorial.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#modelo-basado-en-reglas-simbolicos">
       3.2.5.1. Modelo basado en reglas (simbólicos).
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#gramaticas-de-restricciones-constraint-grammar">
         3.2.5.1.1. Gramáticas de restricciones (
         <em>
          Constraint grammar
         </em>
         )
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#modelos-estadisticos">
       3.2.5.2. Modelos estadísticos
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#cadena-de-markov">
         3.2.5.2.1. Cadena de Markov
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#modelo-oculto-de-markov">
         3.2.5.2.2. Modelo oculto de Markov
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#conditional-random-fields-y-otros">
         3.2.5.2.3.
         <em>
          Conditional Random Fields
         </em>
         y otros.
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#situacion-actual">
     3.2.6. Situación actual
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recursos">
     3.2.7. Recursos.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-sintactico">
   3.3. Análisis sintáctico.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#arquitectura-estandar-de-un-parser">
     3.3.1. Arquitectura estándar de un
     <em>
      parser
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-de-representacion">
     3.3.2. Modelos de representación
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principal-problema-computacional">
     3.3.3. Principal problema computacional
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gramaticas-formales">
     3.3.4. Gramáticas formales
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#context-free-grammars">
       3.3.4.1. Context free grammars
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probabilistic-context-free-grammar-y-modelos-probabilisticos">
       3.3.4.2.
       <em>
        Probabilistic Context Free Grammar
       </em>
       y modelos probabilísticos
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chunkers">
     3.3.5.
     <em>
      Chunkers
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estrategias">
     3.3.6. Estrategias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formato-conll">
     3.3.7. Formato CONLL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     3.3.8. Situación actual
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#herramientas">
     3.3.9. Herramientas
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia">
   3.4. Bibliografía
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Análisis categorial y sintáctico</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#unidades-de-comunicacion-basica-la-palabra-type-token-y-lema">
   3.1. Unidades de comunicación básica. La palabra.
   <em>
    Type
   </em>
   ,
   <em>
    token
   </em>
   y lema.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#tokenizacion">
     3.1.1. Tokenización
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#lematizacion-y-stemming">
     3.1.2. Lematización y
     <em>
      stemming
     </em>
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-morfologico-y-categorial">
   3.2. Análisis morfológico y categorial.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algunos-conceptos-linguisticos">
     3.2.1. Algunos conceptos lingüísticos.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#relevancia-del-analisis-categorial-en-mineria-de-textos">
     3.2.2. Relevancia del análisis categorial en Minería de textos
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#representacion-formal-de-la-informacion-morfologica-y-categorial">
     3.2.3. Representación formal de la información morfológica y categorial
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#arquitectura-de-un-pos-tagger">
     3.2.4. Arquitectura de un
     <em>
      PoS_tagger
     </em>
     .
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#algoritmos-de-desambiguacion-categorial">
     3.2.5. Algoritmos de desambiguación categorial.
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#modelo-basado-en-reglas-simbolicos">
       3.2.5.1. Modelo basado en reglas (simbólicos).
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#gramaticas-de-restricciones-constraint-grammar">
         3.2.5.1.1. Gramáticas de restricciones (
         <em>
          Constraint grammar
         </em>
         )
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#modelos-estadisticos">
       3.2.5.2. Modelos estadísticos
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#cadena-de-markov">
         3.2.5.2.1. Cadena de Markov
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#modelo-oculto-de-markov">
         3.2.5.2.2. Modelo oculto de Markov
        </a>
       </li>
       <li class="toc-h5 nav-item toc-entry">
        <a class="reference internal nav-link" href="#conditional-random-fields-y-otros">
         3.2.5.2.3.
         <em>
          Conditional Random Fields
         </em>
         y otros.
        </a>
       </li>
      </ul>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#situacion-actual">
     3.2.6. Situación actual
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recursos">
     3.2.7. Recursos.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#analisis-sintactico">
   3.3. Análisis sintáctico.
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#arquitectura-estandar-de-un-parser">
     3.3.1. Arquitectura estándar de un
     <em>
      parser
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#modelos-de-representacion">
     3.3.2. Modelos de representación
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#principal-problema-computacional">
     3.3.3. Principal problema computacional
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#gramaticas-formales">
     3.3.4. Gramáticas formales
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#context-free-grammars">
       3.3.4.1. Context free grammars
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#probabilistic-context-free-grammar-y-modelos-probabilisticos">
       3.3.4.2.
       <em>
        Probabilistic Context Free Grammar
       </em>
       y modelos probabilísticos
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#chunkers">
     3.3.5.
     <em>
      Chunkers
     </em>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#estrategias">
     3.3.6. Estrategias
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#formato-conll">
     3.3.7. Formato CONLL
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     3.3.8. Situación actual
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#herramientas">
     3.3.9. Herramientas
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#bibliografia">
   3.4. Bibliografía
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="analisis-categorial-y-sintactico">
<span id="label-pos"></span><h1><span class="section-number">3. </span>Análisis categorial y sintáctico<a class="headerlink" href="#analisis-categorial-y-sintactico" title="Permalink to this headline">#</a></h1>
<div class="note admonition">
<p class="admonition-title">Nota</p>
<p>Para preparar este tema, consulta los capítulos 8,… de Juravsky y Martin (2022) <em>Speech and Language Processing</em>. <a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a>.</p>
</div>
<section id="unidades-de-comunicacion-basica-la-palabra-type-token-y-lema">
<h2><span class="section-number">3.1. </span>Unidades de comunicación básica. La palabra. <em>Type</em>, <em>token</em> y lema.<a class="headerlink" href="#unidades-de-comunicacion-basica-la-palabra-type-token-y-lema" title="Permalink to this headline">#</a></h2>
<p>Si bien el concepto de “palabra” se suele utilizar como unidad mínima y básica de comunicación, realmente desde la palabra no tiene en lingüística una definición clara: es un concepto vago muy difícil de delimitar.</p>
<p>En lingüística de corpus, lingüística computacional y procesamiento del lenguaje natural, más que con el concepto de “palabra”, se trabaja con los conceptos de <em>type</em> (“tipo”) y <em>token</em> (“caso”) (<a class="reference external" href="https://es.wikipedia.org/wiki/Caso_y_tipo">introducidos por el filósofo Charles S. Peirce</a> a principio de siglo XX):</p>
<ul class="simple">
<li><p><em>Type</em> es la palabra entendida como clase o tipo. Una secuencia de caracteres diferente de cualquier otra secuencia.</p></li>
<li><p><em>Token</em> es cada una de las instancias o casos concretos de esas clase <em>type</em> que se pueda hallar en un texto <a class="footnote-reference brackets" href="#id7" id="id1">1</a>.</p></li>
</ul>
<p>Se suele ejemplificar la diferencia entre ambos conceptos con el verso de G. Stein:</p>
<blockquote>
<div><p>“Rose is a rose is a rose is a rose”;</p>
</div></blockquote>
<p>pero para españolizarlo un poco vamos a coger como ejemplo el siguiente verso de <a class="reference external" href="https://www.youtube.com/watch?v=dv958EeZXHc">esta canción</a> de <em>Mecano</em>, que es una versión simplificada del verso de Stein:</p>
<blockquote>
<div><p>“Una rosa es una rosa es”.</p>
</div></blockquote>
<p>En este verso encontramos tres <em>types</em>:</p>
<ul class="simple">
<li><p>“una”</p></li>
<li><p>“rosa”</p></li>
<li><p>“es”;</p></li>
</ul>
<p>pero seis <em>tokens</em>: 2 <em>tokens</em> del <em>type</em> “una”, 2 del <em>type</em> “rosa” y 2 del <em>type</em> “es”. Son por tanto <span class="math notranslate nohighlight">\(2+2+2 = 6\)</span> <em>tokens</em>. Este texto está formado por seis <em>tokens</em> y tres <em>types</em>.</p>
<p>Como se puede comprobar, esta diferencia es la base conceptual del cálculo de frecuencias textuales. El cálculo más simple es contar, como se ha hecho antes, la cantidad de <em>tokens</em> de cada <em>type</em> en un texto:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p><em>type</em></p></th>
<th class="head"><p><em>tokens</em></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>una</p></td>
<td><p><span class="math notranslate nohighlight">\(2\)</span></p></td>
</tr>
<tr class="row-odd"><td><p>rosa</p></td>
<td><p><span class="math notranslate nohighlight">\(2\)</span></p></td>
</tr>
<tr class="row-even"><td><p>es</p></td>
<td><p><span class="math notranslate nohighlight">\(2\)</span></p></td>
</tr>
</tbody>
</table>
<p>En esta línea, <strong>el tamaño de un corpus siempre se mide en cantidad de <em>tokens</em>.</strong></p>
<section id="tokenizacion">
<h3><span class="section-number">3.1.1. </span>Tokenización<a class="headerlink" href="#tokenizacion" title="Permalink to this headline">#</a></h3>
<p>El primer paso a la hora de procesar un texto es, por tanto, hallar los <em>tokens</em> y, con ello, los <em>types</em> que forman el texto. A este proceso se le denomina <strong>tokenización</strong>.</p>
<p>El método de tokenización más simple es separar cada token por espacio en blaco. <em>Token</em> quedaría así definido como la secunencia de caracteres separada por un espacio en blanco. Desde un punto de vista lingüístico, esta aproximación presenta algunas limitaciones:</p>
<ol class="simple">
<li><p>Los signos de puntuación son un <em>type</em> diferente, per aparecen pegados a la palabra anterior o posterio. Es necesario algún tipo de regla más allá del espacio en blanco que separe los signos de puntuación.</p></li>
<li><p>Hay unidades lingüísticas que están formadas por más de un <em>type</em>. Me refiero a las llamadas “unidades multipalabra”, como por ejemplo las formas complejas de los verbos: “he comido”, “había creídos”, “fue resuelto”, etc.</p></li>
<li><p>La situación contraria se produce con las contracciones como “del” o “al” y en general formas aglutinantes (“dáselo”). En este caso se podría considerar <em>types</em> diferentes porque responde a diferentes palabras y habría que separarlas.</p></li>
</ol>
<p>Un <em>tokenizado</em> estándar resuelve el primer problema de los signos de puntuación, pero no los otros dos. Esto se deja para el lematizados, que se comentará después.</p>
<p>Por otro lado, no siempre la tokenización depende del espacio en blanco. Como se comentó antes, un <em>token</em> es la instancia de un <em>type</em>. Este puede ser cualquier secuencia de caracteres que se repitan en el texto, incluso se podría tokenizar por caracteres individuales.</p>
<p>Los sistemas neuronales explotan diversas formas de tokenización para mejorar los análisis. En capítulos siguiente se verá cómo. Los sistemas de PLN estándar suelen trabajar con la tokenización por espacio en blanco. La herramienta NLTK (<em>Natural Language Toolkit</em>) dispone de diferentes <a class="reference external" href="https://www.nltk.org/api/nltk.tokenize.html">tokenizadores</a>.<a class="footnote-reference brackets" href="#id8" id="id2">3</a></p>
</section>
<section id="lematizacion-y-stemming">
<h3><span class="section-number">3.1.2. </span>Lematización y <em>stemming</em><a class="headerlink" href="#lematizacion-y-stemming" title="Permalink to this headline">#</a></h3>
<p><em>Type</em> y <em>token</em> se refieren siempre a formas flexionadas, es decir, a formas con variaciones morfológicas. Así, “catamos” y “cantaré” son <em>types</em> distintos; al igual que “casa” y “casas”.</p>
<p>Para agrupar todos los <em>tokens</em> relacionados con la misma palabra (es decir, la forma sin flexionar o la unidad léxica que podemos encontrar, por ejemplo, en los diccionarios) se realiza un proceso de <em>lematización</em>. La lematización consiste en asignar a cada palabra lo que en lingüística se denomina su “forma no marcada”: el infinitivo para verbos, o la forma masculino singular para nombres y verbos. La forma no marcada es la que aparece en el diccionario. El lema es una manera de nombrar la palabra en toda su diversidad flexiva.</p>
<p>La lematización es un fenómenos complejo porque para saber el lema de un <em>token</em> es necesario analizar morfológicamente la palabra. Hay muchos casos de ambigüedad. Por ejemplo, el lema del <em>token</em> “traje” puede ser tanto “traer” (si es verbo) como “traje” (si es nombre), como en el siguiente texto:</p>
<blockquote>
<div><ul class="simple">
<li><p>¿Usted no nada nada?</p></li>
<li><p>Es que no traje traje.</p></li>
</ul>
</div></blockquote>
<p>Por ello en algunas aplicaciones como en recuperación de información, en vez de una lematización completa, se utiliza un proceso similar pero más rápido y sencillo denominado <em><strong>stemming</strong></em>. Este consiste en reducir cada <em>token</em> a su raíz o lexema, es decir, la parte invarible del <em>token</em> (siempre y cuando responda a una flexió morfológica regular) que, en principio, asume el significado general de la palabra. Así, por ejemplo, de las diferentes formas del verbo “amar” (amaría, amaré, amado, ame, etc.), un <em>stemmer</em> reduciría cada <em>token</em> a su raíz “am-”, mientras que un lematizador lo relacionaría con el lema “amar”.</p>
<!-- *Reflexión:* para minería de textos, ¿qué es mejor, dejar el corpus con los *tokens*, lematizarlo o trabajar solo con las raíces léxicas (*stemm*)? -->
</section>
</section>
<section id="analisis-morfologico-y-categorial">
<h2><span class="section-number">3.2. </span>Análisis morfológico y categorial.<a class="headerlink" href="#analisis-morfologico-y-categorial" title="Permalink to this headline">#</a></h2>
<p>La herramienta de PLN que realiza el análisis morfológico y categorial es el <em>Part of Speech tagger</em> (<em>pos_tagger</em> o analizador categorial).</p>
<p>El objetivo principal de una analizador categorial es asignar a cada <em>token</em> de un texto su categoría gramatical correspondiente, incluidos signos de puntuación. En concreto, los datos que analiza un <em>pos_tagger</em> estándar suelen ser, por cada <em>token</em>:</p>
<ul class="simple">
<li><p>el lema,</p></li>
<li><p>la categoría gramatical (“nombre, verbo, adjetivo, …”),</p></li>
<li><p>rasgos morfológicos (género, número, voz, tiempo, etc.).</p></li>
</ul>
<p>El mayor problema que resuelve un analizador categorial es la <em>ambigüedad categorial</em> que vimos anteriormente: aquellos <em>tokens</em> que pueden pertencer a dos o más categorías gramaticales.</p>
<section id="algunos-conceptos-linguisticos">
<h3><span class="section-number">3.2.1. </span>Algunos conceptos lingüísticos.<a class="headerlink" href="#algunos-conceptos-linguisticos" title="Permalink to this headline">#</a></h3>
<p>A modo de recordatorio, en esta sección se repasan algunos conceptos lingüísticos que se deben tener claros para trabajar con <em>PoS_taggers</em>.<a class="footnote-reference brackets" href="#id9" id="id3">4</a></p>
<p>Las palabras de un idioma se clasifican en categorías gramaticales o “clases de palabras”. Cada categoría agrupas palabras que tienen un corportamiento lingüístico similar: palabras con rasgos distributivos y morfológicos similares, y en algunos casos también rasgos semánticos parecidos.</p>
<p>Si bien no hay una lista fija de categorías gramaticales (las diferentes teorías suelen presentar pequeñas variantes), en español las categorías gramaticales suelen ser: determinantes (incluyendo aquí artículos, demostrativos, posesivos, numerales e indefinidos), sustantivos, adjetivos, pronombres, verbos, adverbios, preposiciones, conjunciones e interjecciones.</p>
<p>Estas clases se agrupan en dos grandes grupos: las categorías abiertas y cerradas. Las abiertas son aquellas en las que constantemente está apareciendo palabras nuevas (neologismos) y desapareciendo otras (arcaísmos): nombres, verbos y adjetivos sobre todo. Las clases cerradas son las clases más estables porque apenas cambian en el tiempo (preposiciones, determinantes, conjunciones, interjecciones principalmente).</p>
<p>Este diferencia es relevante desde el punto de vista computacional por dos hechos:</p>
<ol class="simple">
<li><p>Todo sistema de PLN debe estar preparado para analizar palabras nuevas. En una clase abierta el sistema de PLN se puede encontrar con palabras que no ha visto nunca antes (bien porque no está en el diccionario, bien porque no está en los corpus de aprendizaje, o bien porque es un neologismo) y debe ser capaz de analizarla. Este problema se da sobre todo con los nombres. Los sistemas neuronales actuales han mostrado ser muy eficaces para tratar este problema.</p></li>
<li><p>Las clases cerradas suelene estar formadas por pocas palabras. Esto provoca que la frecuencia de uso de las palabras de clases cerradas (preposiciones, conjunciones, pronombres, etc.) sea muy alta. Así, al extraer las frecuencias de cualquier texto encontramos pocas palabras con frecuencias muy altas (las palabras de categorías cerradas) y muchas palabras con frecuencias muy bajas (el llamado <a class="reference external" href="https://es.wikipedia.org/wiki/H%C3%A1pax"><em>hápax legómena</em></a>, que se produce por la gran cantidad de palabras de categorías abiertas que aparecen solo una vez). Esto complica los análisis de frecuencia. Para evitar esta situación, las palabras de categoras cerradas se suelen filtrar antes de extraer frecuencias: son las llamadas <em>stop words</em>.</p></li>
</ol>
<p>Por su flexión, hay categorías cuyas palabras son variables o “flexivas” y categorías de palabras invariables. Son categorías variables los nombres (con flexión de género y número), verbos (con flexión en tiempo, modo, voz, aspectos, número y persona), adjetivos (género, número y grado), pronombres y algunos adverbios y determinantes. La flexión tiene implicaciones semánticas, por lo que su análisis es más complejo. Esta es la razón de ser del análisis morfológico completo, donde de cada <em>token</em> se especifica automáticamente no solo su categoría gramatical, sino también sus rasgos flexivos.</p>
<p>Finalmente, por su función en el texto, se diferencia entre clases de palabras con significado léxico (nombres, verbos, adjetivos, adverbios) y clases de palabras con “significado” gramatical (determinantes, preposiciones, pronombres, etc.). Este significado gramatical no es significado pleno. Se refiere a que esas palabras pueden modificar o determinar el significado de las palabras con significado léxico con las que aparecen, pero en sí mismas y por sí solas no podemos decir que tengan un significado completo. Una preposición como “ante”, por ejemplo, podemos intuir rasgos semánticos (“frente a algo o delate de algo”), pero su función es completar ese “algo” con indicación de posición (“se paró <em>ante</em> de la puerta”).</p>
</section>
<section id="relevancia-del-analisis-categorial-en-mineria-de-textos">
<h3><span class="section-number">3.2.2. </span>Relevancia del análisis categorial en Minería de textos<a class="headerlink" href="#relevancia-del-analisis-categorial-en-mineria-de-textos" title="Permalink to this headline">#</a></h3>
<p>Así como un proceso de tokenización es un paso inleduble para realizar minería de textos, el análisis categorial no siempre es necesario. Éste es un proceso que requiere recursos computacionales y, si el corpus es muy amplio, también tiempo de procesamiento. Por ello se debe tener claro qué se necesita para valorar si es necesario utilizar un <em>pos tagger</em> o no.</p>
<p>Un análisis categorial es en muchas ocasiones la base del sistema de PLN, porque los análisis sintáticos y muchos de los análisis semánticos y pragmáticos dependen de las clases de palabras: necesitan saber el lema de cada <em>token</em>, la clase de palabra a la que pertenecen y/o sus rasgos morfológicos.</p>
<p>Un proceso muy común en minería de texto y de poco coste computacional es realizar un filtro de “stop words”: elimiar todas aquellas palabras que pertenecen a categorías cerradas y que no tienen significado léxico (preposiones, conjunciones, artículos, etc.). Este filtrado NO necesita realizar el análisis categorial completo: como son categorías cerradas (es, por tanto, un conjunto finito de palabras), se pueden listar en un fichero y filtrar con un simple <em>pattern matching</em>. También un proceso de <em>stemming</em> requiere poco tiempo de proceso (no es necesario realizar todo el ánalisis categorial) y permitiría tratar <em>token</em> de categorías flexivas como un solo <em>type</em>.</p>
<p>Otras aplicaciones de minería de textos sí dependen de las categorías gramaticales y por tanto requieren realizar el análisis categorial y morfológico. Entre otras:</p>
<ul class="simple">
<li><p>la <em>extracción de entidades</em> necesita saber qué palabras son nombres y en especial los nombres propios;</p></li>
<li><p>la <em>extracción de eventos</em> necesita saber qué palabras son verbos y qué palabras son nombres;</p></li>
<li><p>el <em>análisis de sentimientos y opiniones</em> depende mucho de los adjetivos;</p></li>
<li><p>la <em>detección de autoría</em> determina automáticamente quién es el autor de un texto sobre todo por cómo se utilizan las palabras de categorías cerradas (preposiciones, conjunciones, etc). Se ha demostrado que sus frecuencias de uso depende mucho del estilo personal de escritura de cada persona. La frecuencia de uso de otras clases de palabras como nombres o verbos depende más del tema del texto y no suelen ser buenos indicadores para detectar automáticamente la autoría de un texto.</p></li>
<li><p>etc.</p></li>
</ul>
</section>
<section id="representacion-formal-de-la-informacion-morfologica-y-categorial">
<h3><span class="section-number">3.2.3. </span>Representación formal de la información morfológica y categorial<a class="headerlink" href="#representacion-formal-de-la-informacion-morfologica-y-categorial" title="Permalink to this headline">#</a></h3>
<p>Antes de exponer los métodos de análisis categorial, vamos a ver cómo se representa formalmente esta información lingüística.</p>
<p>La información categorial y morfológica se representa explícitamente mediante etiquetas o tags. Actualmente hay diversas propuestas, cada una con un juego de etiquetas diferente. Antes de usar un <em>PoS_tagger</em>, es muy importante saber con qué juego de etiquetas representa la información para poder luego interpretar la información correctamente. Las listas de etiquetas (o <em>tag sets</em>) comunes hoy día en PLN son los siguientes:</p>
<ul>
<li><p><em>Penn Treebank tag set</em> (solo para inglés):</p>
<ul>
<li><p><a class="reference external" href="https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html">https://www.cs.upc.edu/~nlp/SVMTool/PennTreebank.html</a></p></li>
<li><p>Ejemplos:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>JJ = adjetivo
NN = nombre común
VB = verbo
...
</pre></div>
</div>
</li>
</ul>
</li>
<li><p><em>Universal tagset</em> (<em>Universal dependencies project</em>):</p>
<ul>
<li><p><a class="reference external" href="https://universaldependencies.org/u/pos/">https://universaldependencies.org/u/pos/</a></p></li>
<li><p>Ejemplos:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>ADJ   = adjetivo
NOUN  = nombre
VERB  = verbo
...
</pre></div>
</div>
</li>
<li><p>Este modelo, el más utilizado hoy día, está adaptado a más de 50 idiomas (y sigue creciendo). Es el más apropiado para minería de textos multilingüe. Ver <a class="reference external" href="https://universaldependencies.org/">https://universaldependencies.org/</a></p></li>
</ul>
</li>
<li><p><em>EAGLES tag set</em> (para varios idiomas):</p>
<ul>
<li><p><a class="reference external" href="http://blade10.cs.upc.edu/freeling-old/doc/tagsets/tagset-es.html">http://blade10.cs.upc.edu/freeling-old/doc/tagsets/tagset-es.html</a></p></li>
<li><p><a class="reference external" href="https://freeling-user-manual.readthedocs.io/en/latest/tagsets/">https://freeling-user-manual.readthedocs.io/en/latest/tagsets/</a></p></li>
<li><p><a class="reference external" href="http://www.ilc.cnr.it/EAGLES96/annotate/annotate.html">http://www.ilc.cnr.it/EAGLES96/annotate/annotate.html</a></p></li>
<li><p>Ejemplo:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>AQ0CP0  = adjetivo calificativo común plural
NCFP000 = nombre común femenino plural
VMIP1S0 = verbo principal indicativo presente primera persona singular
...
</pre></div>
</div>
</li>
<li><p>Este tipo de etiqueta es más complejo pero contiene bastante información. Cada posición es un rasgo morfológico. El primero (A, N, V, etc.) indica la categoría gramatical. El resto de posiciones, dependiendo de la categoría, aporta una información morfológica u otra. Así, para nombre, las posicones indican:</p>
<ul class="simple">
<li><p>1 categoría</p></li>
<li><p>2: tipo: común o propio</p></li>
<li><p>3: género</p></li>
<li><p>4 número</p></li>
<li><p>5-6 rasgos semántico</p></li>
<li><p>7 grado</p></li>
</ul>
</li>
<li><p>Ver las tablas para saber qué información contiene cada posición del resto de categorías.</p></li>
<li><p>Este modelo se planteó como un modelo multilingüe. Así, no todas las palabras de todos los idiomas tienen esa información morfológica. Si una palabra no tiene un rasgo morfológico determinado, en la etiqueta aparece <span class="math notranslate nohighlight">\(0\)</span>.</p></li>
<li><p>Por ejemplo, en la etiqueta “NCFP000” no es relevante ni el rasgo semántico (codificado como 00) ni el grado (codificado como 0). Sí es relevante la categoría (N: nombre), el tipo (C: común), el género (F: femenino) y el número (P: plural). Esta etiqueta se asociaría por ejemplo a la palabra “camisas”.</p></li>
</ul>
</li>
</ul>
<p>Como se puede observar, los <em>tag sets</em> utilizados en PLN suelen tener más categorías que las utilizadas en lingüística teórica. Por ejemplo, presentan etiquetas específicas para los signos de puntuación o para números y fechas, entre otros casos.</p>
</section>
<section id="arquitectura-de-un-pos-tagger">
<h3><span class="section-number">3.2.4. </span>Arquitectura de un <em>PoS_tagger</em>.<a class="headerlink" href="#arquitectura-de-un-pos-tagger" title="Permalink to this headline">#</a></h3>
<p>La siguiente imagen muestra una sencilla arquitectura para un <em>pos_tagger</em>:</p>
<p><img alt="ArquitecturaPoStagger" src="_images/arquitecturaPoStagger.png" /></p>
<p>La entrada es un texto que ha sido previamente tokenizado. Los signos de puntuación, por ejemplo, estarán separados de las palabras anterior o posterior (según proceda). La salida será cada <em>token</em> de entrada junto a su información categorial y morfológica. Normalmente, la salida de un <em>pos tagger</em> es:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>token lema etiqueta_PoS
</pre></div>
</div>
<p>Para poder determinar el lema y la categoría de cada <em>token</em>, así como la información morfológica, el <em>pos_tagger</em> necesita algún tipo de recurso. Básicamente dos: un diccionario que contenga la información morfológica de cada palabra; y un conjunto de reglas (gramática) que deriven la información morfológica según los rasgos de la palabra y de las palabas del contexto. Este puede haber sido creado a mano (reglas manuales) o  mediante aprendizaje automático. Esto es una analizador en dos fases (consulta diccionario y desambiguación), que es la arquitectura básica de un <em>PoS_tagger</em>.</p>
<p>Un ejemplo de <em>PoS_tagger</em> para español es <em>Freeling</em>. Antes de seguir, prueba su demo:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://nlp.lsi.upc.edu/freeling/demo/demo.php">https://nlp.lsi.upc.edu/freeling/demo/demo.php</a></p></li>
</ul>
</section>
<section id="algoritmos-de-desambiguacion-categorial">
<h3><span class="section-number">3.2.5. </span>Algoritmos de desambiguación categorial.<a class="headerlink" href="#algoritmos-de-desambiguacion-categorial" title="Permalink to this headline">#</a></h3>
<p>Si un <em>token</em> solo puede pertenecer a una categoría gramatical, su análisis morfológico es sencillo: basta con consultar el diccionario para saber su categoría. El problema viene cuando un <em>token</em> puede pertenecer a dos o más categorías gramaticales (ambigüedad categorial). Este es el caso más común, pues más del 60% de las palabras de un texto en español suelen presentar ambigüedad categorial. En esta sección se presentan los principales algoritmos para resolver la ambigüedad categorial.</p>
<section id="modelo-basado-en-reglas-simbolicos">
<h4><span class="section-number">3.2.5.1. </span>Modelo basado en reglas (simbólicos).<a class="headerlink" href="#modelo-basado-en-reglas-simbolicos" title="Permalink to this headline">#</a></h4>
<p>Los primeros sistemas utilizaban reglas morfológica simples creadas a mano por lingüistas. Se seguía el modelo de dos fases: una primera que asigna las categorías gramaticales a cada <em>token</em> según el diccionario, y una segunda que aplica reglas de desambiguación en el caso de que el <em>token</em> tenga asignadas dos o más etiquetas.</p>
<p>Las reglas de desambiguación era básicamente expresiones regulares tipo:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>\b.*ing\b = Verbo Infinitivo en inglés.
\b.*mente\b = adverbio en español.
las\s[a-z]*as → Nombre femenino plural en español.
etc.
</pre></div>
</div>
<p>Ejemplo de esta aproximación es el sistema TAGGIT, de 1971. Constaba de 71 etiquetas y 3300 reglas de desambiguación. Con esto alcanzó un nivel de precisión del 77%.<a class="footnote-reference brackets" href="#id10" id="id4">5</a></p>
<p>La herramienta NLTK tiene implementado un <em>PoS_tagger</em> basado en expresiones regulares que se puede adaptar. Ver <a class="reference external" href="https://www.nltk.org/book/ch05.html">https://www.nltk.org/book/ch05.html</a>.</p>
<section id="gramaticas-de-restricciones-constraint-grammar">
<h5><span class="section-number">3.2.5.1.1. </span>Gramáticas de restricciones (<em>Constraint grammar</em>)<a class="headerlink" href="#gramaticas-de-restricciones-constraint-grammar" title="Permalink to this headline">#</a></h5>
<p>En los años 90 hubo un modelo teórico que tuvo buenos resultados en su aplicación al análisis categorial: las gramáticas de restricciones. Este modelo se basa en la idea de que las reglas de la gramática no tienen por qué ser positivas (reglas que digan cómo es el idioma), sino que pueden ser negativas (reglas que digan como NO es el idioma). Estas reglas negativas son las <strong>restricciones</strong>.</p>
<p>Así, apliado al análisis categorial, una restricción indicaría no qué categoría sería la apropiada para una palabra dado un contexto, sino que categoría seguro que NO es la apropiada a una palabra dado un contexto. Por ejemplo, esta regla</p>
<blockquote>
<div><p>“Un verbo no va precedido nunca de artículo”.</p>
</div></blockquote>
<p>permitiría analizar correctamente el sintagma:</p>
<blockquote>
<div><p>El cura de la iglesia</p>
</div></blockquote>
<p>“Cura” no puede ser verbo (de “curar”) porque va precedido de un artículo. Por tanto, es un nombre.</p>
<p>El sistema principal basado en restricciones es el sistema ENGCG Karlsson et al 1995.<a class="footnote-reference brackets" href="#id11" id="id5">6</a></p>
</section>
</section>
<section id="modelos-estadisticos">
<h4><span class="section-number">3.2.5.2. </span>Modelos estadísticos<a class="headerlink" href="#modelos-estadisticos" title="Permalink to this headline">#</a></h4>
<section id="cadena-de-markov">
<h5><span class="section-number">3.2.5.2.1. </span>Cadena de Markov<a class="headerlink" href="#cadena-de-markov" title="Permalink to this headline">#</a></h5>
<p>La aplicación de modelos de Markov supuso un gran avance en la desambiguación categorial. La categoría gramatical de un <em>token</em> depende en gran medida del contexto lingüístico donde aparece. Las reglas directas no son capaces de modelar ese contexto, pero los modelos de Markov sí.</p>
<p>Dada una secuencia de estados, la propieda de Markov asume que es posible predecir el siguiente estado tendiendo en cuenta únicamente el estado presente. Así, dada una secuencia de palabras (cadena), la propiedad de Markov postula que podemos saber la siguiente palabra a partir de la palabra actual. Así, un modelo de Markov predice un token <span class="math notranslate nohighlight">\(w_i\)</span>  según la probabilidad de la palabra anterior <span class="math notranslate nohighlight">\(w_{i-1}\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(w_i|w_{i-1})\]</div>
<p>Este es un modelo de bigramas porque solo tiene en cuenta la palabra anterior y no todas las palabras anteriores <span class="math notranslate nohighlight">\(w_{i-2} \dots w_{i-n}\)</span>, que es la asunción principal de la propiedad de Markov.</p>
</section>
<section id="modelo-oculto-de-markov">
<h5><span class="section-number">3.2.5.2.2. </span>Modelo oculto de Markov<a class="headerlink" href="#modelo-oculto-de-markov" title="Permalink to this headline">#</a></h5>
<p>Aplicado a cadena de <em>tokens</em> tendríamos un simple predictor de palabras como el que tenemos en el móvil. Lo característico de su aplicación para análisis categorial es que se aplica no a la secuencia de palabras (la cadena visible de <em>tokens</em>), sino a la secuencia de categorías gramaticales: la cadena <strong>oculta</strong> de <em>tags</em>. Se condiera una cadena oculta porque las categorías gramaticales no están explícitamente en el texto, sino que son inferidas. De ahí el nombre de <strong>Modelo Oculto de Markov</strong> o <em><strong>Hidden Markov Model</strong></em> (HMM).</p>
<p>El modelo oculto de Markov necesita, así, en dos probabilidades: una probabilidad de transición de un estado a otro de la cadena (la probabilidad del modelo de Markov simple) y además una probabilidad de emisión del estado oculto al estado visible.</p>
<p>Aplicado al análisis categorial, la probabilidad de transición es la probabilidad de una etiqueta categorial <span class="math notranslate nohighlight">\(t_i\)</span> dada la etiqueta categorial de la palabra anterior <span class="math notranslate nohighlight">\(t_{i-1}\)</span>.</p>
<div class="math notranslate nohighlight">
\[P(t_i|t_{i-1})\]</div>
<p>La probabildad de emisión es la probabilidad de que una palabra dada <span class="math notranslate nohighlight">\(w_i\)</span> esté asociada a una etiqueta categorial <span class="math notranslate nohighlight">\(t_i\)</span>:</p>
<div class="math notranslate nohighlight">
\[P(w_i|t_i)\]</div>
<p>Combinando ambos valores obtenemos la predicción final:</p>
<div class="math notranslate nohighlight">
\[P(t_i) = P(w_i|t_i) * P(t_i|t_{i-1})\]</div>
<p>Por ejemplo, dado el siguiente sintagma ya comentado:</p>
<blockquote>
<div><p>El cura de la iglesia.</p>
</div></blockquote>
<p>Un modelo oculto de Markov predice perfectamente que ese “cura” es nombre y no es verbo (de “curar”) porque la probabilidad de que un artículo (la categoría del <em>token</em> “El”) esté seguido por un verbo es prácticamente 0. Por lo que la probabilidad más alta es que “cura” sea nombre.</p>
<p>Un modelo oculto de Markov puede ser entrenado a partir de un corpus anotado, pero también se puede entrenar de manera iterativa con corpus sin anotar, tomando las palabras no ambiguas como inicio del entrenamiento.</p>
<div class="note admonition">
<p class="admonition-title">Lectura obligatoria</p>
<p>Lee con atención el apartado <a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">“8.4 HMM Part-of-Speech Tagging”</a> del capítulo 8 del libro de Juravsky y Martin (2022) <em>Speech and Language Processing</em>, donde aprenderás los detalles matemáticos y  computacionales del análisis categorial basado en modelos ocultos de Markov.</p>
</div>
</section>
<section id="conditional-random-fields-y-otros">
<h5><span class="section-number">3.2.5.2.3. </span><em>Conditional Random Fields</em> y otros.<a class="headerlink" href="#conditional-random-fields-y-otros" title="Permalink to this headline">#</a></h5>
<p>Si bien con modelos ocultos de Markov un <em>PoS_tagger</em> puede tener una precisión superior al 90%, aún hay aspectos en los que no funciona bien, como por ejemplo cómo analizar palabras que no ha visto antes (y por tanto no tiene probabilidad de emisión). Estos problemas se superaron con un modelo matemático también secuencial pero con más relevancia en cuanto al modelado del contexto: <em><strong>conditional random field</strong></em> (CRF o <a class="reference external" href="https://es.wikipedia.org/wiki/Campo_aleatorio_condicional">“campo aleatorio condicional”</a>). Este permite un tratamiento más rico del contexto. Lee el apartado [“8.5 Conditional Random Fields (CRFs)”] (<a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/8.pdf">https://web.stanford.edu/~jurafsky/slp3/8.pdf</a>) del capítulo 8 del libro de Juravsky y Martin (2022) <em>Speech and Language Processing</em> para más detalles.</p>
<p>Modelos ocultos de Markov y <em>Conditional Random Fields</em> son las dos principales aproximaciones al análisis categorial, pero no las únicas. Se han aplicado otro modelos de aprendizaje, tanto supervisados como no supervisados, como árboles de decisión o máquinas de vectores soporte (<em>support vector machine</em> SVM), etc. Un ejemplo que tuvo impacto en su momento fue el “Transformation-based Tagger” de Brill (1995), que aplicaba un modelo iterativo de aprendizaje de reglas con refinado manual.</p>
</section>
</section>
</section>
<section id="situacion-actual">
<h3><span class="section-number">3.2.6. </span>Situación actual<a class="headerlink" href="#situacion-actual" title="Permalink to this headline">#</a></h3>
<p>El análisis categorial de las lenguas más habladas se considera una tarea prácticamente resuelta. Las dos líneas de investigación principales son:</p>
<ul class="simple">
<li><p>desarrollar analizadores multilingües (en la línea, por ejemplo, de las Dependencias Universales que se verá luego); y</p></li>
<li><p>aplicar redes neuronales, que es hoy día el modelo estándar. En próximos temas se verán las redes neuronales.</p></li>
</ul>
<p>La siguiente página recoge los últimos artículos sobre <em>Part of Speech tagging</em>:</p>
<p><a class="reference external" href="http://nlpprogress.com/english/part-of-speech_tagging.html">http://nlpprogress.com/english/part-of-speech_tagging.html</a></p>
</section>
<section id="recursos">
<h3><span class="section-number">3.2.7. </span>Recursos.<a class="headerlink" href="#recursos" title="Permalink to this headline">#</a></h3>
<p>Cualquier sistema de PLN parte de un PoS tagger. Es el análisis básico. Hay muchos disponibles por la web:</p>
<ul class="simple">
<li><p>Freeling <a class="reference external" href="http://nlp.lsi.upc.edu/freeling/index.php/">http://nlp.lsi.upc.edu/freeling/index.php/</a></p></li>
<li><p>SpaCy: <a class="reference external" href="https://spacy.io/">https://spacy.io/</a></p></li>
<li><p>NLTK: <a class="reference external" href="http://www.nltk.org/">http://www.nltk.org/</a></p></li>
<li><p>Standford CORE NLP: <a class="reference external" href="https://stanfordnlp.github.io/CoreNLP/">https://stanfordnlp.github.io/CoreNLP/</a></p></li>
<li><p>Google CLOUD: <a class="reference external" href="https://cloud.google.com/natural-language/">https://cloud.google.com/natural-language/</a></p></li>
<li><p>En CLARIN hay también varios PoS_taggers: <a class="reference external" href="https://www.clarin.eu/resource-families/tools-part-speech-tagging-and-lemmatisation">https://www.clarin.eu/resource-families/tools-part-speech-tagging-and-lemmatisation</a></p></li>
<li><p>En OpenNLP (Java): <a class="reference external" href="https://opennlp.apache.org/docs/">https://opennlp.apache.org/docs/</a></p></li>
</ul>
<p>y muchos más</p>
</section>
</section>
<section id="analisis-sintactico">
<h2><span class="section-number">3.3. </span>Análisis sintáctico.<a class="headerlink" href="#analisis-sintactico" title="Permalink to this headline">#</a></h2>
<p>Sintaxis: agrupación y relaciones de las palabras dentro de una oración.</p>
<p>Análisis automático: <em>parser</em></p>
<section id="arquitectura-estandar-de-un-parser">
<h3><span class="section-number">3.3.1. </span>Arquitectura estándar de un <em>parser</em><a class="headerlink" href="#arquitectura-estandar-de-un-parser" title="Permalink to this headline">#</a></h3>
<p><img alt="ArquitecturaParser" src="_images/parser.png" /></p>
</section>
<section id="modelos-de-representacion">
<h3><span class="section-number">3.3.2. </span>Modelos de representación<a class="headerlink" href="#modelos-de-representacion" title="Permalink to this headline">#</a></h3>
<p>Análisis basado en <em>constituyentes</em></p>
<p><img alt="Constituyentes" src="_images/constituyentes.png" /></p>
<p>Análisis basado en <em>dependencias</em></p>
<p><img alt="Dependecias_Freeling" src="_images/dependency_parsing_FreeLing.jpg" /></p>
<p>(Créditos de la imagen <a class="reference external" href="http://liceu.uab.cat/~joaquim/language_technology/NLP/PLN_analisis.html#An%C3%A1lisis_de_dependencias">aquí</a>)</p>
</section>
<section id="principal-problema-computacional">
<h3><span class="section-number">3.3.3. </span>Principal problema computacional<a class="headerlink" href="#principal-problema-computacional" title="Permalink to this headline">#</a></h3>
<p>Ambigüedad estructural:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&quot;Ayer vi a tu hermano con los prismáticos&quot;
</pre></div>
</div>
<p>Ambigüedad coordinación:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>&quot;Sirve los platos y los cubiertos limpios&quot;
</pre></div>
</div>
</section>
<section id="gramaticas-formales">
<h3><span class="section-number">3.3.4. </span>Gramáticas formales<a class="headerlink" href="#gramaticas-formales" title="Permalink to this headline">#</a></h3>
<p>Conjunto de reglas formales de análisis sintático.</p>
<section id="context-free-grammars">
<h4><span class="section-number">3.3.4.1. </span>Context free grammars<a class="headerlink" href="#context-free-grammars" title="Permalink to this headline">#</a></h4>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>G = (NT, T, S, P)
NT: {no terminales},
T: {terminales},
S: Símbolo inicial
P: Reglas de producción A -&gt; w: 
    A   NT
    W   (NT U T)*
</pre></div>
</div>
<p>Tal que</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>NT ={S,NP,VP,nprop,n,v,det}, 
T ={Pepe,manzana, come,una},
P:
    S -&gt; NP VP
    NP -&gt; nprop
    NP -&gt; det  n
    VP -&gt; v
    VP -&gt; v NP
</pre></div>
</div>
<p><img alt="AnalisisConstituyentes" src="_images/constituyentes_2.png" /></p>
<p>Estas gramáticas eran muy limitadas y fueron ampliadas con estructuras de rasgos y técnicas de unificación.</p>
<p><img alt="Unificación" src="_images/unificacion.png" /></p>
<p>En lingüística se han desarrollado diferentes modelos basados en estas técnicas las <em>Head-driven phrase structure grammar</em> o las <em>Lexical-Functional Grammar</em> (que sigue siendo un modelo válido: <a class="reference external" href="https://ling.sprachwiss.uni-konstanz.de/pages/home/lfg/">https://ling.sprachwiss.uni-konstanz.de/pages/home/lfg/</a> )</p>
</section>
<section id="probabilistic-context-free-grammar-y-modelos-probabilisticos">
<h4><span class="section-number">3.3.4.2. </span><em>Probabilistic Context Free Grammar</em> y modelos probabilísticos<a class="headerlink" href="#probabilistic-context-free-grammar-y-modelos-probabilisticos" title="Permalink to this headline">#</a></h4>
<p>Añaden peso estadístico a cada regla.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>SV → V SP SP (0.5)
SV → V SP (0.3)
SV → V (0.2)
</pre></div>
</div>
<p>Modelos de aprendizaje automático.</p>
<p>Corpus de aprendizaje y evaluación: <em>treebanks</em></p>
<ul class="simple">
<li><p>Penn Treebank:</p>
<ul>
<li><p><a class="reference external" href="https://catalog.ldc.upenn.edu/LDC99T42">https://catalog.ldc.upenn.edu/LDC99T42</a></p></li>
<li><p><a class="reference external" href="https://www.kaggle.com/nltkdata/penn-tree-bank">https://www.kaggle.com/nltkdata/penn-tree-bank</a></p></li>
</ul>
</li>
<li><p>Ancora (español, catalán):</p>
<ul>
<li><p><a class="reference external" href="http://clic.ub.edu/corpus/en/ancora-descarregues">http://clic.ub.edu/corpus/en/ancora-descarregues</a></p></li>
</ul>
</li>
</ul>
<p>Y muchos otros</p>
</section>
</section>
<section id="chunkers">
<h3><span class="section-number">3.3.5. </span><em>Chunkers</em><a class="headerlink" href="#chunkers" title="Permalink to this headline">#</a></h3>
<p>En ocasiones el análisis sintáctico completo (<em>full parsing</em>) es complejo, consume mucho recurso y no suele obtener buenos resultados.</p>
<p>Lo normal es realizar <em>análisis sintáctico parcial</em> o <em>chunkers</em>: extraer agrupaciones sintáticas (<em>chunks</em>) sin llegar a derivar el árbol sintáctico completo (Abney 1991).</p>
</section>
<section id="estrategias">
<h3><span class="section-number">3.3.6. </span>Estrategias<a class="headerlink" href="#estrategias" title="Permalink to this headline">#</a></h3>
<p>Descendente:
Recursive Descendent:</p>
<p>(El siguiente código es Python y requiere tener instalado <a class="reference external" href="https://www.nltk.org/">NLTK</a>)</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import nltk
nltk.app.rdparser()
</pre></div>
</div>
<p>Ascendente
Shift Reduce:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>import nltk
nltk.app.srparser()
</pre></div>
</div>
</section>
<section id="formato-conll">
<h3><span class="section-number">3.3.7. </span>Formato CONLL<a class="headerlink" href="#formato-conll" title="Permalink to this headline">#</a></h3>
<p>Formato de salida estándar en análisis de dependencias. Además de la información morfológica, por cada palabra indica de quién depende y el tipo de dependencia.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>    Salida CoNLL-U
    # sent_id = 1
    # text = Los hombres que fuman puro tienen cara de canguro .
    1   Los el  DET DET Definite=Def|Gender=Masc|Number=Plur|PronType=Art   2   det _   _
    2   hombres hombre  NOUN    NOUN    Gender=Masc|Number=Plur 6   nsubj   _   _
    3   que que PRON    PRON    PronType=Int,Rel    4   nsubj   _   _
    4   fuman   fumar   VERB    VERB    Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin   2   acl _   _
    5   puro    puro    ADJ ADJ Gender=Masc|Number=Sing 4   obj _   _
    6   tienen  tener   VERB    VERB    Mood=Ind|Number=Plur|Person=3|Tense=Pres|VerbForm=Fin   0   root    _   _
    7   cara    cara    NOUN    NOUN    Gender=Fem|Number=Sing  6   obj _   _
    8   de  de  ADP ADP _   9   case    _   _
    9   canguro canguro NOUN    NOUN    Gender=Masc|Number=Sing 7   nmod    _   _
    10  .   .   PUNCT   PUNCT   PunctType=Peri  6   punct   _   _
</pre></div>
</div>
</section>
<section id="id6">
<h3><span class="section-number">3.3.8. </span>Situación actual<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h3>
<p><em>Transition-based dependency parsing</em> (Nivre 2014). Algoritmo shift-reduce.</p>
<p><em>Neural Network Dependency Parser</em>: <a class="reference external" href="https://nlp.stanford.edu/software/nndep.shtml">https://nlp.stanford.edu/software/nndep.shtml</a></p>
<p>Modelo de dependencias universal: <a class="reference external" href="https://universaldependencies.org/"><em>Universal Dependencies</em></a>:</p>
<blockquote>
<div><p>The Universal Dependencies project (Nivre et al., 2016) provides an inventory of dependency relations that arelinguistically motivated, computationally useful, and cross-linguistically applicable. (Juravsky y Martin 2020, cap. 14)</p>
</div></blockquote>
<p>Representación vectorial (<em>embeddings</em>).</p>
<p><a class="reference external" href="http://nlpprogress.com/english/constituency_parsing.html">http://nlpprogress.com/english/constituency_parsing.html</a></p>
<p><a class="reference external" href="http://nlpprogress.com/english/constituency_parsing.html">http://nlpprogress.com/english/dependency_parsing.html</a></p>
</section>
<section id="herramientas">
<h3><span class="section-number">3.3.9. </span>Herramientas<a class="headerlink" href="#herramientas" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>SpaCy: <a class="reference external" href="https://spacy.io/">https://spacy.io/</a></p></li>
<li><p>STANZA: <a class="reference external" href="https://stanfordnlp.github.io/stanza/">https://stanfordnlp.github.io/stanza/</a></p></li>
<li><p>Freeling: <a class="reference external" href="https://nlp.lsi.upc.edu/freeling/node/1">https://nlp.lsi.upc.edu/freeling/node/1</a></p></li>
<li><p>UD-Pipe: <a class="reference external" href="https://ufal.mff.cuni.cz/udpipe">https://ufal.mff.cuni.cz/udpipe</a></p></li>
</ul>
</section>
</section>
<section id="bibliografia">
<h2><span class="section-number">3.4. </span>Bibliografía<a class="headerlink" href="#bibliografia" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Abney S.P. (1991) “Parsing By Chunks”. In: Berwick R.C., Abney S.P., Tenny C. (eds) Principle-Based Parsing. Studies in Linguistics and Philosophy, vol 44. Springer, Dordrecht. <a class="reference external" href="https://doi.org/10.1007/978-94-011-3474-3_10">https://doi.org/10.1007/978-94-011-3474-3_10</a></p></li>
<li><p>Emily M. Bender (2013) <em>Linguistic Fundamentals for Natural Language Processing. 100 Essentials from Morphology and Syntax</em>, Synthesis Lectures on Human Language Technologies DOI: <a class="reference external" href="https://doi.org/10.1007/978-3-031-02150-3">https://doi.org/10.1007/978-3-031-02150-3</a></p></li>
<li><p>Steven Bird, Ewan Klein, and Edward Loper (2009) <em>Natural Language Processing with Python</em> <a class="reference external" href="https://www.nltk.org/book/">https://www.nltk.org/book/</a></p></li>
<li><p>Juravsky y Martin (2020) <em>Speech and Language Processing</em>. <a class="reference external" href="https://web.stanford.edu/~jurafsky/slp3/">https://web.stanford.edu/~jurafsky/slp3/</a></p></li>
<li><p>Karlsson, F., A. Voutilainen, J. Heikkilä, and A. Anttila (eds.). 1995. <em>Constraint Grammar. A language-independent system for parsing unrestricted text</em>. Berlin and New-York: Mouton de Gruyter</p></li>
</ul>
<hr class="docutils" />
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id7"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>“Token” se asimila en este caso a “occurrence”. Cfr. <a class="reference external" href="https://plato.stanford.edu/entries/types-tokens/#Occ">https://plato.stanford.edu/entries/types-tokens/#Occ</a></p>
</dd>
<dt class="label" id="id8"><span class="brackets"><a class="fn-backref" href="#id2">3</a></span></dt>
<dd><p>Ver <a class="reference external" href="https://www.nltk.org/book/ch03.html">capítulo 3</a> del libro <a class="reference external" href="https://www.nltk.org/book/ch03.html"><em>Natural Language Processing with Python</em></a> para una explicación sencialla.</p>
</dd>
<dt class="label" id="id9"><span class="brackets"><a class="fn-backref" href="#id3">4</a></span></dt>
<dd><p>Bender (2013) presenta una buena introducción a conceptos lingüísticos de uso común en Procesamiento del Lenguaje Natural.</p>
</dd>
<dt class="label" id="id10"><span class="brackets"><a class="fn-backref" href="#id4">5</a></span></dt>
<dd><p>Greene, B. B. and G. M. R.ubin (1971). “Automatic grammatical tagging of English. Technical report”, Department of Linguistics, Brown University,
Providence, Rhode Island.</p>
</dd>
<dt class="label" id="id11"><span class="brackets"><a class="fn-backref" href="#id5">6</a></span></dt>
<dd><p>Karlsson, Voutilainen, Heikkilä and Antilla (eds) 1995. <em>Constraint Grammar: A Language-Independent System for Parsing Unrestricted Text.</em> Mouton de Gruyter, Berlin and New York.</p>
</dd>
</dl>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="bloque1_1Introduccion.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">2. </span>Minería de textos y procesamiento del lenguaje natural.</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="bloque1_3AnalisisSemantico.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">4. </span>Análisis semántico</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Universitat d'Alacant<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>