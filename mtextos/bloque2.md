
(label_tecnicas)=
T√©cnicas para la miner√≠a de textos
==================================

En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio P√©rez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor log√≠stico, que nos servir√° para asentar los conocimientos necesarios para entender posteriores modelos. A continuaci√≥n se estudia con cierto nivel de detalle *skip-grams*, uno de los algoritmos para la obtenci√≥n de *embeddings* incontextuales de palabras. Despu√©s se repasa el funcionamiento de las arquitecturas neuronales *feedforward* y se estudia su aplicaci√≥n a modelos de lengua. El objetivo √∫ltimo es abordar el estudio de la arquitectura m√°s importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un an√°lisis del funcionamiento de los modelos preentrenados.

Los materiales de clase complementan la lectura de algunos cap√≠tulos de un libro de texto ("Speech and Language Processing" de Dan Jurafsky y James H. Martin, borrador de la tercera edici√≥n, disponible online) con anotaciones realizadas por el profesor.

## Primera sesi√≥n (29 de marzo de 2023)

#### Contenidos a preparar antes de la sesi√≥n del 29/03/2023

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de los contenidos de [esta p√°gina](https://jaspock.github.io/me/materials/transformers/regresor) sobre regresi√≥n log√≠stica. Puedes saltar por ahora el apartado de [implementaci√≥n en PyTorch](https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch), ya que ser√° el eje central de la clase presencial. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo prop√≥sito es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo del libro. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.
- Visionado y estudio de los tutoriales en v√≠deo de esta [playlist oficial de PyTorch](https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN).  Estudia al menos los 4 primeros v√≠deos (‚ÄúIntroduction to PyTorch‚Äù, ‚ÄúIntroduction to PyTorch Tensors‚Äù, ‚ÄúThe Fundamentals of Autograd‚Äù y ‚ÄúBuilding Models with PyTorch‚Äù). En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.
- Tras acabar con las dos partes anteriores, realiza este [test de evaluaci√≥n](https://forms.gle/E1xzZHw6hzMWJaNr7) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.

#### Contenidos para la sesi√≥n presencial del 29/03/2023

En la clase presencial, repasaremos los contenidos de la semana anterior y veremos c√≥mo se implementa el regresor log√≠stico en PyTorch siguiendo la implementaci√≥n de un regresor log√≠stico binario y de uno multinomial que se comentan en [este apartado]((https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch)).

La idea es que vayas creando una serie de notebooks en Google Colab en los que incluyas y comentes cada uno de los programas que vamos a ir viendo. En la √∫ltima clase se presentar√° una pr√°ctica m√°s avanzada que implicar√° modificar el c√≥digo del transformer.

## Segunda sesi√≥n (26 de abril de 2023)

Entre la sesi√≥n anterior y la del 26 de abril transcurren varias semanas de trabajo, por lo que la carga de trabajo es mayor que en la sesi√≥n anterior.

Nota: los contenidos de este apartado son provisionales.

#### Contenidos a preparar antes de la sesi√≥n del 26/04/2023

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de [esta p√°gina](https://jaspock.github.io/me/materials/transformers/embeddings) sobre la obtenci√≥n de embeddings incontextuales. Puedes saltar de nuevo el apartado de [implementaci√≥n en PyTorch](https://jaspock.github.io/me/materials/transformers/embeddings#implementaci√≥n-en-pytorch), ya que se estudiar√° en la pr√≥xima clase presencial. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo objetivo es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realiza una segunda lectura del cap√≠tulo. En total, esta parte deber√≠a llevarte unas 4 horas üïíÔ∏è de trabajo.
- Lectura y estudio de [esta p√°gina](https://jaspock.github.io/me/materials/transformers/ffw) sobre la obtenci√≥n de embeddings incontextuales. Puedes saltar de nuevo el apartado de [implementaci√≥n en PyTorch](https://jaspock.github.io/me/materials/transformers/ffw#implementaci√≥n-en-pytorch), ya que se estudiar√° tambi√©n en la pr√≥xima clase presencial. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.
- Primeros pasos en el estudio del modelo transformer. Volveremos a dedicar m√°s horas a esta arquitectura para la pr√≥xima sesi√≥n para abordarla en dos fases. Ahora, lee con detenimiento la introducci√≥n a mecanismos de atenci√≥n de ["Visualizing A Neural Machine Translation Model"](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/), as√≠ como la introducci√≥n visual a los transformers de ["The Illustrated Transformer"](http://jalammar.github.io/illustrated-transformer/). A continuaci√≥n, realiza una lectura no muy pausada del apartado 9.7 (solo este apartado) del cap√≠tulo ["Deep learning architectures for sequence processing"](https://web.archive.org/web/20221216193204/https://web.stanford.edu/~jurafsky/slp3/9.pdf). En total, esta parte deber√≠a llevarte unas 4 horas üïíÔ∏è de trabajo. Volveremos a este cap√≠tulo para la pr√≥xima sesi√≥n. 
- Realizaci√≥n del [test de evaluaci√≥n]() de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.

#### Contenidos para la sesi√≥n presencial del 26/04/2023


## Tercera sesi√≥n (10 de mayo de 2023)

#### Contenidos a preparar antes de la sesi√≥n del 10 de mayo
