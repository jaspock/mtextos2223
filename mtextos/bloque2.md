
(label_tecnicas)=
T√©cnicas para la miner√≠a de textos
==================================

En este bloque se aborda el estudio de algunos modelos neuronales utilizados para procesar textos. El profesor de este bloque es Juan Antonio P√©rez Ortiz. El bloque comienza con un repaso del funcionamiento del regresor log√≠stico, que nos servir√° para asentar los conocimientos necesarios para entender posteriores modelos. A continuaci√≥n se estudia con cierto nivel de detalle uno de los algoritmos para la obtenci√≥n de *embeddings* incontextuales de palabras. Despu√©s se describe el funcionamiento de las arquitecturas neuronales *feedforward* y su aplicaci√≥n a modelos de lengua. El objetivo √∫ltimo es abordar el estudio de la arquitectura m√°s importante de los sistemas actuales de procesamiento de textos: el transformer. Una vez estudiadas estas arquitecturas, finalizaremos con un an√°lisis de los modelos preentrenados.

Los materiales de clase complementan la lectura de algunos cap√≠tulos de un libro de texto con anotaciones realizadas por el profesor.

## Contenidos a preparar antes de la sesi√≥n del 29 de marzo de 2023

Las actividades a realizar antes de esta clase son:

- Lectura y estudio de [esta p√°gina](https://jaspock.github.io/me/materials/transformers/regresor) sobre regresi√≥n log√≠stica. Puedes saltar el apartado de [implementaci√≥n en PyTorch](https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch), ya que ser√° el eje central de la pr√≥xima clase presencial. Como ver√°s, la p√°gina te indica qu√© contenidos has de leer del libro. Tras una primera lectura, lee las anotaciones del profesor, cuyo objetivo es ayudarte a entender los conceptos clave del cap√≠tulo. Despu√©s, realizar una segunda lectura del cap√≠tulo. En total, esta parte deber√≠a llevarte unas 3 horas üïíÔ∏è de trabajo.
- Visionado y estudio de los tutoriales en v√≠deo de esta [playlist oficial de PyTorch](https://www.youtube.com/playlist?list=PL_lsbAsL_o2CTlGHgMxNrKhzP97BaG9ZN).  M√≠rate al menos los 4 primeros v√≠deos (‚ÄúIntroduction to PyTorch‚Äù, ‚ÄúIntroduction to PyTorch Tensors‚Äù, ‚ÄúThe Fundamentals of Autograd‚Äù y ‚ÄúBuilding Models with PyTorch‚Äù). En total, esta parte deber√≠a llevarte unas 2 horas üïíÔ∏è de trabajo.
- Realizaci√≥n del [test de evaluaci√≥n](https://forms.gle/E1xzZHw6hzMWJaNr7) de estos contenidos. Son pocas preguntas y te llevar√° unos minutos.

### Contenidos para la sesi√≥n presencial del 29 de marzo de 2023

En la clase presencial, repasaremos los contenidos de la semana anterior y veremos c√≥mo se implementa el regresor log√≠stico en PyTorch siguiendo la implementaci√≥n de un regresor log√≠stico binario y de uno multinomial que se comentan en [este apartado]((https://jaspock.github.io/me/materials/transformers/regresor#regresores-implementados-en-pytorch)).

La idea es que vayas creando una serie de notebooks en Google Colab en los que incluyas y comentes cada uno de los programas que vamos a ir viendo. En la √∫ltima clase se presentar√° una pr√°ctica m√°s avanzada que implicar√° modificar el c√≥digo del transformer.

## Contenidos a preparar antes de la sesi√≥n del 26 de abril de 2023



## Contenidos a preparar antes de la sesi√≥n del 10 de mayo





