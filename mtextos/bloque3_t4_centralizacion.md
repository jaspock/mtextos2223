
T4. Centralizaci√≥n de datasets y modelos: Huggingface, OpenAI
====================================

```{admonition} Nota
:class: note
Lee con atenci√≥n el tema 4 del bloque 2. Realiza las lecturas propuestas y finalmente contesta el cuestionario que encontrar√°s en la secci√≥n de evaluaci√≥n relativo a este tema, el cual se encuentra en el √≠ndice del bloque 2.  En la clase presencial repasaremos los conceptos te√≥ricos principales correspondoentes a la sesi√≥n. **El plazo para realizar las lecturas y el cuestionario es: Apertura el 15/03/2023- Cierre 23:59 del 22/03/2023** (el d√≠a anterior a la clase presencial).

Tiempo de dedicaci√≥n: 2 horas (as√≠ncrona) + 2 horas trabajo independiente
```

Contenidos:

- [Huggingface: Introducci√≥n](#introduccion)
- [Huggingface: Repositorio de Datasets](#repositorio-de-datasets)
- [Huggingface: Repositorio de Modelos pre-entrenados](#repositorio-de-modelos-pre-entrenados)
- [OpenAI: Tecnolog√≠as de Generaci√≥n (GPT, Copilot, ChatGPT)](#Tecnolog√≠as-de-generaci√≥n)

## Introducci√≥n

Huggingface.co una compa√±√≠a centrada en el PLN la cual ha desarrollado las [**librer√≠as Transformers**](https://huggingface.co/transformers/), **centralizado datasets** y ha **creado modelos de aprendizaje pre-entrenados** disponibles a trav√©s de sus librer√≠as de programaci√≥n.
Las librer√≠as de Huggingface actualmente dan soporte a empresas muy importantes del mercado tecnol√≥gico. Ver <https://huggingface.co/>.

## Repositorio de Datasets

Proporciona conjuntos de datos para muchas tareas de PLN como clasificaci√≥n de texto, respuesta a preguntas, modelado de lenguaje, etc.  
Instalaci√≥n de librer√≠a de manipulaci√≥n de datasets
Para la **instalaci√≥n** de la librer√≠a de manipulaci√≥n de datasets se debe ejecutar la siguiente instrucci√≥n pip:

````
>>> pip install datasets
````

Para asegurarnos de que Transformers dataset se ha instalado correctamente es necesario ejecutar la siguiente instrucci√≥n:

````
>>> python -c "from datasets import load_dataset; print(load_dataset('squad', split='train')[0])"

````

Esta instrucci√≥n debe descargar la versi√≥n 1 del conjunto de datos de respuesta a preguntas de Stanford, cargar su divisi√≥n de entrenamiento e imprimir el primer ejemplo de entrenamiento de la siguiente manera:

````
{'id': '5733be284776f41900661182', 'title': 'University_of_Notre_Dame', 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend "Venite Ad Me Omnes"...', 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?', 'answers': {'text': array(['Saint Bernadette Soubirous'], dtype=object), 'answer_start': array([515], dtype=int32)}}
````

### Listar datasets disponibles en el repositorio

Para listar los conjuntos de datos disponibles es necesario ejecutar la siguiente funci√≥n ``datasets.list_datasets ()`` que pertenece a la clase ``datasets``.

````
>>> from datasets import list_datasets
>>> datasets_list = list_datasets()
>>> len(datasets_list)
656
>>> print(', '.join(dataset for dataset in datasets_list))
aeslc, ag_news, ai2_arc, allocine, anli, arcd, art, billsum, blended_skill_talk, blimp, blog_authorship_corpus, bookcorpus, boolq, break_data,
c4, cfq, civil_comments, cmrc2018, cnn_dailymail, coarse_discourse, com_qa, commonsense_qa, compguesswhat, coqa, cornell_movie_dialog, cos_e,
cosmos_qa, crime_and_punish, csv, definite_pronoun_resolution, discofuse, docred, drop, eli5, empathetic_dialogues, eraser_multi_rc, esnli,
event2Mind, fever, flores, fquad, gap, germeval_14, ghomasHudson/cqc, gigaword, glue, ‚Ä¶
````

Otra alternativa es:

1. Ir a la web <https://huggingface.co>
2. Seleccionar el men√∫ Datasets: <https://huggingface.co/datasets>
3. Filtrar por categor√≠a, idioma, tarea y/o licencia

### ¬øC√≥mo cargar datasets?

Haciendo uso de la funci√≥n ``load_dataset`` se nos permite recuperar cualquier dataset registrado en el repositorio. Por ejemplo, el dataset **MRPC** que ha sido proporcionado en el √≠ndice de referencia GLUE (<https://gluebenchmark.com/leaderboard>).

```
>>> from datasets import load_dataset
>>> dataset = load_dataset('glue', 'mrpc', split='train')
```

O podemos ver otro ejemplo como el de [**eHealth-KD**](https://knowledge-learning.github.io/ehealthkd-2020/)

````
>>> from datasets import load_dataset
>>> dataset = load_dataset("ehealth_kd")
````

No obstante, la librer√≠a ``datasets`` permite adem√°s **cargar conjuntos de datos propios** que no formen parte del repositorio. Por ejemplo:
````
>>> from datasets import load_dataset
>>> dataset = load_dataset('csv', data_files='my_file.csv')
````

Para m√°s detalles sobre las distintas funciones y par√°metros permitidos para manipular datasets ver la siguiente documentaci√≥n:
- <https://huggingface.co/docs/datasets/quicktour.html>

### Tareas, subtareas e idiomas de datasets

**Categor√≠as:**
En este repositorio podemos encontrar un amplio catalogo de tareas(categor√≠as) por las cuales filtrar y y especificar el tipo de dateset que estamos buscando. Hemos de resaltar que estos datasets existen originalmente en diferentes formatos, nos obstante en una vez incluido en este repositorio, el formato es estandar. Por tal motivo, a trav√©s de las libr√≠as de manipulaci√≥n (las mencionadas enteriormente) que ofrece Huggingface, podemos acceder a ellos y gestionarlos. Adem√°s, estos datasets se encuentran caracterizados por **idioma**, **subtarea** en la que se puede utilizar y la **licencia de uso**.

```{image} /images/bloque3/t4/hf_dataset_categoria.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 1. Categor√≠as generales(o tareas) que permiten el filtro de datasets

**M√°s de 134 tareas y m√°s de 194 idiomas:**

```{image} /images/bloque3/t4/hf_dataset_tareas_idiomas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 2. Idiomas de los datasets

## Repositorio de Modelos pre-entrenados

La biblioteca de Transformers permite el **uso de modelos previamente entrenados** para tareas de Comprensi√≥n del lenguaje natural (NLU), i.e. como analizar el sentimiento de un texto, y Generaci√≥n del lenguaje natural (NLG), i.e. como completar un mensaje con texto nuevo o traducir a otro idioma.
A groso modo listamos los modelos que nos podemos encontrar
- **An√°lisis de sentimiento**: Conocer si un texto es positivo o negativo
- **Generaci√≥n de texto** (en ingl√©s): proporcionar un mensaje para el cual el modelo generar√° un texto.
- **Reconocimiento de entidades nombradas** (NER): Dado en una oraci√≥n de entrada se etiqueta cada palabra con la entidad que esta representa (persona, lugar, organizaci√≥n, etc.)
- **Respuesta a preguntas**: Teniendo en cuenta un modelo de un contexto determinado, dado un pregunta se obtiene una respuesta.
- **Relleno de texto con m√°scara**: Dado un texto con palabras enmascaradas (p. Ej., Reemplazado por [M√ÅSCARA]), completar los espacios en blanco.
- **Resumen**: Generaci√≥n de un resumen a partir de texto extenso.
- **Traducci√≥n**: Traducci√≥n de un texto a otro idioma.
- **Extracci√≥n de caracter√≠sticas**: Obtener una representaci√≥n tensorial del texto.
Tomado de https://huggingface.co/transformers/quicktour.html

**Listado de tareas tal y como las podemos encontrar en el repositorio:**
El listado de tareas, como categor√≠as, en las que podemos filtar los distintos modelos preentrenados que ofrece el repositorio Huggingface, es igual de amplio que el de los datasets. Como podemos observar, a partir de 2022 tal y como se describe m√°s adelante, en el siguiente imagen este repositorio no solo ofrece modelos prentrenados para el modelado del lenguaje, sino tambi√©n para desarrollar tareas de distintas modalidades: multimodal, lenguaje, audio, visi√≥n(imagen), datos estructurados(tabulados), y otros.


```{image} /images/bloque3/t4/hf_modelos_tareas.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 300px
:align: center
```

Figura 3. Tareas filtro modelos
 
**Idiomas para los que se han entrenado los modelos:** 
El listado de idiomas,como categor√≠as, en las que podemos filtar los distintos modelos preentrenados que ofrece el repositorio Huggingface, es igual de amplio que el de los datasets.


Una explicaci√≥n detallada sobre cada una de estas tareas y ejemplos de uso con Huggingface Transformer la podemos encontrar en el siguiente enlace: 
- <https://huggingface.co/transformers/task_summary.html>


**Huggingface a partir de 2022**
A mediados de 2022 esta plataforma federativa da un paso agigantado expandiendo datasets y modelos preentrenados de solo ofrecer recursos para la modalidad de Procesamiento del Lenguaje Natural, a ofrecer recursos Multimodales, Visi√≥n por Computadora, Procesamiento de Audio, Procesamiento de datos Tabulares y  para Aprendisaje por reforzamiento.

En la mayor√≠a de los casos se ofrece una ejemplo de uso y documentaci√≥n. Poner en marcha cualquiera de estas tareas, reajustando o no los modelos prentrenados que se ofrecen en esta plataforma, se encuentra bien documentado y ejemplificado en ella: Ver Categor√≠as <https://huggingface.co/tasks>


```{image} /images/bloque3/t4/doc-tareas-hf.jpg
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```
Figura 4. Categor√≠as de documentaciones agrupadas por tareas y modalidades 

Ejemplo de An√°lisis de Sentimientos con Huggingface Transformer:

````
>>> from transformers import pipeline
>>> classifier = pipeline('sentiment-analysis')
>>> classifier('We are very happy to show you the ü§ó Transformers library.')
[{'label': 'POSITIVE', 'score': 0.9997795224189758}]
````

Si os fij√°is hemos cargado un modelo pre-entrenado a trav√©s del pipeline  ``sentiment-analysis`` para utilizarlo como clasificador. Este **modelo** se puede **reentrenar** a escenarios espec√≠ficos si queremos realizando un ajuste sobre un nuevo corpus. Para **m√°s detalles ver la clase pr√°ctica** [``bloque3_p3_SA-Transformers-Training-FineTuning``](https://jaspock.github.io/mtextos/bloque3_p3_SA-Transformers-Training-FineTuning.html)


Si queremos que el pipeline sea multilingue, podemos indicar el modelo exacto que contemple un diccionario de este tipo y el pipeline lo ensamblar√° internamente. Mirad el siguiente ejemplo:

````
>>> from transformers import pipeline
>>> classifier = pipeline('sentiment-analysis', model='nlptown/bert-base-multilingual-uncased-sentiment' )
>>> classifier('Estoy muy triste')
[{'label': '1 star', 'score': 0.7241697907447815}]
````

Para otras tareas como Rellenado de M√°scaras podemos ver como podemos simplemente indicar el tipo de tarea para que el pipeline seleccione el tipo de configuraci√≥n m√°s adecuada a esta y el modelo que queremos aplicarle. Con solo cambiar el modelo base podemos hacer esta tarea unilingue a multilingue o cambiar de idioma. Ver el ejemplo a continuaci√≥n:

````
>>> from transformers import AutoModelWithLMHead, AutoTokenizer
>>> model = AutoModelWithLMHead.from_pretrained('mrm8488/RuPERTa-base')
>>> tokenizer = AutoTokenizer.from_pretrained("mrm8488/RuPERTa-base", do_lower_case=True)

>>> from transformers import pipeline

>>> pipeline_fill_mask = pipeline("fill-mask", model=model, tokenizer=tokenizer)

>>> pipeline_fill_mask("Espa√±a es un pa√≠s muy <mask> en la UE")

[{'score': 0.19951821863651276,
  'sequence': 'Espa√±a es un pa√≠s muy importante en la UE',
  'token': 1560,
  'token_str': ' importante'},
 {'score': 0.04137842729687691,
  'sequence': 'Espa√±a es un pa√≠s muy grande en la UE',
  'token': 2741,
  'token_str': ' grande'},
 {'score': 0.029216745868325233,
  'sequence': 'Espa√±a es un pa√≠s muy peque√±o en la UE',
  'token': 2948,
  'token_str': ' peque√±o'},
 {'score': 0.02563760057091713,
  'sequence': 'Espa√±a es un pa√≠s muy popular en la UE',
  'token': 5782,
  'token_str': ' popular'},
 {'score': 0.022264542058110237,
  'sequence': 'Espa√±a es un pa√≠s muy antiguo en la UE',
  'token': 5240,
  'token_str': ' antiguo'}]
````

### Listado de Pipelines 
En Huggingface podemos encontrar una serie de Pipelines ya preparados para enfrentar tareas concretas a los cuales les podemos suministrar distintos modelos y tokenizadores transformes. Ver ejemplos: <https://huggingface.co/transformers/main_classes/pipelines.html>

### ¬øC√≥mo buscar y reutilizar modelos pre-entrenados en la plataforma?

A continuaci√≥n, se listan los pasos a seguir:
1. Dirigirse al repositorio <https://huggingface.co/>
2. Seleccionar el men√∫ ``models`` que nos llevar√° a <https://huggingface.co/models>
3. Filtrar el listado de modelos seg√∫n la tarea, idioma, librer√≠a (Pytorch o TensorFlow), dataset sobre el que fue entrenado, o licencia. Por ejemplo:  tarea ``Text Classification``; idioma ``es``.
4. Elegir un modelo de la lista. Por ejemplo: ``bert-base-multilingual-uncased-sentiment``
5. Obtendremos la documentaci√≥n necesaria para utilizar el modelo.

**Conociendo el nombre del modelo** a utilizar entonces podemos **hacer uso** de este a trav√©s de la librer√≠a **Transformer**. 
En la propia documentaci√≥n se aporta el **c√≥digo de ejemplo** para hacer uso del modelo y en algunos casos una [**interfaz para probarlo**](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment): 

````
from transformers import AutoTokenizer, AutoModelForSequenceClassification
tokenizer = AutoTokenizer.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment")  # cargando el toquenizador basado en el modelo preentrenado
model = AutoModelForSequenceClassification.from_pretrained("nlptown/bert-base-multilingual-uncased-sentiment") # cargando del modelo preentrenado
````

### Configuraciones de modelos trasnformers
Los **modelos pre-entrenados** que se brindan en el repositorio se **basan** en alguna de las **arquitecturas Transformers** descrita en la documentaci√≥n del repositorio (<https://huggingface.co/docs>).
Si tomamos como referencia la arquitectura de modelo Transformer [DistilBERT](https://huggingface.co/transformers/model_doc/distilbert.html#overview) podemos conocer c√≥mo **gestionar** los distintos **par√°metros**, [**configuraciones de red neuronal**](https://huggingface.co/transformers/model_doc/distilbert.html#distilbertconfig), [**tokenizador**](https://huggingface.co/transformers/model_doc/distilbert.html#distilberttokenizer) y **ejemplos documentados** para cada tipo de tarea, tal y como podemos encontrar en el siguiente enlace (<https://huggingface.co/course/chapter7/>).

````
>>> # !pip install transformers
>>> from transformers import DistilBertTokenizer, DistilBertModel
>>> import torch

>>> tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased') # cargando de toquenizador basado en el modelo preentrenado
>>> model = DistilBertModel.from_pretrained('distilbert-base-uncased') # cargando el modelo preentrenado

>>> inputs = tokenizer("Hello, my dog is cute", return_tensors="pt")
>>> outputs = model(**inputs)

>>> last_hidden_states = outputs.last_hidden_state
>>> print(last_hidden_states)

tensor([[[-1.8296e-01, -7.4054e-02,  5.0267e-02,  ..., -1.1261e-01,
           4.4493e-01,  4.0941e-01],
         [ 7.0631e-04,  1.4825e-01,  3.4328e-01,  ..., -8.6039e-02,
           6.9475e-01,  4.3353e-02],
         [-5.0721e-01,  5.3086e-01,  3.7163e-01,  ..., -5.6287e-01,
           1.3756e-01,  2.8475e-01],
         ...,
         [-4.2251e-01,  5.7314e-02,  2.4338e-01,  ..., -1.5223e-01,
           2.4462e-01,  6.4155e-01],
         [-4.9384e-01, -1.8895e-01,  1.2641e-01,  ...,  6.3241e-02,
           3.6913e-01, -5.8252e-02],
         [ 8.3269e-01,  2.4948e-01, -4.5440e-01,  ...,  1.1998e-01,
          -3.9257e-01, -2.7785e-01]]], grad_fn=<NativeLayerNormBackward>)

````

Es importante conocer que las **configuraciones** de modelos Transformer ya **cuentan** con **modelos base pre-entrenados**. En el caso de ``DistilBERT`` podemos encontrar ``distilbert-base-uncased``. 


## Tecnolog√≠as de generaci√≥n

### GPT
GPT significa "Generative Pretrained Transformer". Es un modelo de lenguaje que utiliza t√©cnicas de deep learning para generar texto de manera aut√≥noma. GPT ha sido entrenado en una amplia cantidad de contenido textual. !Es **orientado a liber√≠as**! Es decir, se puede incorporar el componente en tu propia aplicaci√≥n.


- GPT-1: Es la primera versi√≥n de GPT, [entrenado con 117 millones de par√°metros](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf). Aunque es significativamente m√°s limitada que las versiones posteriores, a√∫n es capaz de generar texto aceptable en muchos contextos. 
La arquietectura de GPT-1 es principalmente un conjunto de 12 bloques de transformadores decodificadores colocados uno tras otro(ej. 12x ver la imagen). Los datos de texto se codifican mediante una [codificaci√≥n de pares de bytes](https://arxiv.org/pdf/1508.07909.pdf) adaptada a caracteres. La [incrustaci√≥n de posici√≥n es aprendida, en lugar de la t√≠pica sinusoidal est√°tica](https://arxiv.org/pdf/1706.03762.pdf). La longitud m√°xima para tokens consecutivos es 512. La capa superior es simplemente una capa softmax adaptada a la tarea de aprendizaje espec√≠fica.
- GPT-2: Es la segunda versi√≥n de GPT, con solo [1.5 mil millones de par√°metros](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). Es capaz de generar texto coherente y a menudo convincente. GPT-2 tiene b√°sicamente la misma arquitectura que GPT-1, pero el modelo m√°s grande contiene 48 bloques(48x ver la imagen) de transformadores. La segunda capa de normalizaci√≥n se mueve a la primera posici√≥n en un bloque y el √∫ltimo bloque contiene una capa de normalizaci√≥n adicional. Los pesos se inicializan de forma ligeramente diferente y se aumenta el tama√±o del vocabulario. El n√∫mero de tokens consecutivos se incrementa a 1024.
- [GPT-3](https://arxiv.org/abs/2005.14165): Es la tercera versi√≥n de GPT y es uno de los modelos de lenguaje m√°s grandes y avanzados jam√°s entrenados. Tiene m√°s de [175 mil millones de par√°metros](https://arxiv.org/abs/2005.14165), lo que le permite generar texto muy convincente en una amplia variedad de contextos. GPT-3 tiene la misma arquitectura que GPT-2, pero el n√∫mero de bloques aument√≥ a 96 en el modelo m√°s grande y el tama√±o del contexto (n√∫mero de tokens consecutivos) aument√≥ a 2048. Las [capas de autoatenci√≥n de varios cabezales se alternan entre los t√≠picos densos los escasos y los dispersos](https://arxiv.org/pdf/1904.10509.pdf). 


GPT-1 se entrena de manera autosupervisada (aprende a predecir la siguiente palabra en datos de texto) y se ajusta de manera de aprendizaje supervisado. GPT-2 se entrena de forma totalmente autosupervisada, centr√°ndose en la transferencia de *zero-shot* y GPT-3 se entrena previamente de manera autosupervisada explorando un poco m√°s *few-shots fine-tuning*.  

```{image} /images/bloque3/t4/GPT-1-2-3_architecture.png
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```
Figura 4. Arquitecturas GPT. Fuente <https://newsletter.theaiedge.io/p/the-chatgpt-models-family> 

Adem√°s de estas versiones, tambi√©n existen variantes m√°s peque√±as de GPT para diferentes usos, como GPT-3 Lite y GPT-2 Medium. Cada una de estas variantes tiene un tama√±o y capacidad diferente, lo que las hace m√°s adecuadas para diferentes aplicaciones y escenarios.



#### Entrenamiento del GPT

- GPT-1 est√° preentrenado en el conjunto de datos de BooksCorpus, que contiene ~7000 libros que suman ~5 GB de datos: <https://huggingface.co/datasets/bookcorpus>.

- GPT-2 se entrena previamente con el conjunto de datos de WebText, que es un conjunto m√°s diverso de datos de Internet que contiene ~8 millones de documentos para aproximadamente ~40 GB de datos: <https://huggingface.co/datasets/openwebtext>

- GPT-3 utiliza una versi√≥n ampliada del conjunto de datos de WebText, dos corpus de libros basados en Internet que no se divulgan y la Wikipedia en ingl√©s que constituy√≥ ~600 GB de datos.


La implementaci√≥n de GPT-2 se puede encontrar en los siguientes repositorios:

- TensorFlow por OpenAI: <https://github.com/openai/gpt-2/blob/master/src/model.py>

- PyTorch por Andrej Karpathy: <https://github.com/karpathy/minGPT/blob/master/mingpt/model.py>

A continuaci√≥n se muestra un ejemplo de uso de GPT2 en un Pipeline:

````
from transformers import pipeline, set_seed
generator = pipeline('text-generation', model='gpt2')
set_seed(42)
generator("My name is", max_length=30, num_return_sequences=5)

````

GPT-3 API se encuentra disponible en el siguiente enlace: <https://platform.openai.com/docs/introduction/overview>

#### Ventajas

- Alto rendimiento en tareas de lenguaje natural: GPT est√° entrenado en una gran cantidad de texto en internet, lo que le permite desarrollar una comprensi√≥n profunda del lenguaje natural y su uso en diferentes contextos. Esto hace que mejore la capacidad de rendimiento y calidad en tareas como la traducci√≥n autom√°tica, la generaci√≥n de texto y la respuesta a preguntas.

- Facilidad de uso: GPT es un modelo pre-entrenado, lo que significa que no es necesario entrenarlo desde cero para cada tarea espec√≠fica. Esto significa que es m√°s f√°cil de usar para los desarrolladores y requiere menos recursos de hardware y tiempo de entrenamiento.

- Adaptabilidad: GPT puede ser finetuneado o adaptado a diferentes tareas y contextos espec√≠ficos. Esto permite que el modelo se ajuste a los requisitos espec√≠ficos de cada proyecto y mejore su rendimiento.

- Capacidad generativa: GPT es un modelo generativo, lo que significa que es capaz de generar texto de forma aut√≥noma. Esto es √∫til en una variedad de aplicaciones, como la generaci√≥n de contenido, la creaci√≥n de di√°logos virtuales y la respuesta a preguntas.

#### Desventajas
- Bias y desigualdades: Al estar entrenado en una gran cantidad de texto en internet, GPT puede incorporar los sesgos y desigualdades presentes en la fuente de datos. 

- Inseguridad: GPT es un modelo de aprendizaje autom√°tico, lo que significa que su rendimiento puede ser afectado por la calidad y la representatividad de la fuente de datos utilizada para su entrenamiento. Adem√°s, el modelo puede ser **vulnerable a ataques y manipulaciones**, como la generaci√≥n de texto falsificado o la respuesta a preguntas inapropiadas.

- Costos computacionales: GPT es un modelo grande y complejo que requiere una gran cantidad de recursos computacionales para su entrenamiento y uso. Esto puede resultar en costos elevados para el hardware y la energ√≠a, lo que puede ser un obst√°culo para algunos usuarios.

- Limitaciones en la comprensi√≥n del contexto: Aunque GPT ha sido entrenado en una gran cantidad de texto, todav√≠a puede tener dificultades para comprender el contexto en el que se utiliza el lenguaje natural. Esto puede resultar en respuestas poco precisas o inapropiadas en ciertos contextos.

### Copilot
Es asistente de inteligencia artificial dise√±ado, por OpenAI, para ayudar enel completamiento de c√≥digo mediante el uso de la conversaci√≥n natural. Copilot utiliza modelos de lenguaje avanzados para comprender tus necesidades y brindarte la informaci√≥n y la ayuda que necesitas. Puedes interactuar con Copilot en una variedad de plataformas y dispositivos, incluyendo mensajer√≠a, aplicaciones de chat, aplicaciones de escritorio y m√°s. !Es **orientado a servicios en la nube**! Es decir, se se accede a los servicios online a trav√©s de una API.

Copilot est√° dise√±ado para ayudarte a realizar una amplia gama de tareas y responder preguntas de forma eficiente y precisa. Algunos ejemplos de las tareas que puedes realizar con Copilot incluyen:

- **Consultar informaci√≥n** sobre el clima, la hora actual y otras condiciones meteorol√≥gicas.
- Obtener informaci√≥n sobre **eventos actuales, noticias y tendencias**.
- Realizar **b√∫squedas en l√≠nea** y encontrar informaci√≥n sobre temas espec√≠ficos.
- **Programar recordatorios y citas**.
- **Obtener recomendaciones** de restaurantes, pel√≠culas y otras formas de entretenimiento.
- **Traducir** palabras y frases a otros idiomas.
- **Obtener informaci√≥n sobre la bolsa de valores**, la tasa de cambio y otras cotizaciones financieras.
- **Resolver problemas** **matem√°ticos** y **responder preguntas** sobre **conceptos cient√≠ficos** y **tecnol√≥gicos**.

Copilot est√° dise√±ado para ayudarte a realizar muchas tareas cotidianas y responder preguntas de una manera conveniente y r√°pida. Ejemplo de ello, lo podemos encontrar en la integraci√≥n de pluggins en [Visual Studio Code](https://docs.github.com/en/copilot/getting-started-with-github-copilot/getting-started-with-github-copilot-in-visual-studio-code) para la completaci√≥n de c√≥digos.


#### Ventajas

- Copilot utiliza una **interfaz de conversaci√≥n natural** (Visual y API) para interactuar con los usuarios, lo que hace que sea f√°cil y agradable de usar.

- Est√° entrenado en una amplia gama de informaci√≥n y puede **ayudar a los usuarios a encontrar y proporcionar informaci√≥n sobre una amplia variedad de temas**.

- Puede ayudar a los usuarios a realizar tareas y **obtener informaci√≥n de manera m√°s r√°pida y eficiente**, lo que les **permite ser m√°s productivos**.

- Est√° **dise√±ado para proporcionar una experiencia de usuario amigable y personalizada**, lo que puede mejorar la satisfacci√≥n del usuario y fidelizaci√≥n.

- Puede **integrarse con otros servicios en l√≠nea** para proporcionar una experiencia de usuario m√°s completa.

#### Desventajas
- Costo: Copilot es un producto de OpenAI(empresa privada) y puede ser costoso pagar el uso de servicios para algunos usuarios, especialmente para aquellos que requieren una gran cantidad de uso o integraciones.

- Accesibilidad limitada: **Solo est√° disponible como una API**, por lo que solo puede ser utilizado por desarrolladores y no est√° disponible directamente para el p√∫blico en general.

- Capacidad limitada: Aunque Copilot est√° entrenado en una amplia gama de informaci√≥n, **todav√≠a hay l√≠mites en su capacidad para comprender y responder** a todas las preguntas y tareas.

- Confidencialidad y privacidad: Al usar Copilot, **debes compartir tus datos y preocuparte por la privacidad y seguridad de ellos**.

- Requiere habilidades t√©cnicas: Para **integrar esta tecnolog√≠a** en tus aplicaciones y servicios, **debes tener habilidades** t√©cnicas y conocimientos en programaci√≥n.

#### Alternativas a Copilot

- **Dialogflow**: Una plataforma de Google que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.

- **IBM Watson Assistant: Una plataforma de inteligencia artificial de IBM que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.

- **Microsoft Bot Framework**: Un marco de trabajo de Microsoft que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n para varias plataformas, incluidas las aplicaciones de mensajer√≠a, los sitios web y las aplicaciones de escritorio.

- **Amazon Lex**: Un servicio de Amazon Web Services que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.

- **Rasa**: Un marco de software de c√≥digo abierto que permite a los desarrolladores crear chatbots y aplicaciones de conversaci√≥n.

### ChatGPT

Es un modelo de lenguaje entrenado utilizando una gran cantidad de texto en internet. Se trata de una tecnolog√≠a de procesamiento del lenguaje natural que permite a los usuarios interactuar con el modelo mediante el uso de conversaciones naturales. !Es **orientado a servicios en la nube**! Es decir, se se accede a los servicios online a trav√©s de una API.

Algunas de las funcionalidades m√°s destacadas incluyen:

- **Responder preguntas**: ChatGPT puede responder preguntas sobre una amplia gama de temas, incluyendo **historia, geograf√≠a, ciencias, tecnolog√≠a, programaci√≥n y mucho m√°s**.

- **Completar oraciones o fragmentos de texto**: ChatGPT puede utilizar el contexto y la informaci√≥n previa para completar oraciones o fragmentos de texto de manera eficiente.

- **Generar texto**: ChatGPT puede generar texto en una variedad de formatos, como descripciones de productos, rese√±as de pel√≠culas y mucho m√°s.

- **Traducci√≥n de idiomas**: ChatGPT puede traducir palabras y frases a otros idiomas, lo que lo hace ideal para aquellos que desean comunicarse en un idioma distinto al suyo.

- **Resumen de texto**: ChatGPT puede resumir grandes cantidades de texto en una forma concisa y f√°cil de entender.

- **An√°lisis de sentimientos**: ChatGPT puede analizar el contenido de un texto para determinar el sentimiento que se expresa en √©l, como por ejemplo si es positivo, negativo o neutral.

En la web oficial de OpenAI podemos ver un amplio listado de ejemplos de aplicaciones de esta tecnolog√≠a:

- Q&A
- Correcci√≥n gramtical 
- Resumir un texto
- Traducir un texto complejo en un simple concepto.
- Llamadas a APIs para usar t√©cnicas de PLN 
- Generar comandos de programaci√≥n a partir de instrucciones en lenguaje natural
- Traducci√≥n autom√°tica
- Generar codificaci√≥n de programaci√≥n: para llamar APIs, sentencias SQL, estructuras de programaci√≥n, etc., desde instrucciones en lenguaje natural
- Crear tabulaciones a √†rtir de texto
- Separar contenido no estructurado
- Tareas de clasificaci√≥n. 
  - Extracci√≥n de categor√≠as impl√≠citas en textos 
- Generar descripciones y explicaciones a partir de c√≥digos Python
- Convertir el t√≠tulo de una pel√≠cula en un emoji
- Hallar la complejidad computacional de una funci√≥n
- Traducir de un lenguaje de programaci√≥n a otro
- Detecci√≥n de sentimientos para un fragmento de texto.
- Explicar una pieza complicada de c√≥digo.
- Extraer palabras clave de un bloque de texto.
- Convertir la descripci√≥n de un producto en un texto publicitario.
- Generador de nombres de productos
- Solucionar de errores de Python
  - Encontrar y corregir errores en el c√≥digo fuente.
- Crear de hojas de c√°lculo
- Responder preguntas de JavaScript
- Responder preguntas sobre modelos de lenguaje
- Crear una lista de elementos para un tema determinado.
- Extracci√≥n de informaci√≥n
- Crear  microhistorias
- Convertir texto en de tercera persona
- Generar esquemas para un tema.
- Conversaci√≥n abierta con un asistente de IA.


A continuaci√≥n se muestra la evaluaci√≥n de modelos hasta lo que hoy conocemos como ChatGPT:

```{image} /images/bloque3/t4/GPT-1-2-3_datasources.png
:alt: comic xkcd 2421
:class: bg-primary mb-1
:width: 600px
:align: center
```
Figura 5. Evoluci√≥n de GPT hasta llegar a ChatGPT. Fuente <https://newsletter.theaiedge.io/p/the-chatgpt-models-family> 

#### Ejemplo de uso de la API ChatGPT: 
````
import openai

# Inicializar la API de OpenAI
openai.api_key = "tu_api_key_aqui"

# Hacer una pregunta a ChatGPT
response = openai.Completion.create(
    engine="text-davinci-002", # asignamos el nombre del modelo a utilizar. Ejemplo: "text-davinci-003", "text-davinci-002", "text-davinci-001", "code-davinci-002", ...
    prompt="Qu√© es el sol?", # Entrada
    max_tokens=1024, # Dimensionalidad de la ventana 
    n=1,
    stop=None,
    temperature=0.5,
)
# Imprimir la respuesta
print(response["choices"][0]["text"])
>>> "El sol es una estrella."
````

N√≥tese que para poder utilizar esta librer√≠a se ha de emplear un servicio en la nube del cual se ha de requerir una clave de acceso. Las instruciones para conseguirlas las pod√©is encontrar en el siguiente enlace: <https://platform.openai.com/account/api-keys>


#### Ventajas

- **Gran capacidad de comprensi√≥n del lenguaje natural**: ChatGPT ha sido entrenado en una amplia variedad de textos y ha desarrollado una comprensi√≥n profunda del lenguaje humano, lo que le permite responder de manera fluida y natural a las preguntas y comentarios de los usuarios.

- **Personalizaci√≥n**: Puede ser personalizado para diferentes aplicaciones y usos, lo que lo hace ideal para una amplia variedad de industrias.

- **Alta disponibilidad**: Est√° **disponible en la nube** y puede ser accedido desde cualquier lugar con una conexi√≥n a Internet, lo que lo hace muy accesible para los usuarios.

- **Rapidez y eficiencia**: Es **capaz de procesar grandes cantidades de informaci√≥n** en un tiempo muy corto, lo que lo hace ideal para aplicaciones en tiempo real.

- **Mejora continua**: Est√° en constante desarrollo y mejora por parte de OpenAI, lo que significa que sus capacidades y funciones contin√∫an mejorando con el tiempo.

#### Desventajas

- **Limitaciones en la comprensi√≥n del contexto**: Aunque ChatGPT ha sido entrenado en una amplia variedad de textos, todav√≠a puede tener dificultades para comprender el contexto completo de una conversaci√≥n, especialmente en situaciones m√°s complejas.

- **Responsabilidad por la precisi√≥n de la informaci√≥n**: Puede proporcionar informaci√≥n que no sea precisa o que sea enga√±osa. Es responsabilidad del usuario verificar la informaci√≥n proporcionada por ChatGPT antes de tomar decisiones o acciones importantes.

- **Requisitos de infraestructura**: Para utilizar ChatGPT, es **necesario tener acceso a la infraestructura** y los recursos necesarios para conectarse y comunicarse con el modelo. Esto **puede ser un obst√°culo para algunos usuarios que no cuenten con la infraestructura adecuada**. **Acceso a internet obligatorio!!!**

- **Costo**: Su uso **puede requerir una inversi√≥n significativa en t√©rminos de costos de licenciamiento** y recursos de infraestructura.

#### Alternativas a ChatGPT
Algunas alternativas son:

- [**PEER de Meta AI**]([https://arxiv.org/pdf/2208.11663.pdf]): un lenguaje entrenado para **imitar el proceso de escritura**. Est√° entrenado en los [datos del historial de edici√≥n de Wikipedia](https://dumps.wikimedia.org/enwiki/). Se especializa en predecir ediciones y explicar las razones de esas ediciones. Es capaz de citar y citar documentos de referencia para respaldar las afirmaciones que genera. Es un transformador de 11 billones de par√°metros con la arquitectura t√≠pica de codificador-decodificador, y est√° superando a GPT-3 en la tarea en la que se especializa.

- [**LaMDA de Google AI**](https://arxiv.org/pdf/2201.08239.pdf): un modelo de lenguaje entrenado para **aplicaciones de di√°logo**. Est√° pre-entrenado en de ~3 billones de documentos y ~1 billones de di√°logos y ajustado en datos generados por humanos para mejorar la calidad, la seguridad y la veracidad del texto generado. Tambi√©n est√° ajustado para aprender a llamar a un sistema externo de recuperaci√≥n de informaci√≥n, como la B√∫squeda de Google, una calculadora y un traductor, lo que lo convierte en un candidato mucho m√°s fuerte para reemplazar la B√∫squeda de Google que ChatGPT. Es un decodificador de  135 billones par√°metros solo el transformer.

- [**PaLM de Google AI**](https://arxiv.org/pdf/2204.02311.pdf) - El m√°s grande de todos: ¬°540 billlones de par√°metros! Con capacidades innovadoras en aritm√©tica y razonamiento de sentido com√∫n. Est√° entrenado en 780 mil millones de tokens provenientes de conversaciones en redes sociales multiling√ºes, p√°ginas web multiling√ºes filtradas, libros, repositorios de GitHub, Wikipedia multiling√ºe y noticias.



## Bibliograf√≠a

[1] <https://huggingface.co/>

[2] <https://openai.com/blog/chatgpt/>

[3] Zhang, Y., Sun, S., Galley, M., Chen, Y. C., Brockett, C., Gao, X., ... & Dolan, B. (2019). Dialogpt: Large-scale generative pre-training for conversational response generation. arXiv preprint arXiv:1911.00536.

[4] https://newsletter.theaiedge.io/p/the-chatgpt-models-family
